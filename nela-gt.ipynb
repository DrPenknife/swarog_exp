{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aefad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d503a6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-25 12:33:21--  https://docs.google.com/uc?export=download&confirm=&id=18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.203.206, 2a00:1450:401b:810::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.203.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0k-34-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hu3hf9fb264l5mne6gmhhtkla9f5dc4e/1661423550000/05536367513713019519/*/18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV?e=download&uuid=a2cba134-b25d-4da1-a1bb-a764bceac670 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-08-25 12:33:24--  https://doc-0k-34-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hu3hf9fb264l5mne6gmhhtkla9f5dc4e/1661423550000/05536367513713019519/*/18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV?e=download&uuid=a2cba134-b25d-4da1-a1bb-a764bceac670\n",
      "Resolving doc-0k-34-docs.googleusercontent.com (doc-0k-34-docs.googleusercontent.com)... 142.250.75.1, 2a00:1450:401b:80d::2001\n",
      "Connecting to doc-0k-34-docs.googleusercontent.com (doc-0k-34-docs.googleusercontent.com)|142.250.75.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51522510 (49M) [text/csv]\n",
      "Saving to: ‘pubhealth.csv’\n",
      "\n",
      "pubhealth.csv       100%[===================>]  49,13M  1,79MB/s    in 14s     \n",
      "\n",
      "2022-08-25 12:33:39 (3,40 MB/s) - ‘pubhealth.csv’ saved [51522510/51522510]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!id=\"18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV\"; conf=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$id -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p'); wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$conf&id=$id\" -O pubhealth.csv && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f796758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      " 0    494808\n",
      "1    262288\n",
      "Name: label, dtype: int64\n",
      "shape  \n",
      " (757096, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 36.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (757096,) y (757096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "data = pd.read_csv(\"raw/nela-gt-2020.csv\",sep=\",\")\n",
    "print(\"labels\\n\",data[\"label\"].value_counts())\n",
    "print(\"shape  \\n\",data.shape)\n",
    "\n",
    "bootstrap_size = 0\n",
    "\n",
    "if bootstrap_size != 0:\n",
    "    bootstrap_factor = bootstrap_size / data.shape[0]\n",
    "    bootstrap = np.random.uniform(size=data.shape[0]) < bootstrap_factor\n",
    "    data = data.iloc[bootstrap]\n",
    "\n",
    "data[\"text\"] = [ str(v) for v in data[\"text\"]]\n",
    "X, y = data[\"text\"].values, data[\"label\"].values\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b756ff71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'speaker', 'text', 'explanation', 'url', 'label'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b38faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# SAVE\n",
    "with open('nela-gt-X.pickle', 'wb') as handle:\n",
    "    pickle.dump(X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('nela-gt-y.pickle', 'wb') as handle:\n",
    "    pickle.dump(y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4abe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 37.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (757096,) y (757096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "with open('nela-gt-X.pickle', 'rb') as handle:\n",
    "    X=pickle.load(handle)\n",
    "    \n",
    "with open('nela-gt-y.pickle', 'rb') as handle:\n",
    "    y=pickle.load(handle)\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c31e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "\n",
    "class BERTEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=16, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return csr_matrix(allembeds)\n",
    "\n",
    "\n",
    "class BertHead(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.head = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        self.head.fit(X, y)\n",
    "\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X=None):    \n",
    "        return self.head.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fe3749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 47319/47319 [24:03<00:00, 32.78it/s]\n"
     ]
    }
   ],
   "source": [
    "bert = BERTEmbeddings()\n",
    "X_dstil_numpy = bert.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a23149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# SAVE\n",
    "with open('nela-gt_BERTEmbeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_dstil_numpy, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c36cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 40.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (757096,) y (757096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "with open('nela-gt_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    X_dstil_numpy = pickle.load(handle)\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626216ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=1\n",
    "train = foldids[fold][1]\n",
    "test = foldids[fold][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737cfcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "arr = []\n",
    "\n",
    "\n",
    "\n",
    "def doTrain(X,y):\n",
    "    clf = BaggingClassifier(base_estimator=svm.SVC(class_weight='balanced'),\n",
    "                            n_estimators=10, random_state=0, max_samples=0.2).fit(X, y)\n",
    "    #ens = []\n",
    "    #cls = svm.SVC() # LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "    #cls.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def doTest(cls, X):\n",
    "    return cls.predict(X)\n",
    "\n",
    "for fold,j in enumerate(foldids):\n",
    "    train = foldids[fold][1]\n",
    "    test = foldids[fold][2]\n",
    "    \n",
    "    pca = PCA(n_components=512)\n",
    "    X_train_pca = pca.fit_transform(X_dstil_numpy[train])\n",
    "    X_test_pca = pca.transform(X_dstil_numpy[test])\n",
    "    \n",
    "    cls = doTrain(X_train_pca, y[train])\n",
    "    \n",
    "    y_pred = doTest(cls, X_test_pca)\n",
    "    \n",
    "    bac = balanced_accuracy_score(y[test], y_pred)\n",
    "    \n",
    "    arr.append(bac)\n",
    "\n",
    "    print(\" BAC=\", bac)\n",
    "\n",
    "print(10*\"-\")\n",
    "print(\"AVG. BAC=\",np.mean(arr),\"+/-\",np.std(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6943f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BAC= 0.8007864947600388\n",
      " BAC= 0.8005056569270981\n",
      " BAC= 0.8005197171602432\n",
      " BAC= 0.8003859727183564\n",
      " BAC= 0.8013379078784156\n",
      " BAC= 0.7998190364851836\n",
      " BAC= 0.8008250541308373\n",
      " BAC= 0.8015369305476252\n",
      " BAC= 0.8006174432269242\n",
      " BAC= 0.7999808147250818\n",
      "----------\n",
      "AVG. BAC= 0.8006315028559804 +/- 0.0005052330705177547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "arr = []\n",
    "\n",
    "\n",
    "\n",
    "def doTrain(X,y):\n",
    "    #clf = BaggingClassifier(base_estimator=LogisticRegression(class_weight='balanced', max_iter=10000),\n",
    "    #                        n_estimators=10, random_state=0, max_samples=0.42).fit(X, y)\n",
    "    #ens = []\n",
    "    cls = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "    cls.fit(X, y)\n",
    "    return cls\n",
    "\n",
    "def doTest(cls, X):\n",
    "    return cls.predict(X)\n",
    "\n",
    "for fold,j in enumerate(foldids):\n",
    "    train = foldids[fold][1]\n",
    "    test = foldids[fold][2]\n",
    "    \n",
    "    pca = PCA(n_components=512)\n",
    "    X_train_pca = pca.fit_transform(X_dstil_numpy[train])\n",
    "    X_test_pca = pca.transform(X_dstil_numpy[test])\n",
    "    \n",
    "    cls = doTrain(X_train_pca, y[train])\n",
    "    \n",
    "    y_pred = doTest(cls, X_test_pca)\n",
    "    \n",
    "    bac = balanced_accuracy_score(y[test], y_pred)\n",
    "    \n",
    "    arr.append(bac)\n",
    "\n",
    "    print(\" BAC=\", bac)\n",
    "\n",
    "print(10*\"-\")\n",
    "print(\"AVG. BAC=\",np.mean(arr),\"+/-\",np.std(arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

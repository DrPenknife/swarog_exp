{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ff1106-af3b-43b2-b825-e35fc2eb1fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "from bentoml.io import JSON\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle5 as pickle\n",
    "import sqlite3\n",
    "\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6302b3f3-d848-413b-8704-0b5b02b3e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle5 as pickle\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0ee09192-84a1-4b76-acb5-ccc72bb8f2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vectorizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyCustomUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "print(\"load vectorizer\")    \n",
    "with open(f'../../pickles/swarog_data/tfidf_vectorizer_full.pickle', 'rb') as handle:\n",
    "    unp = MyCustomUnpickler(handle)\n",
    "    tfidf_vectorizer = unp.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1651cbfa-8a02-4936-ae1b-31217291ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sentence = \"\"\"Last week, I had what I've been calling a \"pothole moment\"-- a flare of intense feeling that tripped me up. At our fall conference, my son's middle school guidance counselor suggested that the pandemic years would have been the ideal time for him to learn to type, even though, to my knowledge, during those years, his school never once mentioned typing. She told me that no, the school isn't going to circle back to teach it, though it's clearly increasingly necessary. But maybe, the counselor suggested brightly, I could just teach my son now?  As it turned out, the call to \"just\" do anything made my breath come short. I mean, didn't I just nearly break my own soul keeping my two kids from loneliness and despair for two tough years, making sure that they learned a few things for when the world woke up again? Didn't I teach swimming when there were no swim lessons, and every last game I knew during the long summer when there was no camp? Isn't it a miracle that either kid knows any math at all?  In front of the guidance counselor, I found myself growing tense. \"No,\" I said. \"I cannot 'just' teach anything right now.\" I heard the little snarl in my throat. Heart racing, I stepped back to find a new tone. In perhaps an overshare, or perhaps just what the moment needed, I told the woman in front of me that I'd just been swamped by sudden deep grief -- not about typing, but for all that's been broken, for the crazy years we've been through.  And, with a bit of grace, the counselor saw me. She was kind. She got it. \"It's normal,\" she said. \"We're all sort of that way.\"  So that's me, waving my hand, sharing my pothole. I think I'm not alone, that a lot of our roads these days can veer into the uneven without much warning. As we move towards a holiday season where many of us will gather with family and friends, it's interesting to notice all we're holding: The lingering pandemic and its aftereffects. The long shadow of a world at war. Grief about climate change. Grief about gun violence. Fear about the price of gas and groceries. And other rifts -- in our families, in our towns, in our country. Some of these feel like they will never be mended.  Yet alongside noting the general tenderness I find in myself and others, I've also been thinking about joy, what it is, how to find it. As it happens, I'm not alone. There are at least three books this fall that take up the call to think about joy and ask us to think with them. As the holidays approach, I've been reading them. I don't know about you, but sometimes the call to holiday joy puts me in an uneasy place. But these three books are actually about moving past what feels forced and fake towards what might feel genuine and alive, together.  In short, as we all begin to try to think about what it might mean to really be near one another, these books have a lot to offer now.  The books -- and this is not the start of a joke -- are actually by a poet, a psychoanalyst and a pastor, three lovely people who certainly should all walk into a bar together to chat. Poet Ross Gay's \"Inciting Joy\" makes the pursuit of joy feel positively radical. Poet and psychoanalyst Nuar Alsadir's \"Animal Joy\" is, among other things, an exploration of the unexpected art of really laughing. And in \"How to Begin When Your World Is Ending, A Spiritual Field Guide to Joy Despite Everything,\" pastor Molly Phinney Baskette -- who's lived through cancer and tended the griefs of her congregants -- doesn't quite offer a how-to manual, but in some ways, it comes close.  These three books share common wisdoms: That joy is not really about scripted, expected or store-bought things. It is more about the unscripted realm of deep connection, the willingness to be vulnerable, to be present with sorrow, to be honest, to be surprised. It's sharing the unexpected, rather than holding it back. It's also about the moments when, despite all, we feel met.  Tellingly, none of these books are about joy alone. There's an awful lot of suffering baked in -- the loss of parents, the loss of children, the loss of the selves we thought we were. Even what is funny can be upending: \"Every joke is a tiny revolution\" says Alsadir, quoting George Orwell, also noting that many standup routines could be tragedy in another tone.  As a poet and psychoanalyst, Alsadir is fascinated by the way moments that make us laugh and feel deeply happen when the tender and taboo parts of life see the light. When something -- our deep laughter, our authentic grief -- upends the narratives we'd rather keep steady, we also discover the possibility for transformation.  For Phinney Baskette, the pastor, the process of joy comes in trusting that these broken open moments -- where we may feel most lost -- are actually the places where we might just connect more deeply to ourselves, to others, and to the strength we need to go on. Joy emerges when we can call out to people -- as she did when she was presiding over Holy Week after discovering she had cancer and found herself unexpectedly asking three congregants to pray for her, instead.  Perhaps my favorite description of joy comes from Gay's book. In what's a very long metaphor, a shaggy dog parable of its own, Gay describes joy as the potluck we come to, to greet our old companion, Sorrow. The potluck is vast: \"We put our mechanic on the list, our chiropractor, and the neighbors we wave at but not much more than that.\"  And then it turns out that as we bring our dishes, and as we put out our index cards in front of them, saying whether they have nuts or gluten or dairy, as we greet each other, introducing our sorrows, something uproarious happens: The party gets joyous, and zany, and most importantly, real. Suddenly we are \"sweaty, stomping and shaking, tearing it up, the pictures falling off the walls, the books from the shelves, some logs ablaze even spilling from the stove, riotous this care, this carrying this excitement, this joy.\" Suddenly, in other words, having invited in our sorrows, we can finally dance at the party of our lives.  As we come to the Thanksgiving holiday, I can't help but think about this, about bringing our dishes to friends and loved ones, and bringing also what is real, hungry, and vulnerable too.  I had a glimpse of this lately, how good it could be. I was recently away for a weekend among some brilliant writer friends, some of the smartest women I know. Many of us haven't seen each other in years. We started in delight just to be together again. We shared new work, wine, ideas, hugs.  But later in the weekend, something deeper started to happen. We brought more of ourselves to the table. I heard from a friend with an eating disorder, one who had cancer during the pandemic, one losing her ex-husband to drug addiction, one who'd been shut out of a dream job, and is trying -- quite bravely -- to work around bitterness. I was able to talk about my potholes, my exhaustion, my long burnout and the feeling trying to chart my way in the dark.  We all laughed a lot. I cried in front of my friend Nicole. \"It's totally ok to be fragile,\" she said. \"We're all fragile now.\"  Yeah, and maybe we're ready to be joyful, too. It's winter. It's cold. The holidays are coming. We're about to try to find light in darkness. We're about to gather with a bunch of people we love, and those we try hard to.  Sometimes those people we try to love are the very same people who also make us feel, and sometimes the feelings are challenging. In other words, the holidays are full of potholes. The holidays are coming. The world is going to try to sell us the kind of joy we can buy in stores. What I'm hoping for also -- for each of us -- is that we find some togetherness, some real community. I'm hoping that we make it to that potluck, where we laugh and laugh and laugh.\"\"\"\n",
    "vec = tfidf_vectorizer.transform([_sentence]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "42118064-ab14-40ea-bbd0-557c6fd54acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4799,   5318,  10634, 104568, 108471, 108476, 112439, 113342,\n",
       "       116824, 125742, 127732, 127815, 128276, 131303, 133993, 134985,\n",
       "       138461, 140364, 143631, 144275, 145980, 146119, 150779, 152520,\n",
       "       152696, 154983, 156977, 159340, 161245, 164801, 173827, 174212,\n",
       "       179546, 183414, 183638, 183858, 185022, 185081, 185155, 185179,\n",
       "       185906, 186387, 188989, 189807, 191053, 192435, 193948, 194163,\n",
       "       194836, 195740, 197763, 197995, 199252, 204161, 205273, 205799,\n",
       "       207095, 207259, 209696, 210853, 213886, 216301, 216792, 217653,\n",
       "       220920, 222465, 222842, 223659, 223980, 224267, 226588, 227271,\n",
       "       227631, 227671, 234066, 234717, 237132, 237940, 245405, 246235,\n",
       "       247139, 247211, 248661, 252394, 252523, 252556, 254180, 257508,\n",
       "       257514, 258159, 258258, 264711, 264735, 265253, 265621, 268521,\n",
       "       272574, 273709, 278952, 282587, 286154, 287912, 295664, 296367,\n",
       "       296475, 297443, 298458, 298940, 299492, 300072, 303698, 303994,\n",
       "       304081, 304451, 305800, 306882, 307510, 308214, 308233, 310405,\n",
       "       311548, 311782, 314416, 317857, 319620, 320710, 323057, 323637,\n",
       "       324671, 325532, 328034, 329433, 329814, 330166, 331647, 332322,\n",
       "       332588, 333561, 336977, 337584, 337892, 338728, 339639, 340696,\n",
       "       341955, 343812, 344224, 344683, 345462, 346745, 346759, 347425,\n",
       "       352687, 353805, 353992, 354369, 358452, 358567, 360258, 367575,\n",
       "       367677, 368105, 368929, 369578, 371559, 372621, 373549, 377216,\n",
       "       377271, 381658, 383275, 383698, 384102, 388585, 389451, 389962,\n",
       "       391367, 392180, 405027, 406099, 407387, 407434, 407446, 415747,\n",
       "       415767, 419353, 420416, 422648, 422890, 422965, 433801, 434107,\n",
       "       434126, 434577, 434602, 434617, 434648, 436780, 436794, 436886,\n",
       "       439845, 442131, 442648, 442970, 444029, 444792, 445198, 445493,\n",
       "       447103, 447723, 447746, 449022, 449035, 449087, 449139, 449623,\n",
       "       449712, 449770, 454590, 457295, 457540, 460724, 460874, 465513,\n",
       "       466873, 466998, 469641, 470002, 473393, 473821, 475000, 475180,\n",
       "       477881, 481812, 486175, 486311, 491456, 491758, 493008, 501711,\n",
       "       504639, 504969, 505134, 505405, 505956, 507848, 508021, 510467,\n",
       "       516866, 517728, 519042, 519132, 521864, 526203, 528046, 528557,\n",
       "       530411, 532031, 535122, 536017, 539190, 543594, 544662, 545240,\n",
       "       546136, 546669, 547595, 547722, 551854, 553207, 556514, 570933,\n",
       "       573432, 576361, 580495, 580602, 582297, 582321, 583997, 586366,\n",
       "       587028, 587941, 590497, 595212, 597334, 597570, 600964, 601134,\n",
       "       602950, 603320, 606946, 609648, 609713, 609858, 610249, 610283,\n",
       "       611265, 623355, 625132, 625208, 626172, 627738, 630872, 631502,\n",
       "       638372, 644986, 645117, 645266, 647715, 649703, 650582, 651981,\n",
       "       652213, 653055, 654169, 658859, 658996, 659362, 660123, 660178,\n",
       "       660360, 661324, 663730, 665210, 672964, 673944, 676867, 676932,\n",
       "       677102, 678092, 678113, 678413, 681233, 681442, 684778, 685299,\n",
       "       685345, 687160, 688014, 689469, 689849, 689850, 690321, 691253,\n",
       "       694639, 694645, 694851, 695089, 695953, 698585, 698974, 700218,\n",
       "       700487, 700814, 700829, 703573, 703687, 704641, 705153, 708572,\n",
       "       708930, 710418, 710899, 710919, 710932, 711130, 713679, 717668,\n",
       "       717888, 717953, 719117, 719158, 719492, 720011, 721688, 722861,\n",
       "       724825, 724877, 725172, 725783, 727450, 727650, 727953, 728032,\n",
       "       729056, 729769, 732358, 734660, 734882, 734921, 736841, 736922,\n",
       "       738682, 739457, 739547, 740050, 745197, 745339, 745359, 745364,\n",
       "       747662, 748786, 748789, 749023, 754778, 755254, 759877, 763473,\n",
       "       765207, 765339, 766139, 766774, 768509, 768587, 768637, 770315,\n",
       "       770480, 773802, 776355, 776948, 777362, 777816, 778263, 778926,\n",
       "       779177, 780112, 780278, 780835, 782247, 787202, 787223, 788584,\n",
       "       792261])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vec > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "901fd7bb-77e2-4ee1-954d-d85d2f83a58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy    holiday pothole grief book potluck psychoanalyst sorrow feel poet counselor baskette phinney laugh teach moment pastor try  m friend congregant u typing pandemic gay find front come greet cancer three think feeling deep fragile sometimes lot alone loss dish perhaps also unexpected call long overshare gather guidance tone together aftereffect uproarious joke suddenly world vulnerable actually real upends noting school hoping sharing snarl broken way mended deeply riotous how to store bought people zany standup shaggy tenderness something gluten year chiropractor bringing really make togetherness tellingly happens parable   unscripted burnout one veer put stomping maybe towards sweaty upending suggested weekend tripped  just joyous kid holding brightly swamped joyful thing loneliness quite begin happen light uneven smartest bitterness stove exhaustion son fall heard orwell spilling ablaze trusting fascinated short know bravely ex husband scripted molly offer flare metaphor taboo back baked uneasy love mechanic rather tended presiding thanksgiving started lingering kind anything many swim even discovering delight dairy unexpectedly despair word emerges coming tearing inciting waving tender positively laughter despite companion cried rift trying manual realm racing darkness hug exploration authentic lovely miracle swimming woke excitement nicole log hungry shelf glimpse lately others laughed shaking math quoting nut though introducing tense throat wisdom spiritual grocery mean addiction awful laughing among transformation wine index pray pursuit breath woman steady chart discover found self chat ross willingness grace every party never genuine family brilliant importantly dance soul deeper place ideal sudden bunch yeah disorder holy shadow life description funny tragedy routine ok intense eating connect revolution tiny guide describes much stepped another challenging lesson going honest circle falling crazy alive work narrative invited winter ending loved neighbor surprised strength dog vast radical favorite dark totally suffering week dream wave alongside lived cold animal interesting normal climate card art carrying shut notice knowledge losing increasingly gas table mentioned sell tough learned camp store bar told fake possibility writer keeping reading ca walk thinking connection said summer sort none learn warning moving necessary gun asking buy last two season ready knew type approach certainly finally bit george shared break growing clearly forced field ask road parent drug present heart price town wall picture game violence new common fear turned middle met calling brought saw needed list everything conference either lost bring old near turn hard sure idea nearly expected care thought recently instead process full hand start open close able talk job war community little move seen away past share general yet got making child keep least change whether later saying without good help around see go part right made take may country get day like say time'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "_names = tfidf_vectorizer.get_feature_names_out()\n",
    "_chain = sorted(list(zip(vec[np.where(vec > 0)], _names[np.where(vec > 0)[0]])), key=lambda tup: -tup[0])[0:]\n",
    "\" \".join([re.sub(r'[^a-zA-Z0-9]', ' ', t[1]) for t in _chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e0808d4c-80a5-484e-b9f9-39703f31a214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foxconn worker zhengzhou yuan cnn bonus livestreams protest dormitory recruit'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_chainstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dd235963-3c9b-4a50-92bf-f5818380fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 -> 0      joy  holiday pothole grief book potluck psychoanalyst sorrow feel\n",
      "9 -> 0      joy  holiday pothole grief book potluck psychoanalyst sorrow\n",
      "8 -> 0      joy  holiday pothole grief book potluck psychoanalyst\n",
      "7 -> 0      joy  holiday pothole grief book potluck\n",
      "6 -> 0      joy  holiday pothole grief book\n",
      "5 -> 0      joy  holiday pothole grief\n",
      "4 -> 0      joy  holiday pothole\n",
      "3 -> 100      joy  holiday\n",
      "2 -> 1526      joy \n",
      "1 -> 1526      joy\n"
     ]
    }
   ],
   "source": [
    "for _range in range(0,10):\n",
    "    _names = tfidf_vectorizer.get_feature_names_out()\n",
    "    _chain = sorted(list(zip(vec[np.where(vec > 0)], _names[np.where(vec > 0)[0]])), key=lambda tup: -tup[0])[:10-_range]\n",
    "    _chainstr = \" \".join([re.sub(r'[^a-zA-Z0-9]', '', t[1]) for t in _chain])\n",
    "\n",
    "    conn = sqlite3.connect('../../pickles/swarog_data/swarog.sqlite')\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"select rowid,label, dataset, body from rawsearch where body match '{_chainstr}'\"\"\")\n",
    "    r=c.fetchall()\n",
    "    print(10-_range,\"->\",len(r), \" \", _chainstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8efcb0-d87f-4e41-bf12-101649eeeba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "select rowid from rawsearch where body match 'cough' limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e9614adb-3365-421f-990f-c50231e71441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\x10the', '\\x10there', '!', '#', '$', '%', '&', \"'+currentpage\",\n",
       "       \"'-\", \"'-et\", \"'-ratgeber\"], dtype=object)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_names[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f5180c65-7f83-4ee5-b7a1-32b229a98dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5850309101334102, 'covid')\n",
      "(0.47071270874709104, 'coffee')\n",
      "(0.4392921130423506, 'drinking')\n",
      "(0.3728242280386614, 'affect')\n",
      "(0.32278927246519923, 'test')\n",
      "[(1, 'mmcovid_en', 10308), (0, 'mmcovid_en', 12148), (0, 'mmcovid_en', 12155), (0, 'mmcovid_en', 12174), (0, 'mmcovid_en', 12178), (0, 'mmcovid_en', 12180), (1, 'pubhealth', 17024), (1, 'pubhealth', 19212), (0, 'pubhealth', 19401), (0, 'qprop', 36254)]\n"
     ]
    }
   ],
   "source": [
    "_sentence = \"\"\"drinking coffee affect a covid test\"\"\"\n",
    "vec = tfidf_vectorizer.transform([_sentence]).toarray()[0]\n",
    "\n",
    "_names = tfidf_vectorizer.get_feature_names_out()\n",
    "_words = sorted(list(zip(vec[np.where(vec > 0)], _names[np.where(vec > 0)[0]])), key=lambda tup: -tup[0])\n",
    "\n",
    "conn = sqlite3.connect('../../pickles/swarog_data/swarog.sqlite')\n",
    "_resp = []\n",
    "for _range in range(0,min(20,len(_words))):\n",
    "    x=_words[_range]\n",
    "    _chain = (x[0],re.sub(r'[^a-zA-Z0-9]', '', x[1]))\n",
    "    \n",
    "    if len(_chain[1]) == 0:\n",
    "        continue\n",
    "\n",
    "    print(_chain)\n",
    "    \n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"select rowid from rawsearch where body match '{_chain[1]}' limit 1000\"\"\")\n",
    "    r=[x[0] for x in c.fetchall()]\n",
    "    _resp.extend(r)\n",
    "    # print(10-_range,\"->\",len(r), \"    \", _chainstr)\n",
    "    \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "ctr=Counter(_resp)\n",
    "common = ctr.most_common(10)\n",
    "\n",
    "#print(common)\n",
    "ids = [str(x[0]) for x in common]\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute(f\"\"\"select label, dataset, rowid from raw where rowid IN ({\",\".join(ids)})\"\"\")\n",
    "print( c.fetchall())\n",
    "conn.close()\n",
    "#_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b598498a-7c00-464a-9eaa-1ca76da3d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3535,)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_resp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "bd7f0019-983f-4bc8-b0d6-db318ebee039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19311, 6),\n",
       " (12039, 5),\n",
       " (10308, 5),\n",
       " (16952, 5),\n",
       " (16740, 5),\n",
       " (33780, 5),\n",
       " (12013, 5),\n",
       " (17736, 5),\n",
       " (16864, 5),\n",
       " (10297, 5)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9d877490-5b56-4134-9c4d-1f6d75e8161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['breath', 'cough', 'covid-19', 'dry', 'hallmark', 'shortness',\n",
       "       'sign'], dtype=object)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()[np.where(vec > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "304adb55-e81a-4531-9f62-fad58bbd43c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tfidf annoy index...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([732, 3465, 4416, 5744, 8478, 34, 172, 201, 423, 802],\n",
       " [4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_related_articles(txt):\n",
    "    vec = tfidf_vectorizer.transform([txt]).toarray()[0]\n",
    "    vec[np.where(vec > 0)]=1\n",
    "    hits = tfidf.get_nns_by_vector(vec, 10, search_k=10000000000, include_distances=True)\n",
    "    print(\"-->\",hits)\n",
    "    resp = []\n",
    "    conn = sqlite3.connect('../../pickles/swarog_data/swarog.sqlite')\n",
    "    c = conn.cursor()\n",
    "    for index,_id in enumerate(hits[0]):\n",
    "        c.execute(\"\"\"select raw.body,raw.label, raw.dataset \n",
    "                from raw join tfidf on (tfidf.gid=raw.rowid) \n",
    "                where tfidf.rowid=?\"\"\",[_id+1])\n",
    "        r=c.fetchall()\n",
    "        resp.append({\n",
    "            'text':r[0][0],\n",
    "            'label':r[0][1],\n",
    "            'dataset':r[0][2],\n",
    "            'distance':hits[1][index]})\n",
    "    conn.close()\n",
    "    return resp\n",
    "\n",
    "\n",
    "\n",
    "tfidf = AnnoyIndex(7000, 'hamming')\n",
    "print(\"load tfidf annoy index...\")\n",
    "tfidf.load('../../pickles/swarog_data/swarog_tfidf.ann')\n",
    "\n",
    "\n",
    "hits = tfidf.get_nns_by_vector(vec, 10, search_k=1000000, include_distances=True)\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f1ca0462-a607-4edb-ac2d-e791278c5415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> ([732, 4416, 5744, 8478, 172, 1978, 2071, 3130, 3377, 3941], [4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'COVID-19 is mutating and is now more contagious than before.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 4.0},\n",
       " {'text': 'COVID-19 is transmitted by farting',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 4.0},\n",
       " {'text': 'Eucalyptus prevents or eliminates COVID-19.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 4.0},\n",
       " {'text': 'If you have a runny nose or cough, you donâ€™t have COVID-19.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 4.0},\n",
       " {'text': 'Authorities discreetly re-authorised hydroxychloroquine for COVID-19',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 5.0},\n",
       " {'text': 'Aspirin cures COVID-19.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 5.0},\n",
       " {'text': 'Hydroxychloroquine cures COVID-19.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 5.0},\n",
       " {'text': 'Ozone serves as a disinfectant against COVID-19.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 5.0},\n",
       " {'text': 'Nicotine protects from COVID-19.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 5.0},\n",
       " {'text': 'COVID-19 is a thrombosis and is cured with anticoagulant, anti-inflammatory or antibiotic aspirin.',\n",
       "  'label': 1,\n",
       "  'dataset': 'covid_fake_news',\n",
       "  'distance': 5.0}]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_articles(\"dry cough, and shortness of breath are hallmark signs COVID-19\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env.hator",
   "language": "python",
   "name": "env.hator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

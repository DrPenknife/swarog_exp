{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE raw\n",
    "             (dataset TEXT, id INT, body TEXT, label INT)''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE bert\n",
    "             (dataset TEXT, id INT, vec TEXT)''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE bertnp\n",
    "             (dataset TEXT, id INT, vec BLOB)''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "compressor = 'zlib'\n",
    "\n",
    "def adapt_array(arr):\n",
    "    \"\"\"\n",
    "    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "    \"\"\"\n",
    "    # zlib uses similar disk size that Matlab v5 .mat files\n",
    "    # bz2 compress 4 times zlib, but storing process is 20 times slower.\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(codecs.encode(out.read(),compressor))  # zlib, bz2\n",
    "\n",
    "def convert_array(text):\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    out = io.BytesIO(codecs.decode(out.read(),compressor))\n",
    "    return np.load(out)\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "# sqlite3.register_converter(\"array\", convert_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE bertnp\n",
    "             (dataset TEXT, id INT, vec array)''')\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8972/8972 [00:22<00:00, 400.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7332/7332 [00:19<00:00, 379.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10075/10075 [00:27<00:00, 364.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 51270/51270 [02:50<00:00, 299.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 44898/44898 [02:41<00:00, 277.39it/s]\n",
      "/tmp/ipykernel_4157/3735554623.py:39: DtypeWarning: Columns (3,5,6,9,10,11,12,13,14,16,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"../raw/grafn.csv\",sep=\",\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 63930/63930 [03:17<00:00, 323.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "conn = sqlite3.connect('swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "data = pd.read_csv(\"../raw/covid_fake_news.csv\",sep=\"\\t\")\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    values_to_insert = [('covid_fake_news', index, row['text'], row['label'])]\n",
    "    c.executemany(\"\"\"INSERT INTO raw(dataset, id, body, label) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "data = pd.read_csv(\"../raw/mmcovid_en.csv\",sep=\",\")\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    values_to_insert = [('mmcovid_en', index, row['text'], row['label'])]\n",
    "    c.executemany(\"\"\"INSERT INTO raw(dataset, id, body, label) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "\n",
    "data = pd.read_csv(\"../raw/pubhealth.csv\",sep=\",\")\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    values_to_insert = [('pubhealth', index, row['text'], row['label'])]\n",
    "    c.executemany(\"\"\"INSERT INTO raw(dataset, id, body, label) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "\n",
    "data = pd.read_csv(\"../raw/qprop.csv\",sep=\"\\t\")\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    values_to_insert = [('qprop', index, row['text'], row['label'])]\n",
    "    c.executemany(\"\"\"INSERT INTO raw(dataset, id, body, label) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "    \n",
    "data = pd.read_csv(\"../raw/isot.csv\",sep=\",\")\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    values_to_insert = [('isot', index, row['text'], row['label'])]\n",
    "    c.executemany(\"\"\"INSERT INTO raw(dataset, id, body, label) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "data = pd.read_csv(\"../raw/grafn.csv\",sep=\",\")\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    values_to_insert = [('grafn', index, row['text'], row['label'])]\n",
    "    c.executemany(\"\"\"INSERT INTO raw(dataset, id, body, label) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import bert CLS vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8972/8972 [00:34<00:00, 259.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7332/7332 [00:18<00:00, 392.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10075/10075 [00:36<00:00, 275.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 51270/51270 [02:57<00:00, 288.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 44898/44898 [02:15<00:00, 330.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 63930/63930 [03:29<00:00, 304.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import json\n",
    "import codecs\n",
    "\n",
    "conn = sqlite3.connect('swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "\n",
    "with open('../covid_fake_news_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    for index, row in tqdm(enumerate(dst),total=len(dst)):\n",
    "        values_to_insert = [('covid_fake_news', index, adapt_array(row))]\n",
    "        c.executemany(\"\"\"INSERT INTO bertnp(dataset, id, vec) VALUES (?,?,?)\"\"\", values_to_insert)\n",
    "        conn.commit()\n",
    "        \n",
    "        \n",
    "with open('../mmcovid_en_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    for index, row in tqdm(enumerate(dst),total=len(dst)):\n",
    "        values_to_insert = [('mmcovid_en', index, adapt_array(row))]\n",
    "        c.executemany(\"\"\"INSERT INTO bertnp(dataset, id, vec) VALUES (?,?,?)\"\"\", values_to_insert)\n",
    "        conn.commit()\n",
    "        \n",
    "with open('../pubhealth_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    for index, row in tqdm(enumerate(dst),total=len(dst)):\n",
    "        values_to_insert = [('pubhealth', index, adapt_array(row))]\n",
    "        c.executemany(\"\"\"INSERT INTO bertnp(dataset, id, vec) VALUES (?,?,?)\"\"\", values_to_insert)\n",
    "        conn.commit()\n",
    "        \n",
    "with open('../qprop_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    for index, row in tqdm(enumerate(dst),total=len(dst)):\n",
    "        values_to_insert = [('qprop', index, adapt_array(row))]\n",
    "        c.executemany(\"\"\"INSERT INTO bertnp(dataset, id, vec) VALUES (?,?,?)\"\"\", values_to_insert)\n",
    "        conn.commit()\n",
    "\n",
    "with open('../isot_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    for index, row in tqdm(enumerate(dst),total=len(dst)):\n",
    "        values_to_insert = [('isot', index, adapt_array(row))]\n",
    "        c.executemany(\"\"\"INSERT INTO bertnp(dataset, id, vec) VALUES (?,?,?)\"\"\", values_to_insert)\n",
    "        conn.commit()\n",
    "                \n",
    "with open('../grafn_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    for index, row in tqdm(enumerate(dst),total=len(dst)):\n",
    "        values_to_insert = [('grafn', index, adapt_array(row))]\n",
    "        c.executemany(\"\"\"INSERT INTO bertnp(dataset, id, vec) VALUES (?,?,?)\"\"\", values_to_insert)\n",
    "        conn.commit()\n",
    "        \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-pickle `TF-IDF` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA = {\n",
    "    \"X\" : [],\n",
    "    \"category\" : [],\n",
    "    \"y\" : []\n",
    "}\n",
    "\n",
    "X=DATA[\"X\"]\n",
    "category=DATA[\"category\"]\n",
    "y=DATA[\"y\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-pickle bert `CLS` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6542/996138349.py:57: DtypeWarning: Columns (3,5,6,9,10,11,12,13,14,16,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"../raw/grafn.csv\",sep=\",\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA = {\n",
    "    \"X\" : [],\n",
    "    \"category\" : [],\n",
    "    \"y\" : []\n",
    "}\n",
    "\n",
    "X=DATA[\"X\"]\n",
    "category=DATA[\"category\"]\n",
    "y=DATA[\"y\"]\n",
    "\n",
    "\n",
    "with open('../covid_fake_news_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(0,len(dst)))\n",
    "    data = pd.read_csv(\"../raw/covid_fake_news.csv\",sep=\"\\t\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "with open('../mmcovid_en_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(1,len(dst)))\n",
    "    data = pd.read_csv(\"../raw/mmcovid_en.csv\",sep=\",\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "\n",
    "with open('../pubhealth_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(2,len(dst)))\n",
    "    data = pd.read_csv(\"../raw/pubhealth.csv\",sep=\",\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "\n",
    "    \n",
    "with open('../qprop_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(3,len(dst)))\n",
    "    data = pd.read_csv(\"../raw/qprop.csv\",sep=\"\\t\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "with open('../isot_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(4,len(dst)))\n",
    "    data = pd.read_csv(\"../raw/isot.csv\",sep=\",\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "with open('../grafn_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(5,len(dst)))\n",
    "    data = pd.read_csv(\"../raw/grafn.csv\",sep=\",\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "    \n",
    "DATA[\"X\"]=np.array(X)\n",
    "DATA[\"category\"]=np.array(category)\n",
    "DATA[\"y\"]=np.array(y)\n",
    "DATA[\"folds\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5x2 CV experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def experiment(foldids, X, y, cls = LogisticRegression(max_iter=10000), fit=True):\n",
    "\n",
    "    scores = {\n",
    "        'Accuracy': {'func': accuracy_score},\n",
    "        'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "        'F1': {'func': f1_score},\n",
    "        'Precision': {'func': precision_score},\n",
    "        'Recall': {'func': recall_score},\n",
    "        'G-mean': {'func': geometric_mean_score}\n",
    "    }\n",
    "\n",
    "    for score_name, score_dict in scores.items():\n",
    "        scores[score_name][\"list\"] = []\n",
    "        scores[score_name][\"lab\"] = []\n",
    "\n",
    "    for fold,j in enumerate(foldids):\n",
    "        train = foldids[fold][1]\n",
    "        test = foldids[fold][2]\n",
    "        xin, yin = X[train], np.array(y[train])\n",
    "        \n",
    "        pca = PCA(n_components=512)\n",
    "        pca.fit(xin)\n",
    "        \n",
    "        \n",
    "        if fit == True:\n",
    "            cls.fit(pca.transform(xin), yin)\n",
    "        y_pred = cls.predict(pca.transform(X[test]))\n",
    "        for score_name, score_dict in scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                scorvaln = score_dict['func'](y[test], y_pred, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](y[test], y_pred, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](y[test], y_pred)\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval)\n",
    "        #print(\" \")\n",
    "\n",
    "    #clear_output()\n",
    "    for score_name, score_dict in scores.items():\n",
    "        score_dict['avg'] = np.mean(score_dict['list'])\n",
    "        score_dict['std'] = np.std(score_dict['list'])\n",
    " \n",
    "    # Print stats\n",
    "    numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "    scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "    head = \"| %-20s | %-10s |\" +  numlabels * \" %-10s |\" \n",
    "    headv = [\"Score\", \"Average\"]\n",
    "    headv.extend([\"Kat_\"+str(i+1) for i in range(numlabels)])\n",
    "    row=head % tuple(headv)\n",
    "    print(\"+\"*len(row))\n",
    "    print(row)\n",
    "    print(\"+\"*len(row))\n",
    "    for score_name, score_dict in sorted(scores.items()) :\n",
    "        headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "        for i in range(numlabels):\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels* \" %4.1f ± %3.1f |\"\n",
    "                vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                headv.append(np.mean(vals)*100)\n",
    "                headv.append(np.std(vals)*100)\n",
    "            else:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels * \" %-10s |\" \n",
    "                headv.append(\"-\")\n",
    "        print(head % tuple(headv))\n",
    "    print(\"+\"*len(row))\n",
    "    return cls, scores, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording test/train folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 0\n",
      "TASK 1\n",
      "TASK 2\n",
      "TASK 3\n",
      "TASK 4\n",
      "TASK 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATA[\"folds\"] = []\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "\n",
    "\n",
    "for t in range(np.max(DATA[\"category\"]+1)):\n",
    "    print(\"TASK\",t)\n",
    "    X = DATA[\"X\"][DATA[\"category\"] == t]\n",
    "    y = DATA[\"y\"][DATA[\"category\"] == t]\n",
    "\n",
    "    foldids = []\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, y)):\n",
    "        foldids.append((fold_idx,train,test))\n",
    "\n",
    "    #print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "    DATA[\"folds\"].append(foldids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px dashed red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Task Performance (BERT `CLS`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 97.2 ± 0.1 | -          | -          |\n",
      "| Balanced Accuracy    | 79.3 ± 1.3 | -          | -          |\n",
      "| F1                   | 97.0 ± 0.1 | 68.7 ± 1.6 | 98.5 ± 0.1 |\n",
      "| G-mean               | 76.8 ± 1.7 | -          | -          |\n",
      "| Precision            | 97.0 ± 0.2 | 81.8 ± 3.5 | 97.8 ± 0.1 |\n",
      "| Recall               | 97.2 ± 0.1 | 59.4 ± 2.7 | 99.3 ± 0.2 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "TASK 1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 92.6 ± 0.2 | -          | -          |\n",
      "| Balanced Accuracy    | 88.2 ± 0.7 | -          | -          |\n",
      "| F1                   | 92.4 ± 0.3 | 95.1 ± 0.1 | 85.5 ± 0.7 |\n",
      "| G-mean               | 87.7 ± 0.8 | -          | -          |\n",
      "| Precision            | 92.7 ± 0.2 | 92.3 ± 0.6 | 93.9 ± 1.1 |\n",
      "| Recall               | 92.6 ± 0.2 | 98.0 ± 0.4 | 78.5 ± 1.8 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "TASK 2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 81.7 ± 0.4 | -          | -          |\n",
      "| Balanced Accuracy    | 80.4 ± 0.5 | -          | -          |\n",
      "| F1                   | 81.7 ± 0.4 | 75.5 ± 0.6 | 85.4 ± 0.3 |\n",
      "| G-mean               | 80.2 ± 0.5 | -          | -          |\n",
      "| Precision            | 81.7 ± 0.4 | 75.8 ± 0.6 | 85.2 ± 0.5 |\n",
      "| Recall               | 81.7 ± 0.4 | 75.2 ± 1.1 | 85.6 ± 0.6 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "TASK 3\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 94.0 ± 0.1 | -          | -          |\n",
      "| Balanced Accuracy    | 80.5 ± 0.4 | -          | -          |\n",
      "| F1                   | 93.7 ± 0.1 | 96.7 ± 0.1 | 70.3 ± 0.6 |\n",
      "| G-mean               | 78.5 ± 0.5 | -          | -          |\n",
      "| Precision            | 93.7 ± 0.1 | 95.5 ± 0.1 | 79.5 ± 0.8 |\n",
      "| Recall               | 94.0 ± 0.1 | 98.0 ± 0.1 | 63.0 ± 0.9 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "TASK 4\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 99.7 ± 0.0 | -          | -          |\n",
      "| Balanced Accuracy    | 99.7 ± 0.0 | -          | -          |\n",
      "| F1                   | 99.7 ± 0.0 | 99.7 ± 0.0 | 99.7 ± 0.0 |\n",
      "| G-mean               | 99.7 ± 0.0 | -          | -          |\n",
      "| Precision            | 99.7 ± 0.0 | 99.6 ± 0.1 | 99.7 ± 0.0 |\n",
      "| Recall               | 99.7 ± 0.0 | 99.7 ± 0.0 | 99.6 ± 0.1 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "TASK 5\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 91.5 ± 0.1 | -          | -          |\n",
      "| Balanced Accuracy    | 85.0 ± 0.2 | -          | -          |\n",
      "| F1                   | 91.3 ± 0.1 | 94.7 ± 0.1 | 78.0 ± 0.2 |\n",
      "| G-mean               | 84.3 ± 0.2 | -          | -          |\n",
      "| Precision            | 91.3 ± 0.1 | 93.6 ± 0.1 | 82.4 ± 0.3 |\n",
      "| Recall               | 91.5 ± 0.1 | 96.0 ± 0.1 | 74.1 ± 0.3 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATA[\"folds\"] = []\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "\n",
    "stl=[]\n",
    "\n",
    "for t in range(np.max(DATA[\"category\"]+1)):\n",
    "    print(\"TASK\",t)\n",
    "    X = DATA[\"X\"][DATA[\"category\"] == t]\n",
    "    y = DATA[\"y\"][DATA[\"category\"] == t]\n",
    "\n",
    "    foldids = []\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, y)):\n",
    "        foldids.append((fold_idx,train,test))\n",
    "\n",
    "    #print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "    DATA[\"folds\"].append(foldids)\n",
    "    model1, scores1, pca = experiment(foldids, X, y, \n",
    "                                 LogisticRegression(max_iter=10000))\n",
    "    stl.append((model1,pca,scores1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px dashed red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (186477, 768) y (186477,)\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      | Kat_3      | Kat_4      | Kat_5      | Kat_6      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 79.6 ± 0.1 | -          | -          | -          | -          | -          | -          |\n",
      "| Balanced Accuracy    | 85.7 ± 0.1 | -          | -          | -          | -          | -          | -          |\n",
      "| F1                   | 79.6 ± 0.1 | 96.7 ± 0.2 | 82.8 ± 0.4 | 72.8 ± 0.5 | 72.8 ± 0.2 | 87.2 ± 0.1 | 77.9 ± 0.1 |\n",
      "| G-mean               | 85.1 ± 0.1 | -          | -          | -          | -          | -          | -          |\n",
      "| Precision            | 80.2 ± 0.1 | 95.2 ± 0.3 | 75.4 ± 0.6 | 61.5 ± 0.7 | 73.1 ± 0.2 | 85.0 ± 0.2 | 83.8 ± 0.3 |\n",
      "| Recall               | 79.6 ± 0.1 | 98.2 ± 0.2 | 91.9 ± 0.5 | 89.3 ± 0.4 | 72.4 ± 0.4 | 89.5 ± 0.2 | 72.9 ± 0.3 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "X = DATA[\"X\"]\n",
    "y = DATA[\"y\"]\n",
    "category = DATA[\"category\"]\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in enumerate(rskf.split(X, y)):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", category.shape)\n",
    "\n",
    "domain_model, domain_scores, domain_pca = experiment(foldids, X, \n",
    "                                         category, LogisticRegression(max_iter=10000, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px dashed red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global model with category pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def cls_max_predict(X_test):\n",
    "    ypred = np.array([stl[t][0].predict(stl[t][1].transform(X_test)) for t in range(6)])\n",
    "    return np.mean(ypred, axis=0) > 0.5\n",
    "\n",
    "def cls_weighted_predict(X_test):\n",
    "    ypred = np.array([stl[t][0].predict(stl[t][1].transform(X_test)) for t in range(6)])\n",
    "    return np.average(ypred, weights=[78,83,74,73,90,73], axis=0) > 0.4\n",
    "\n",
    "def cls_max_predict_raw(X_test):\n",
    "    ypred = np.array([stl[t][0].predict(stl[t][1].transform(X_test)) for t in range(6)])\n",
    "    return ypred\n",
    "\n",
    "def cls_predict(X_test):\n",
    "    Xd=domain_pca.transform(X_test)\n",
    "    domain_pred = domain_model.predict(Xd)\n",
    "    ypred = []\n",
    "    for i,dpred in enumerate(domain_pred):\n",
    "        model, model_pca, _ = stl[dpred] \n",
    "        xpca = model_pca.transform(X_test[i:i+1])\n",
    "        ypred.append(model.predict(xpca)[0])\n",
    "    return ypred\n",
    "        \n",
    "def experiment2(foldids, X, y, cback):\n",
    "\n",
    "    scores = {\n",
    "        'Accuracy': {'func': accuracy_score},\n",
    "        'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "        'F1': {'func': f1_score},\n",
    "        'Precision': {'func': precision_score},\n",
    "        'Recall': {'func': recall_score},\n",
    "        'G-mean': {'func': geometric_mean_score}\n",
    "    }\n",
    "\n",
    "    for score_name, score_dict in scores.items():\n",
    "        scores[score_name][\"list\"] = []\n",
    "        scores[score_name][\"lab\"] = []\n",
    "\n",
    "    for fold,j in enumerate(foldids):\n",
    "        train = foldids[fold][1]\n",
    "        test = foldids[fold][2]\n",
    "        xin, yin = X[train], np.array(y[train])\n",
    "        \n",
    "        y_pred = cback(X[test])\n",
    "        \n",
    "        for score_name, score_dict in scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                scorvaln = score_dict['func'](y[test], y_pred, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](y[test], y_pred, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](y[test], y_pred)\n",
    "                score_dict['list'].append(scorval)\n",
    "                print(score_name, scorval)\n",
    "        print(\" \")\n",
    "\n",
    "    clear_output()\n",
    "    for score_name, score_dict in scores.items():\n",
    "        score_dict['avg'] = np.mean(score_dict['list'])\n",
    "        score_dict['std'] = np.std(score_dict['list'])\n",
    " \n",
    "    # Print stats\n",
    "    numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "    scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "    head = \"| %-20s | %-10s |\" +  numlabels * \" %-10s |\" \n",
    "    headv = [\"Score\", \"Average\"]\n",
    "    headv.extend([\"Kat_\"+str(i+1) for i in range(numlabels)])\n",
    "    row=head % tuple(headv)\n",
    "    print(\"+\"*len(row))\n",
    "    print(row)\n",
    "    print(\"+\"*len(row))\n",
    "    for score_name, score_dict in sorted(scores.items()) :\n",
    "        headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "        for i in range(numlabels):\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels* \" %4.1f ± %3.1f |\"\n",
    "                vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                headv.append(np.mean(vals)*100)\n",
    "                headv.append(np.std(vals)*100)\n",
    "            else:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels * \" %-10s |\" \n",
    "                headv.append(\"-\")\n",
    "        print(head % tuple(headv))\n",
    "    print(\"+\"*len(row))\n",
    "    return scores, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 169.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (186477, 768) y (186477,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8906895183346025\n",
      "Balanced Accuracy 0.8738772228942854\n",
      "F1 0.890706375178059 [0.9199887  0.82753486]\n",
      "Precision 0.8907234951046911 [0.92023432 0.82705902]\n",
      "Recall 0.8906895183346025 [0.9197432  0.82801124]\n",
      "G-mean 0.8726727407962598\n",
      " \n",
      "Accuracy 0.8902378858405371\n",
      "Balanced Accuracy 0.8740442812747655\n",
      "F1 0.8903299506300135 [0.919563  0.8272626]\n",
      "Precision 0.8904298659660493 [0.92090929 0.82467358]\n",
      "Recall 0.8902378858405371 [0.91822063 0.82986793]\n",
      "G-mean 0.8729271766294437\n",
      " \n",
      "Accuracy 0.8901210866697412\n",
      "Balanced Accuracy 0.8736792558319861\n",
      "F1 0.8901881059739745 [0.9195088 0.8269338]\n",
      "Precision 0.8902592631467223 [0.92048511 0.82505225]\n",
      "Recall 0.8901210866697412 [0.91853456 0.82882395]\n",
      "G-mean 0.8725270445120594\n",
      " \n",
      "Accuracy 0.8908063236019649\n",
      "Balanced Accuracy 0.8742422345763926\n",
      "F1 0.8908483410282234 [0.92004304 0.82786372]\n",
      "Precision 0.8908920018230735 [0.92065762 0.82667567]\n",
      "Recall 0.8908063236019649 [0.91942927 0.8290552 ]\n",
      "G-mean 0.8730736603814897\n",
      " \n",
      "Accuracy 0.8904535655680563\n",
      "Balanced Accuracy 0.8735410651778874\n",
      "F1 0.8904635080996898 [0.9198248  0.82712163]\n",
      "Precision 0.8904735416496804 [0.91996922 0.82684173]\n",
      "Recall 0.8904535655680563 [0.91968042 0.82740171]\n",
      "G-mean 0.8723217025699785\n",
      " \n",
      "Accuracy 0.8904845663785153\n",
      "Balanced Accuracy 0.8743973812452943\n",
      "F1 0.8905837749318338 [0.91973431 0.82769444]\n",
      "Precision 0.8906921527351259 [0.92118979 0.82489657]\n",
      "Recall 0.8904845663785153 [0.91828342 0.83051134]\n",
      "G-mean 0.8732953658976153\n",
      " \n",
      "Accuracy 0.890206887675758\n",
      "Balanced Accuracy 0.8735876304050934\n",
      "F1 0.890253094942496 [0.91959819 0.82694616]\n",
      "Precision 0.8903012673833978 [0.92027038 0.82564812]\n",
      "Recall 0.890206887675758 [0.91892698 0.82824828]\n",
      "G-mean 0.8724102770111375\n",
      " \n",
      "Accuracy 0.8907312469164933\n",
      "Balanced Accuracy 0.874350801684055\n",
      "F1 0.8907944592678206 [0.91996103 0.82787051]\n",
      "Precision 0.890861395461057 [0.92088707 0.82608402]\n",
      "Recall 0.8907312469164933 [0.91903686 0.82966475]\n",
      "G-mean 0.8732081544432301\n",
      " \n",
      "Accuracy 0.8914295520114973\n",
      "Balanced Accuracy 0.874654915501095\n",
      "F1 0.8914379288329449 [0.92054098 0.82865316]\n",
      "Precision 0.89144637151626 [0.92066383 0.82841478]\n",
      "Recall 0.8914295520114973 [0.92041816 0.82889167]\n",
      "G-mean 0.8734568946779143\n",
      " \n",
      "Accuracy 0.8895085694673845\n",
      "Balanced Accuracy 0.8732835056941879\n",
      "F1 0.8896101443677675 [0.91901707 0.82616766]\n",
      "Precision 0.8897211498155903 [0.9204932  0.82333356]\n",
      "Recall 0.8895085694673845 [0.91754568 0.82902133]\n",
      "G-mean 0.872161075430089\n",
      " \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 89.0 ± 0.0 | -          | -          |\n",
      "| Balanced Accuracy    | 87.4 ± 0.0 | -          | -          |\n",
      "| F1                   | 89.1 ± 0.0 | 92.0 ± 0.0 | 82.7 ± 0.1 |\n",
      "| G-mean               | 87.3 ± 0.0 | -          | -          |\n",
      "| Precision            | 89.1 ± 0.0 | 92.1 ± 0.0 | 82.6 ± 0.1 |\n",
      "| Recall               | 89.0 ± 0.0 | 91.9 ± 0.1 | 82.9 ± 0.1 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "X = DATA[\"X\"]\n",
    "y = DATA[\"y\"]\n",
    "\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "\n",
    "gscr, gpca = experiment2(foldids, X, y, cls_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe_domain = Pipeline([('pca', domain_pca), ('head', domain_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# SAVE\n",
    "with open('domain_cls.pickle', 'wb') as handle:\n",
    "    pickle.dump(pipe_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_model_pipe = []\n",
    "for i,elem in enumerate(stl):\n",
    "    with open(f'model_{i}.pickle', 'wb') as handle:\n",
    "        p = Pipeline([('pca', elem[1]), ('head', elem[0])])\n",
    "        domain_model_pipe.append(p)\n",
    "        pickle.dump(p, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-pickle models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "# load\n",
    "with open('../../pickles/models/domain_cls.pickle', 'rb') as handle:\n",
    "    pipe_domain = pickle.load(handle)\n",
    "    \n",
    "domain_model_pipe = []\n",
    "for i in range(6):\n",
    "    with open(f'../../pickles/models/model_{i}.pickle', 'rb') as handle:\n",
    "        p=pickle.load(handle)\n",
    "        domain_model_pipe.append(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-537d6c1da7af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcls_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cls_predict' is not defined"
     ]
    }
   ],
   "source": [
    "cls_predict(X[0:2]), y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: Model(tag=\"big_puppy:wo3hgzkt6wg6itgm\")\n"
     ]
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "saved_model = bentoml.sklearn.save_model(\"big_puppy\", bp)\n",
    "print(f\"Model saved: {saved_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENTO models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: Model(tag=\"domain_cls:4ljhzhc27sjskdg5\")\n"
     ]
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "saved_model = bentoml.sklearn.save_model(\"domain_cls\", pipe_domain, signatures={\n",
    "        \"predict\": {\"batchable\": True, \"batch_dim\": 0},\n",
    "        \"predict_proba\": {\"batchable\": True, \"batch_dim\": 0},\n",
    "    })\n",
    "print(f\"Model saved: {saved_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: Model(tag=\"model_0:4zhlxcc27sxigdg5\")\n",
      "Model saved: Model(tag=\"model_1:426ssdc27s5j2dg5\")\n",
      "Model saved: Model(tag=\"model_2:44j2ors27sxkedg5\")\n",
      "Model saved: Model(tag=\"model_3:45jvync27svgwdg5\")\n",
      "Model saved: Model(tag=\"model_4:46tsvgs27s3rgdg5\")\n",
      "Model saved: Model(tag=\"model_5:476kxhc27sbbcdg5\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    saved_model = bentoml.sklearn.save_model(f\"model_{i}\", domain_model_pipe[i], signatures={\n",
    "        \"predict\": {\"batchable\": True, \"batch_dim\": 0},\n",
    "        \"predict_proba\": {\"batchable\": True, \"batch_dim\": 0},\n",
    "    })\n",
    "    print(f\"Model saved: {saved_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check rest api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"result\":1,\"result_proba\":[0.12162435873841593,0.8783756412615841],\"domain\":5,\"domain_proba\":[3.7271139151509206e-05,0.2524385728679994,0.0001431362617570179,0.011453838931336738,0.350516113051998,0.38541106774775735],\"similar_articles\":[{\"text\":\"The Most Rev. Michael Bruce Curry, the 27th Presiding Bishop and Primate of the Episcopal Church, delivered the address titled “The Power of Love” at Prince Harry and Meghan Markle’s wedding Saturday at St. George’s Chapel at Windsor Castle.  The transcript is below:  i  Dr. Martin Luther King, Jr. once said:  \\\\\"We must discover the power of love,  the redemptive power of love.  And when we discover that, we will be able to make of this old world  a new world.  Love is the only way.\\\\\"  There\\'s power in love. Do not underestimate it.  Anyone who has ever fallen in love,  knows what I mean.  But think about love in any form or experience of it.  It actually feels good to be loved, and to express love.  There is something right about it. And there\\'s a reason.  An old medieval poem says it:  \\\\\"Where true love is found, God himself is there.\\\\\"  The Bible, 1 John 4 says it this way.  \\\\\"Beloved, let us love one another,  because love is from God;  Everyone who loves is born of God  Whoever does not love does not know God  For God is love.\\\\\" (1John 4:4-8)  There\\'s power in love.  Love can help and heal when nothing else can.  Love can lift up and liberate for living when nothing else will.  And the love that brings two people together  is the same love that can bind them together,  Whether on mountaintops of happiness  and through valleys of hardship.  Love is strong as death  It\\'s flashes are flashes of fire.  Many waters cannot quench love  Love can see you through! There\\'s power in love.  ii  But the love of which we speak is not only for couples getting married or just for interpersonal relationships.  Jesus of Nazareth taught us that the way of love is  the way to a real relationship with the God who created all of us,  and the way to true relationship with each other as children of that one God,  as brothers and sisters in God\\'s human family.  One scholar said it this way:  \\\\\"Jesus had founded the most revolutionary movement in human history: a movement  built on the unconditional love of God for the world and the mandate to live that love.\\\\\"  (Charles Marsh\\'s The Beloved Community)  And in so doing, to change lives and the world itself! There\\'s a reason.  An old spiritual may suggest why:  \\\\\"If you cannot preach like Peter, There is a balm in Gilead  And you cannot pray like Paul, To make the wounded whole  You can tell the love of Jesus, There is a balm in Gilead  How he died to save us all To heal the sin sick soul\\\\\"  \\\\\"Just tell the love of Jesus, how he died to save us all.\\\\\"  He didn\\'t sacrifice his life for himself  Or anything he could get out of it  He did it for others, for the other, for the good and wellbeing of others  That\\'s love  How does St. Paul say it?  Love is not jealous, rude, or boastful.  Love does not insist on its own way.  Love is unselfish, sacrificial, kind and just.  Love seeks the good and the well-being of the other.  Love makes room and space for the other to be. (See 1 Corinthians 13:4-7)  This love, this is the way of Jesus. And it\\'s game changer.  Imagine our homes and families when this way of love is the way.  Imagine our neighborhoods and communities when love is the way.  Imagine our governments and countries when love is the way.  Imagine business and commerce when this love is the way.  Imagine our world when love is the way.  No child would go to bed hungry in such a world as that.  Poverty would become history in such a world as that. T  he earth would be as a sanctuary in such a world as that.  We would treat one another as children of God, regardless of differences.  We would learn how to lay our swords and shields down by the riverside  to study war no more.  There would be a new heaven, a new earth, a new world.  A new and beautiful human family  The very dream of God  Love is strong as death  It\\'s flashes are flashes of fire.  Many waters cannot quench love  iii  The late French Jesuit, Pierre Teilhard de Chardin, was at once a scientist, a Roman Catholic priest, a theologian, a true mystic. His was one of the great minds and spirits of the 20th century.  He suggested that the discovery and harnessing of fire  was one of the great technological discoveries of human history.  Fire made it possible to cook food, thereby reducing the spread of disease.  Fire made it possible to stay warm in cold climates,  thereby marking human migration possible.  Fire made the Bronze Age, the Iron Age, the Industrial Revolution possible.  If you drove here this morning, you did so in part because of harnessed fire.  I flew here from the U.S. due to controlled burn of fire.  Fire is involved in broadcasting this wedding around the world.  And we can text, tweet, email, and otherwise socially engage one another  due to fire. Fire was one of the great technological discoveries of humanity.  In light of this, de Chardin said that if human beings ever harness  the energies of love,  then for the second time in the history of the world,  we will have discovered fire.  Love is the very fire and energy of real life!  Dr. King was right:  \\\\\"We must discover the power of love,  he redemptive power of love.  And when we discover that,  we will be able to make of this old world a new world.  Love is the only way.\\\\\"  My brother, my sister, God love you, God bless you.  My brothers, my sisters, God love you, God bless you.  And may God hold us all  In those almighty hands of love.  Amen.\",\"label\":0,\"distance\":0.6646488904953003},{\"text\":\"You will love, love, love this passionate lady! She gets it!https://www.youtube.com/watch?v=5Zs6IMuee98\",\"label\":1,\"distance\":0.7604187726974487},{\"text\":\"You will love, love, love this passionate lady! She gets it!https://www.youtube.com/watch?v=5Zs6IMuee98\",\"label\":1,\"distance\":0.7604187726974487},{\"text\":\"Meet the five-year-old who loves to shear UP NEXT Meet the five-year-old who loves to shear Sunrise \",\"label\":0,\"distance\":0.8035925030708313},{\"text\":\"You will absolutely love this: \",\"label\":1,\"distance\":0.8515824675559998},{\"text\":\"With marriage equality nationwide, and with more people now than ever supporting the love between two human beings regardless of gender, Adidas celebrated that love on Valentine s Day with the following Instagram pic with the caption,  The love you take is equal to the love you make.  The love you take is equal to the love you make.A photo posted by adidas (@adidas) on Feb 13, 2016 at 11:30pm PSTPretty adorable, right? Who could hate that pic, right? Well, leave it to the internet to show the dark underbelly of society that can t wrap their heads around the idea of two people of the same gender in love.As Buzzfeed reports, several folks were none too pleased with Adidas having the audacity to show this image of two women in love presumably kissing, and many decided to write nasty comments.instagram.comFirst of all, whoever this  leepapi  is, they are not only a bigot, but clearly a moron. The image is of two women. Geez.However, I digress. Adidas wasn t about to let this bigotry go unchecked and smacked down these bigots were they stand, and in the most magnificently glorious way, well, ever.Check it out:instagram.comCommenting directly to the bigots, Adidas responds to the one jerk by simply waving goodbye with a kiss, and the second by explaining that Valentine s Day is for everyone who is in love, not just heterosexuals.These responses were an absolute thing of beauty and quite honestly, I may go out and buy some Adidas active-wear just because of it. Well done.Featured image from Instagram\",\"label\":1,\"distance\":0.9692625403404236},{\"text\":\"Mieko Kobayashi showed up an hour late to her date with Kazuhiko Kobayashi to see how serious he was about her. Toru Hanai/Reuters  In honor of Valentine\\'s Day, Reuters interviewed 20 couples from around the world to ask them about their love story.  From arranged marriages that have lasted over 40 years to young love between teenagers, these pairs prove that love has no age, gender, race, or sexuality.  Keep reading to be reminded that love is real no matter where you are.\",\"label\":0,\"distance\":0.9866335391998291},{\"text\":\"You ll love this guy   he tells it like it is! \",\"label\":1,\"distance\":1.0058404207229614},{\"text\":\"You ll love this guy   he tells it like it is! \",\"label\":1,\"distance\":1.0058404207229614},{\"text\":\"CAN YOU DO IT? WE D LOVE TO KNOW WHAT SURPRISED YOU. Via: Playbuzz\",\"label\":1,\"distance\":1.0311551094055176}]}'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post(\n",
    "     \"http://localhost:3000/predict\",\n",
    "     headers={\"content-type\": \"application/json\"},\n",
    "     data='{\"text\":\"I love it\"}'\n",
    ").text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "compressor = 'zlib'\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def adapt_array(arr):\n",
    "    \"\"\"\n",
    "    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "    \"\"\"\n",
    "    # zlib uses similar disk size that Matlab v5 .mat files\n",
    "    # bz2 compress 4 times zlib, but storing process is 20 times slower.\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(codecs.encode(out.read(),compressor))  # zlib, bz2\n",
    "\n",
    "def convert_array(text):\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    out = io.BytesIO(codecs.decode(out.read(),compressor))\n",
    "    return np.load(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def get_text(id):\n",
    "    conn = sqlite3.connect('../swarog.sqlite')\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT body from raw where rowid = ?\" , [id+1])\n",
    "    rows = c.fetchall()\n",
    "    return rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT annoy index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186477\n",
      "indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 186477/186477 [00:40<00:00, 4631.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "conn = sqlite3.connect('../swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "t = AnnoyIndex(768, 'angular')\n",
    "\n",
    "c.execute(\"SELECT MAX(ROWID) as total from bertnp\")\n",
    "rows = c.fetchall()\n",
    "print(rows[0][0])\n",
    "\n",
    "c.execute(\"SELECT ROWID, vec from bertnp\")\n",
    "\n",
    "print(\"indexing...\")\n",
    "for row in tqdm(c,total=rows[0][0]):\n",
    "    _id, _vec = row[0], convert_array(row[1])\n",
    "    t.add_item(_id, _vec)\n",
    "print(\"building model...\")\n",
    "\n",
    "t.build(100)\n",
    "t.save('swarog.ann')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "t = AnnoyIndex(768, 'angular')\n",
    "t.load('swarog.ann') # super fast, will just mmap the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([150000, 151602, 162433, 165491, 13793, 16095, 93860, 96659, 161544, 174588],\n",
       " [0.0,\n",
       "  0.23311839997768402,\n",
       "  0.23311839997768402,\n",
       "  0.23311839997768402,\n",
       "  0.28941047191619873,\n",
       "  0.2912265658378601,\n",
       "  0.2951999008655548,\n",
       "  0.2951999008655548,\n",
       "  0.2982422411441803,\n",
       "  0.30512621998786926])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get_nns_by_item(150000, 10, search_k=-1, include_distances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rowid          dataset  id  \\\n",
      "0      1  covid_fake_news   0   \n",
      "1      2  covid_fake_news   1   \n",
      "2      3  covid_fake_news   2   \n",
      "3      4  covid_fake_news   3   \n",
      "4      5  covid_fake_news   4   \n",
      "\n",
      "                                                body  \n",
      "0  A post claims compulsory vacination violates t...  \n",
      "1  A photo claims that this person is a doctor wh...  \n",
      "2  Post about a video claims that it is a protest...  \n",
      "3  All deaths by respiratory failure and pneumoni...  \n",
      "4  The dean of the College of Biologists of Euska...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Read sqlite query results into a pandas DataFrame\n",
    "con = sqlite3.connect(\"../swarog.sqlite\")\n",
    "df = pd.read_sql_query(\"SELECT rowid,dataset, id, body from raw where body is not null\", con)\n",
    "\n",
    "# Verify that result of SQL query is stored in the dataframe\n",
    "print(df.head())\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>covid_fake_news</td>\n",
       "      <td>0</td>\n",
       "      <td>A post claims compulsory vacination violates t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>covid_fake_news</td>\n",
       "      <td>1</td>\n",
       "      <td>A photo claims that this person is a doctor wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>covid_fake_news</td>\n",
       "      <td>2</td>\n",
       "      <td>Post about a video claims that it is a protest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>covid_fake_news</td>\n",
       "      <td>3</td>\n",
       "      <td>All deaths by respiratory failure and pneumoni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>covid_fake_news</td>\n",
       "      <td>4</td>\n",
       "      <td>The dean of the College of Biologists of Euska...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rowid          dataset  id  \\\n",
       "0      1  covid_fake_news   0   \n",
       "1      2  covid_fake_news   1   \n",
       "2      3  covid_fake_news   2   \n",
       "3      4  covid_fake_news   3   \n",
       "4      5  covid_fake_news   4   \n",
       "\n",
       "                                                body  \n",
       "0  A post claims compulsory vacination violates t...  \n",
       "1  A photo claims that this person is a doctor wh...  \n",
       "2  Post about a video claims that it is a protest...  \n",
       "3  All deaths by respiratory failure and pneumoni...  \n",
       "4  The dean of the College of Biologists of Euska...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\demo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\demo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "# Lemmatize the stop words\n",
    "tokenizer=LemmaTokenizer()\n",
    "token_stop = tokenizer(' '.join(stop_words))\n",
    "\n",
    "\n",
    "# Create TF-idf model\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, \n",
    "                              tokenizer=tokenizer, max_features=7000)\n",
    "\n",
    "\n",
    "\n",
    "doc_vectors = vectorizer.fit_transform(df['body'])\n",
    "\n",
    "# # Calculate similarity\n",
    "# cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
    "# document_scores = [item.item() for item in cosine_similarities[1:]]\n",
    "# # [0.0, 0.287]\n",
    "\n",
    "# document_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('../swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE stfidf\n",
    "             (dataset TEXT, gid INT, did INT, vec BLOB)''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify(c):\n",
    "    _d =  [x[1]  for x in c.todok().items()]\n",
    "    _xy = [x[0][0]  for x in c.todok().items()], [x[0][1]  for x in c.todok().items()]\n",
    "    return _d, _xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.44834709678418233, 0.5796399641490647, 0.6804427916926344],\n",
       " ([0, 0, 0], [1601, 1711, 3690]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsify(doc_vectors[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# SAVE\n",
    "with open('tfidf.pickle', 'wb') as handle:\n",
    "    pickle.dump(doc_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# LOAD\n",
    "with open('tfidf.pickle', 'rb') as handle:\n",
    "    hand = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 185460/185460 [47:51<00:00, 64.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "conn = sqlite3.connect('../swarog.sqlite')\n",
    "c = conn.cursor()\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    vec = doc_vectors[index].toarray()[0]\n",
    "    values_to_insert = [(row['dataset'], row['rowid'], index, adapt_array(vec))]\n",
    "    c.executemany(\"\"\"INSERT INTO tfidf(dataset, gid, did, vec) VALUES (?,?,?,?)\"\"\", values_to_insert)\n",
    "    conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 185460/185460 [07:55<00:00, 390.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "t = AnnoyIndex(7000, 'angular')\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    vec = doc_vectors[index].toarray()[0]\n",
    "    t.add_item(index, vec)\n",
    "    \n",
    "t.build(100)\n",
    "t.save('swarog_tfidf.ann')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

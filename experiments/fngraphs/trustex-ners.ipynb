{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c253dd-4f4d-4d21-95ad-b13158f36ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b8c529a-ae5c-4661-8a68-f4e3bb333b80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1410)\n",
    "\n",
    "class DatasetLoader:\n",
    "    def get(self, name):\n",
    "        print(f\"loading {name}\")\n",
    "        if name == \"fnkdd\":\n",
    "            data = pd.read_csv(\"/home/rkozik/Desktop/swarog_exp_disk/datasets/fakenewskdd/train.csv\",sep=\"\\t\")\n",
    "            data.head()\n",
    "            body = data[\"text\"].values\n",
    "            labels = 1-data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"mmcovid\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/mmcovid/news_collection.csv\",sep=\"\\t\")\n",
    "            data[\"label\"] = [ 1 if v ==\"fake\" else 0 for v in data[\"label\"]]\n",
    "            data[\"text\"] = [ str(v) for v in data[\"text\"]]\n",
    "            data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"liar\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/liar.csv\", sep=\"\\t\",encoding=\"utf-8\")\n",
    "            def mpx(x):\n",
    "                if x in [0,2]:\n",
    "                    return 0\n",
    "                elif x in [4,5]:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            data[\"text\"] = data[\"statement\"]\n",
    "            data[\"label\"] = [mpx(x) for x in data[\"label\"]]\n",
    "            data=data[ data[\"label\"] != -1] \n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "\n",
    "        if name == \"covidfn\":\n",
    "            data = pd.read_csv(\"covid_fake_news.csv\", sep=\"\\t\")\n",
    "            body = data[\"title\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        return body, labels, total_number_of_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec5cb3-4b6a-42d7-85ee-b5af2327ed5b",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1d03d0-9868-4c57-8677-c8ccb6f2a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.scores = {\n",
    "            'Accuracy': {'func': accuracy_score},\n",
    "            'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "            'F1': {'func': f1_score},\n",
    "            'Precision': {'func': precision_score},\n",
    "            'Recall': {'func': recall_score},\n",
    "            'G-mean': {'func': geometric_mean_score}\n",
    "        }\n",
    "        \n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            score_dict[\"list\"] = []\n",
    "            score_dict[\"lab\"] = []\n",
    "\n",
    "    def update(self, actual, prediction):\n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\",\"G-mean\"]:\n",
    "                scorvaln = score_dict['func'](actual, prediction, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](actual, prediction, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](actual, prediction)\n",
    "                score_dict['list'].append(scorval)\n",
    "                \n",
    "    def print_table(self, labels=None):\n",
    "        # Print stats\n",
    "        scores = self.scores\n",
    "        numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "        scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "        head = \"  %-20s  %-10s  \" +  numlabels * \" %-10s  \" \n",
    "        headv = [\"Score\", \"Average\"]\n",
    "        if labels:\n",
    "            headv.extend([labels[i] for i in range(numlabels)])\n",
    "        else:\n",
    "            headv.extend([\"Lab:\"+str(i+1) for i in range(numlabels)])\n",
    "        row=head % tuple(headv)\n",
    "        # table header\n",
    "        print(\"―\"*len(row))\n",
    "        print(row)\n",
    "        print(\"―\"*len(row))\n",
    "        # table rows\n",
    "        for score_name, score_dict in sorted(scores.items()) :\n",
    "            headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "            for i in range(numlabels):\n",
    "                if score_name in [\"F1\",\"Precision\",\"Recall\", \"G-mean\"]:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels* \"%4.1f ± %4.1f  \"\n",
    "                    vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                    headv.append(np.mean(vals)*100)\n",
    "                    headv.append(np.std(vals)*100)\n",
    "                else:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels * \"%-11s  \" \n",
    "                    headv.append(\"-\")\n",
    "            print(head % tuple(headv))\n",
    "        print(\"―\"*len(row))\n",
    "\n",
    "\n",
    "def get_graph_node_stats(vec, nearestDocIDs, y_train, bodyTrainTFIDF):   \n",
    "    vecdense = vec.toarray()[0]\n",
    "    docids = nearestDocIDs\n",
    "    trlabels = np.array(y_train)\n",
    "    labsum = trlabels[docids].sum()\n",
    "    \n",
    "    ivec = []\n",
    "    labmask = []\n",
    "    for hitdocid in docids:\n",
    "        value=bodyTrainTFIDF[hitdocid].toarray()[0]\n",
    "        intersection = (vecdense>0)*(value>0)\n",
    "        ivec.append(intersection.sum())\n",
    "        labmask.append(trlabels[hitdocid])\n",
    "        \n",
    "    masked_ivec =  np.array(ivec)*np.array(labmask)   \n",
    "    masked_ivec_neg =  np.array(ivec)*(-1*(np.array(labmask)-1)) \n",
    "    ivec = np.array(ivec)\n",
    "    masked_ivec = np.array(masked_ivec)\n",
    "    masked_ivec_neg = np.array(masked_ivec_neg)\n",
    "    \n",
    "    newvec = [labsum, (vecdense>0).sum(),ivec.max(), ivec.max(), masked_ivec.max(), masked_ivec.min(), masked_ivec_neg.max(), masked_ivec_neg.min()]\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842a6fe-2dea-45f6-91c0-f98d14065865",
   "metadata": {},
   "source": [
    "# Swarog Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551840b1-0372-4cf3-be75-520adda5ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "from bentoml.io import JSON\n",
    "from annoy import AnnoyIndex\n",
    "import re\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle5 as pickle\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    " \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "class SwarogModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=4, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, \n",
    "                                           truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, \n",
    "                                       output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return np.array(allembeds)\n",
    "    \n",
    "    def train(self, body, labels):\n",
    "        embeddings = self.transform(body)\n",
    "        self.cls = LogisticRegression(max_iter=1000)\n",
    "        self.cls.fit(embeddings, labels)\n",
    "        self.train_prob = self.cls.predict_proba(embeddings)\n",
    "        \n",
    "    def predict(self, body):\n",
    "        embeddings = self.transform(body)\n",
    "        self.test_prob = self.cls.predict_proba(embeddings)\n",
    "        return  self.cls.predict(embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49565d-f7a3-4ebf-ba03-7bb29d13a3d8",
   "metadata": {},
   "source": [
    "# Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958f52d6-f007-4eb4-bec6-ec915a22bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', \"'\"]\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "    \n",
    "\n",
    "class TrustexModel:\n",
    "    def __init__(self):\n",
    "        # Lemmatize the stop words\n",
    "        self.tokenizer=LemmaTokenizer()\n",
    "        self.token_stop = self.tokenizer(' '.join(stop_words))\n",
    "        \n",
    "    def tfidf(self,body):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=self.token_stop)\n",
    "        self.tfidf_vectorizer.fit(body)\n",
    "        self.vocabulary_tfidf_words = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.graph_knn = []\n",
    "        self.train_labels = labels\n",
    "        from tqdm import tqdm\n",
    "        for id, topIDs in tqdm(enumerate(knn_idx), total=knn_idx.shape[0]):\n",
    "            vec = self.bodyTrainTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], labels, self.bodyTrainTFIDF)\n",
    "            self.graph_knn.append(newvec)\n",
    "        print(\"avg. nodes sim.=\",np.mean([x[2]/x[1] for x in self.graph_knn]))\n",
    "\n",
    "    def graph_transform_test_data(self, body):\n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        self.graph_test_knn = []\n",
    "        for id, topIDs in tqdm(enumerate(knn_test_idx), total=knn_test_idx.shape[0]):\n",
    "            vec = self.bodyTestTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], self.train_labels, self.bodyTrainTFIDF)\n",
    "            self.graph_test_knn.append(newvec)        \n",
    "              \n",
    "    def train(self, body, labels):\n",
    "        print(\"Building similarity graph\")\n",
    "        self.tfidf(body)\n",
    "        self.create_graph(body, labels)\n",
    "          \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.graph_knn, labels)\n",
    "\n",
    "    def predict(self, body):\n",
    "        self.graph_transform_test_data(body)\n",
    "        y_pred = self.cls.predict(self.graph_test_knn)\n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f556a4b-df4e-454d-bd23-4ac3f3d87b22",
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "source": [
    "# Semantic relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cee2e4a1-1388-4769-a855-94eb6e61882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "class Semantic:\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.tokenizer=LemmaTokenizer()\n",
    "        self.token_stop = self.tokenizer(' '.join(stop_words))\n",
    "        self.con=sqlite3.connect(\"svo.db\")\n",
    "        \n",
    "    def extract_svo(self, body, labels):\n",
    "        self.con.execute(\"drop table if exists svo\")\n",
    "        self.con.execute(\"create table if not exists svo(sub, verb, obj, docid, label)\")\n",
    "        self.con.commit()\n",
    "        \n",
    "        for docid, txt in tqdm(enumerate(body), total=len(labels)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            for svo in svos:\n",
    "                if len(svo) == 3:\n",
    "                    self.con.execute(\"insert into svo values(?,?,?,?,?)\", \n",
    "                        [svo[0],self.ps.stem(svo[1]),svo[2], docid, int(labels[docid])])\n",
    "            self.con.commit()\n",
    "        \n",
    "    def tfidf(self,body):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=self.token_stop)\n",
    "        self.tfidf_vectorizer.fit(body)\n",
    "        self.vocabulary_tfidf_words = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.train_labels = labels\n",
    "        \n",
    "    def transform_test_data(self, body):\n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        rsvo_body = self.right_svo[\"redux\"].values\n",
    "        return knn_test_d,knn_test_idx\n",
    "              \n",
    "    def train(self, body, labels):\n",
    "        print(\"Extracting SVO\")\n",
    "        self.extract_svo(body,labels) \n",
    "\n",
    "#         rsvo_body = self.right_svo[\"redux\"].values\n",
    "#         print(self.right_svo.head())\n",
    "        \n",
    "#         print(\"Building similarity graph\")\n",
    "#         self.tfidf(rsvo_body)\n",
    "#         self.create_graph(rsvo_body, labels)\n",
    "          \n",
    "#         self.cls = LogisticRegression(max_iter=10000)\n",
    "#         self.cls.fit(self.graph_knn, labels)\n",
    "\n",
    "    def predict(self, body):\n",
    "        print(body)\n",
    "        tokens = nlp(body)\n",
    "        svos = findSVOs(tokens)\n",
    "        for svo in svos:\n",
    "            if len(svo) != 3:\n",
    "                continue\n",
    "            verb = self.ps.stem(svo[1])\n",
    "            print(\"---\\n\",svo)    \n",
    "            self.svo = pd.read_sql(f\"\"\"\n",
    "                select \n",
    "                    sum(label) as lab, count(*) as ctr, count(distinct docid) as docs\n",
    "                    from svo  \n",
    "                    where 1 \n",
    "                    and (sub='{svo[0]}' and verb='{verb}' and obj='{svo[2]}')\n",
    "                \"\"\", self.con)\n",
    "            print(self.svo)\n",
    "\n",
    "        \n",
    "        #return self.svo\n",
    "        #y_pred = self.cls.predict(self.graph_test_knn)\n",
    "        #return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "938df657-ee63-4388-b674-4da5c80aed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SVO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 8074/8074 [00:55<00:00, 145.20it/s]\n"
     ]
    }
   ],
   "source": [
    "sem = Semantic()\n",
    "sem.train(body[train],labels[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7eb8b871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A post claims compulsory vacination violates the principles of bioethics, that coronavirus doesnâ€™t exist, that the PCR test returns many false positives, and that influenza vaccine is related to COVID-19.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e53c1eb-4847-44fd-a246-8e3f62c5ad3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of Ghana said that the coronavirus was created in a laboratory\n",
      "---\n",
      " ('a laboratory', 'create', 'the coronavirus')\n",
      "   lab  ctr  docs\n",
      "0    0    4     4\n"
     ]
    }
   ],
   "source": [
    "sem.predict(body[test][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7f51d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5G technology creates coronavirus in human cells', 0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[train][209], labels[train][209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "931d6b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>verb</th>\n",
       "      <th>obj</th>\n",
       "      <th>docid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a laboratory</td>\n",
       "      <td>creat</td>\n",
       "      <td>the coronavirus</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 G technology</td>\n",
       "      <td>creat</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a laboratory</td>\n",
       "      <td>creat</td>\n",
       "      <td>the coronavirus</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a laboratory</td>\n",
       "      <td>creat</td>\n",
       "      <td>the coronavirus</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Izmir</td>\n",
       "      <td>creat</td>\n",
       "      <td>5 G base stations</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the new coronavirus epidemic period</td>\n",
       "      <td>creat</td>\n",
       "      <td>5 G base stations</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a special operation</td>\n",
       "      <td>creat</td>\n",
       "      <td>a concentration camp in</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a laboratory</td>\n",
       "      <td>creat</td>\n",
       "      <td>the coronavirus</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mass chipping</td>\n",
       "      <td>creat</td>\n",
       "      <td>Dr. Leonard Coldwell :</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mass chipping</td>\n",
       "      <td>creat</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sub   verb                      obj  docid  \\\n",
       "0                         a laboratory  creat          the coronavirus    105   \n",
       "1                       5 G technology  creat              coronavirus    209   \n",
       "2                         a laboratory  creat          the coronavirus    222   \n",
       "3                         a laboratory  creat          the coronavirus    230   \n",
       "4                                Izmir  creat        5 G base stations    346   \n",
       "5  the new coronavirus epidemic period  creat        5 G base stations    346   \n",
       "6                  a special operation  creat  a concentration camp in    471   \n",
       "7                         a laboratory  creat          the coronavirus    659   \n",
       "8                        mass chipping  creat   Dr. Leonard Coldwell :    829   \n",
       "9                        mass chipping  creat              coronavirus    829   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"\"\"\n",
    "select * from svo \n",
    "where verb = 'creat'\n",
    "limit 10\n",
    "\"\"\", sem.con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debdadc-ecf2-4c4c-95c2-50c7ad4d4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"\n",
    "select \n",
    "    sum(label),count(*), count(distinct docid)\n",
    "    from svo  \n",
    "    where sub='chlorine dioxide' and verb='cures' and obj='COVID-19' \n",
    "    -- limit 10\n",
    "\"\"\", con)\n",
    "right_svo[\"redux\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a0eca-d688-4576-b35c-b8142055f3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85ec904-11b2-402d-8b34-5e2a11f577b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1323189935.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [5], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "tokens = nlp(\"Seated in Mission Control, Chris Kraft neared the end of a tedious Friday afternoon as he monitored a seemingly interminable ground test of the Apollo 1 spacecraft.\")\n",
    "tokens = nlp(\"  WHO suggests oral hygiene to prevent the spread of coronavirus.\")\n",
    "svos = findSVOs(tokens)\n",
    "print(svos)\n",
    "\n",
    "def semantic_train(body, labels):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fda6af14-7ee9-44c9-9d74-b8c1295a8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           program             \n",
      "programs            program             \n",
      "programmed          program             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf80271-52d1-4d69-9f46-3e307c0a0979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "rm: cannot remove 'svodb': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm svodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50af38fd-63a4-446f-bd37-b61d60075c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "con=sqlite3.connect(\"svo.db\")\n",
    "con.execute(\"drop table if exists svo\")\n",
    "con.execute(\"create table if not exists svo(sub, verb, obj, docid, label)\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57b04589-343e-49cd-8209-6d6509a539ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 8074/8074 [00:37<00:00, 218.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "con.commit()\n",
    "for docid, txt in tqdm(enumerate(body[train]), total=len(train)):\n",
    "    tokens = nlp(txt)\n",
    "    svos = findSVOs(tokens)\n",
    "    for svo in svos:\n",
    "        if len(svo) == 3:\n",
    "            con.execute(\"insert into svo values(?,?,?,?,?)\", [svo[0],ps.stem(svo[1]),svo[2],docid, int(labels[train][docid])] )\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55dca4a5-c815-4f57-a68c-dfc2b709d985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(*)\n",
       "0     10618"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"select count(*) from svo\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "044e0c76-122b-45d1-b89a-ae7d0afc3c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>redux</th>\n",
       "      <th>fakes</th>\n",
       "      <th>ctr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thousands of times,Facebook,a claim,hundreds o...</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chlorine dioxide,Drinking water,Lemon Juice,Pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a protest against coronavirus restrictions in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thousands of times,multiple posts on,hundreds ...</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WHO,measures,Boldo tea,that cures,seawater,a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>There ’s a “ direct correlation ” between Caro...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7708</th>\n",
       "      <td>There ’s definitive proof are</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7709</th>\n",
       "      <td>That ’s his quote</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7710</th>\n",
       "      <td>there ’s some good data there</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7711</th>\n",
       "      <td>It ’s the safest time</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7712 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  redux  fakes  ctr\n",
       "0     thousands of times,Facebook,a claim,hundreds o...      0  126\n",
       "1     chlorine dioxide,Drinking water,Lemon Juice,Pa...      0   75\n",
       "2     a protest against coronavirus restrictions in ...      0   57\n",
       "3     thousands of times,multiple posts on,hundreds ...      0   44\n",
       "4     WHO,measures,Boldo tea,that cures,seawater,a p...      0   42\n",
       "...                                                 ...    ...  ...\n",
       "7707  There ’s a “ direct correlation ” between Caro...      1    1\n",
       "7708                      There ’s definitive proof are      0    1\n",
       "7709                                  That ’s his quote      1    1\n",
       "7710                      there ’s some good data there      1    1\n",
       "7711                              It ’s the safest time      0    1\n",
       "\n",
       "[7712 rows x 3 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_svo = pd.read_sql(\"\"\"\n",
    "select \n",
    "    group_concat(sub)  || \" \" || verb || \" \" || obj as redux,\n",
    "    sum(label) as fakes,\n",
    "    count(*) as ctr \n",
    "    from svo group by verb,obj \n",
    "    order by ctr desc \n",
    "    -- limit 10\n",
    "\"\"\", con)\n",
    "right_svo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b992a48-181f-4b67-a320-939543e855ff",
   "metadata": {},
   "source": [
    "# Infrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5526269d-b19e-46e7-803f-8eada642f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Inference:\n",
    "    def train(self,graph, content, labels):\n",
    "        newX=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i])\n",
    "            newX.append(v2)\n",
    "            \n",
    "        self.inf = RandomForestClassifier(max_depth=12)\n",
    "        self.inf.fit(newX, labels)\n",
    "        \n",
    "    def predict(self, graph, content):\n",
    "        newTest=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i])\n",
    "            newTest.append(v2)\n",
    "    \n",
    "        return self.inf.predict(newTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baccae2-b3bd-4e07-a8cd-e55fe8801382",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "827802d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 8972\n",
      "labels fake= 461 real= 8511\n"
     ]
    }
   ],
   "source": [
    "body, labels, total_number_of_claims = loader.get(\"covidfn\")\n",
    "train, test = list(rskf.split(X, labels))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "315f84fb-918d-4814-9e04-3601b4f3fe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 8972\n",
      "labels fake= 461 real= 8511\n",
      "fold-0\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 8074/8074 [00:07<00:00, 1062.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.49843772108989287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 898/898 [00:00<00:00, 1064.68it/s]\n",
      "100%|██████████████████████████████████████| 2019/2019 [00:10<00:00, 184.54it/s]\n",
      "100%|████████████████████████████████████████| 225/225 [00:01<00:00, 185.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              96.5 ±  0.0  -            -            \n",
      "  Balanced Accuracy     67.3 ±  0.0  -            -            \n",
      "  F1                    95.8 ±  0.0  98.2 ±  0.0  50.8 ±  0.0  \n",
      "  G-mean                60.7 ±  0.0  58.9 ±  0.0  58.9 ±  0.0  \n",
      "  Precision             96.5 ±  0.0  96.6 ±  0.0  94.1 ±  0.0  \n",
      "  Recall                96.5 ±  0.0  99.9 ±  0.0  34.8 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              97.1 ±  0.0  -            -            \n",
      "  Balanced Accuracy     77.9 ±  0.0  -            -            \n",
      "  F1                    96.9 ±  0.0  98.5 ±  0.0  66.7 ±  0.0  \n",
      "  G-mean                75.5 ±  0.0  74.9 ±  0.0  74.9 ±  0.0  \n",
      "  Precision             96.8 ±  0.0  97.7 ±  0.0  81.2 ±  0.0  \n",
      "  Recall                97.1 ±  0.0  99.3 ±  0.0  56.5 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Both:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              96.5 ±  0.0  -            -            \n",
      "  Balanced Accuracy     78.6 ±  0.0  -            -            \n",
      "  F1                    96.4 ±  0.0  98.2 ±  0.0  63.5 ±  0.0  \n",
      "  G-mean                76.6 ±  0.0  76.1 ±  0.0  76.1 ±  0.0  \n",
      "  Precision             96.3 ±  0.0  97.8 ±  0.0  69.2 ±  0.0  \n",
      "  Recall                96.5 ±  0.0  98.6 ±  0.0  58.7 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"covidfn\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    inf_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference = Inference()\n",
    "    \n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "        \n",
    "        inference.train(trustex.graph_knn, swarog.train_prob, labels[train])\n",
    "        newpred = inference.predict(trustex.graph_test_knn, swarog.test_prob)\n",
    "        inf_quality.update(labels[test], newpred)\n",
    "\n",
    "        break\n",
    "\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "print(\"Both:\")\n",
    "inf_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a15b3-d116-4d04-af2f-d87dcf77ed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env.hator",
   "language": "python",
   "name": "env.hator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

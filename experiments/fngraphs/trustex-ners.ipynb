{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c253dd-4f4d-4d21-95ad-b13158f36ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6b8c529a-ae5c-4661-8a68-f4e3bb333b80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1410)\n",
    "\n",
    "class DatasetLoader:\n",
    "    def get(self, name):\n",
    "        print(f\"loading {name}\")\n",
    "        if name == \"fnkdd\":\n",
    "            data = pd.read_csv(\"/home/rkozik/Desktop/swarog_exp_disk/datasets/fakenewskdd/train.csv\",sep=\"\\t\")\n",
    "            data.head()\n",
    "            body = data[\"text\"].values\n",
    "            labels = 1-data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"mmcovid\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/mmcovid/news_collection.csv\",sep=\"\\t\")\n",
    "            data[\"label\"] = [ 1 if v ==\"fake\" else 0 for v in data[\"label\"]]\n",
    "            data[\"text\"] = [ str(v) for v in data[\"text\"]]\n",
    "            data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"liar\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/liar.csv\", sep=\"\\t\",encoding=\"utf-8\")\n",
    "            def mpx(x):\n",
    "                if x in [0,2]:\n",
    "                    return 0\n",
    "                elif x in [4,5]:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            data[\"text\"] = data[\"statement\"]\n",
    "            data[\"label\"] = [mpx(x) for x in data[\"label\"]]\n",
    "            data=data[ data[\"label\"] != -1] \n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "\n",
    "        if name == \"covidfn\":\n",
    "            data = pd.read_csv(\"covid_fake_news.csv\", sep=\",\")\n",
    "            body = data[\"headlines\"].values\n",
    "            labels = 1 - data[\"outcome\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        return body, labels, total_number_of_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec5cb3-4b6a-42d7-85ee-b5af2327ed5b",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4d1d03d0-9868-4c57-8677-c8ccb6f2a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.scores = {\n",
    "            'Accuracy': {'func': accuracy_score},\n",
    "            'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "            'F1': {'func': f1_score},\n",
    "            'Precision': {'func': precision_score},\n",
    "            'Recall': {'func': recall_score},\n",
    "            'G-mean': {'func': geometric_mean_score}\n",
    "        }\n",
    "        \n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            score_dict[\"list\"] = []\n",
    "            score_dict[\"lab\"] = []\n",
    "\n",
    "    def update(self, actual, prediction):\n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\",\"G-mean\"]:\n",
    "                scorvaln = score_dict['func'](actual, prediction, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](actual, prediction, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](actual, prediction)\n",
    "                score_dict['list'].append(scorval)\n",
    "                \n",
    "    def print_table(self, labels=None):\n",
    "        # Print stats\n",
    "        scores = self.scores\n",
    "        numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "        scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "        head = \"  %-20s  %-10s  \" +  numlabels * \" %-10s  \" \n",
    "        headv = [\"Score\", \"Average\"]\n",
    "        if labels:\n",
    "            headv.extend([labels[i] for i in range(numlabels)])\n",
    "        else:\n",
    "            headv.extend([\"Lab:\"+str(i+1) for i in range(numlabels)])\n",
    "        row=head % tuple(headv)\n",
    "        # table header\n",
    "        print(\"―\"*len(row))\n",
    "        print(row)\n",
    "        print(\"―\"*len(row))\n",
    "        # table rows\n",
    "        for score_name, score_dict in sorted(scores.items()) :\n",
    "            headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "            for i in range(numlabels):\n",
    "                if score_name in [\"F1\",\"Precision\",\"Recall\", \"G-mean\"]:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels* \"%4.1f ± %4.1f  \"\n",
    "                    vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                    headv.append(np.mean(vals)*100)\n",
    "                    headv.append(np.std(vals)*100)\n",
    "                else:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels * \"%-11s  \" \n",
    "                    headv.append(\"-\")\n",
    "            print(head % tuple(headv))\n",
    "        print(\"―\"*len(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817fbbb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521678b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_node_stats(vec, nearestDocIDs, y_train, bodyTrainTFIDF):   \n",
    "    vecdense = vec.toarray()[0]\n",
    "    docids = nearestDocIDs\n",
    "    trlabels = np.array(y_train)\n",
    "    labsum = trlabels[docids].sum()\n",
    "    \n",
    "    ivec = []\n",
    "    labmask = []\n",
    "    for hitdocid in docids:\n",
    "        value=bodyTrainTFIDF[hitdocid].toarray()[0]\n",
    "        intersection = (vecdense>0)*(value>0)\n",
    "        ivec.append(intersection.sum())\n",
    "        labmask.append(trlabels[hitdocid])\n",
    "        \n",
    "    masked_ivec =  np.array(ivec)*np.array(labmask)   \n",
    "    masked_ivec_neg =  np.array(ivec)*(-1*(np.array(labmask)-1)) \n",
    "    ivec = np.array(ivec)\n",
    "    masked_ivec = np.array(masked_ivec)\n",
    "    masked_ivec_neg = np.array(masked_ivec_neg)\n",
    "    \n",
    "    newvec = [labsum, (vecdense>0).sum(),ivec.max(), ivec.max(), masked_ivec.max(), masked_ivec.min(), masked_ivec_neg.max(), masked_ivec_neg.min()]\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842a6fe-2dea-45f6-91c0-f98d14065865",
   "metadata": {},
   "source": [
    "## Swarog Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "551840b1-0372-4cf3-be75-520adda5ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "from bentoml.io import JSON\n",
    "from annoy import AnnoyIndex\n",
    "import re\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle5 as pickle\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    " \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "class SwarogModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=4, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, \n",
    "                                           truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, \n",
    "                                       output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return np.array(allembeds)\n",
    "    \n",
    "    def train(self, body, labels):\n",
    "        embeddings = self.transform(body)\n",
    "        self.cls = LogisticRegression(max_iter=1000)\n",
    "        self.cls.fit(embeddings, labels)\n",
    "        self.train_prob = self.cls.predict_proba(embeddings)\n",
    "        \n",
    "    def predict(self, body):\n",
    "        embeddings = self.transform(body)\n",
    "        self.test_prob = self.cls.predict_proba(embeddings)\n",
    "        return  self.cls.predict(embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49565d-f7a3-4ebf-ba03-7bb29d13a3d8",
   "metadata": {},
   "source": [
    "## Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "958f52d6-f007-4eb4-bec6-ec915a22bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "def call_it2(instance, name, arg):\n",
    "    return getattr(instance, name)(arg)\n",
    "\n",
    "class TrustexModel:\n",
    "    def __init__(self):\n",
    "        # Lemmatize the stop words\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', \"'\"]\n",
    "        \n",
    "    def tfidf(self,body):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "        self.tfidf_vectorizer.fit(body)\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def calc_node_stats(self, item):\n",
    "        id, topIDs = item\n",
    "        vec = self.bodyTrainTFIDF[id]\n",
    "        newvec = get_graph_node_stats(vec, topIDs[1:], self.train_labels, self.bodyTrainTFIDF)\n",
    "        return newvec\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.graph_knn = []\n",
    "        self.train_labels = labels\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'calc_node_stats')\n",
    "            self.graph_knn = list(tqdm(pool.imap(func_call_it, enumerate(knn_idx),chunksize=64), \n",
    "                                total=knn_idx.shape[0]))\n",
    "        print(\"avg. nodes sim.=\",np.mean([x[2]/(x[1]+0.00001) for x in self.graph_knn]))\n",
    "\n",
    "    def calc_node_stats_test(self, item):\n",
    "        id, topIDs = item\n",
    "        vec = self.bodyTestTFIDF[id]\n",
    "        newvec = get_graph_node_stats(vec, topIDs, self.train_labels, self.bodyTrainTFIDF)\n",
    "        return newvec\n",
    "    \n",
    "    def graph_transform_test_data(self, body):\n",
    "        \n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        \n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'calc_node_stats_test')\n",
    "            self.graph_test_knn = list(tqdm(pool.imap(func_call_it, enumerate(knn_test_idx),chunksize=64), \n",
    "                                total=knn_test_idx.shape[0]))\n",
    "        \n",
    "    def train(self, body, labels):\n",
    "        print(\"Building similarity graph\")\n",
    "        self.tfidf(body)\n",
    "        self.create_graph(body, labels)\n",
    "          \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.graph_knn, labels)\n",
    "        self.trainX = self.graph_knn\n",
    "\n",
    "    def predict(self, body):\n",
    "        print(\"Create test data\")\n",
    "        self.graph_transform_test_data(body)\n",
    "        y_pred = self.cls.predict(self.graph_test_knn)\n",
    "        self.testX = self.graph_test_knn\n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea32b2",
   "metadata": {},
   "source": [
    "## Semantics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d87001be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "ab383a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "# Download stopwords list\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    \n",
    "    \n",
    "def call_it2(instance, name, arg):\n",
    "    return getattr(instance, name)(arg)\n",
    "\n",
    "class Semantics:\n",
    "    def __init__(self):\n",
    "        # Lemmatize the stop words\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', \"'\"]\n",
    "        self.token_stop = self.transform_sentence(' '.join(stop_words))\n",
    "        \n",
    "    def transform_sentence(self, doc):\n",
    "        tokens = nlp(doc)\n",
    "        svos = findSVOs(tokens)\n",
    "        x = []\n",
    "        for s in svos:\n",
    "            x.extend(s)\n",
    "            \n",
    "        if len(x) == 0:\n",
    "            return [\"empty\"]\n",
    "        return x\n",
    "\n",
    "    def tfidf(self,body):\n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'transform_sentence')\n",
    "            tok_body = list(tqdm(pool.imap(func_call_it, body,chunksize=64), \n",
    "                                total=len(body)))\n",
    "        vectors = [\" \".join(x) for x in tok_body]\n",
    "\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "        self.tfidf_vectorizer.fit(vectors)\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def calc_node_stats(self, item):\n",
    "        id, topIDs = item\n",
    "        vec = self.bodyTrainTFIDF[id]\n",
    "        newvec = get_graph_node_stats(vec, topIDs[1:], self.train_labels, self.bodyTrainTFIDF)\n",
    "        return newvec\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=12)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.graph_knn = []\n",
    "        self.train_labels = labels\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'calc_node_stats')\n",
    "            self.graph_knn = list(tqdm(pool.imap(func_call_it, enumerate(knn_idx),chunksize=64), \n",
    "                                total=knn_idx.shape[0]))\n",
    "        print(\"avg. nodes sim.=\",np.mean([x[2]/(x[1]+0.00001) for x in self.graph_knn]))\n",
    "\n",
    "    def calc_node_stats_test(self, item):\n",
    "        id, topIDs = item\n",
    "        vec = self.bodyTestTFIDF[id]\n",
    "        newvec = get_graph_node_stats(vec, topIDs, self.train_labels, self.bodyTrainTFIDF)\n",
    "        return newvec\n",
    "    \n",
    "    def graph_transform_test_data(self, body):\n",
    "        \n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        \n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'calc_node_stats_test')\n",
    "            self.graph_test_knn = list(tqdm(pool.imap(func_call_it, enumerate(knn_test_idx),chunksize=64), \n",
    "                                total=knn_test_idx.shape[0]))     \n",
    "              \n",
    "    def train(self, body, labels):\n",
    "        print(\"Building similarity graph\")\n",
    "        self.tfidf(body)\n",
    "        self.create_graph(body, labels)\n",
    "          \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.graph_knn, labels)\n",
    "        self.trainX = self.graph_knn\n",
    "\n",
    "    def predict(self, body):\n",
    "        self.graph_transform_test_data(body)\n",
    "        y_pred = self.cls.predict(self.graph_test_knn)\n",
    "        self.testX = self.graph_test_knn\n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "26ed9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False  True]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_size = 500\n",
    "if bootstrap_size != 0:\n",
    "    bootstrap_factor = bootstrap_size / len(body[train])\n",
    "    bootstrap = np.random.uniform(size=len(body[train])) < bootstrap_factor\n",
    "    print(bootstrap[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "6d2d28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 490/490 [00:10<00:00, 48.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 490/490 [00:00<00:00, 2616.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4060444801421384\n"
     ]
    }
   ],
   "source": [
    "sema = Semantics()\n",
    "sema.train(body[train][bootstrap],labels[train][bootstrap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "ee536f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 2081.06it/s]\n"
     ]
    }
   ],
   "source": [
    "ypred = sema.predict(body[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "688276d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              73.3 ±  0.0  -            -            \n",
      "  Balanced Accuracy     70.8 ±  0.0  -            -            \n",
      "  F1                    72.7 ±  0.0  63.6 ±  0.0  78.9 ±  0.0  \n",
      "  G-mean                70.7 ±  0.0  69.6 ±  0.0  69.6 ±  0.0  \n",
      "  Precision             73.0 ±  0.0  70.7 ±  0.0  74.6 ±  0.0  \n",
      "  Recall                73.3 ±  0.0  57.7 ±  0.0  83.8 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "met = Metrics()\n",
    "met.update(labels[test], ypred)\n",
    "met.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b992a48-181f-4b67-a320-939543e855ff",
   "metadata": {},
   "source": [
    "\n",
    "## Infrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "5526269d-b19e-46e7-803f-8eada642f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Inference:\n",
    "    def train(self,graph, content, semantics, labels):\n",
    "        newX=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i]) \n",
    "            if semantics:\n",
    "                v2 = np.append(v2, semantics[i])\n",
    "            newX.append(v2)\n",
    "            \n",
    "        self.inf = RandomForestClassifier(max_depth=12)\n",
    "        self.inf.fit(newX, labels)\n",
    "        \n",
    "    def predict(self, graph, content, semantics):\n",
    "        newTest=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i]) \n",
    "            if semantics:\n",
    "                v2 = np.append(v2, semantics[i])\n",
    "            newTest.append(v2)\n",
    "    \n",
    "        return self.inf.predict(newTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baccae2-b3bd-4e07-a8cd-e55fe8801382",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "827802d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 10201\n",
      "labels fake= 9727 real= 474\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "body, labels, total_number_of_claims = loader.get(\"covidfn\")\n",
    "X=range(0,total_number_of_claims)\n",
    "train, test = list(rskf.split(X, labels))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262aa24d",
   "metadata": {},
   "source": [
    "## fnkdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "315f84fb-918d-4814-9e04-3601b4f3fe01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fnkdd\n",
      "total_number_of_claims= 4986\n",
      "labels fake= 2972 real= 2014\n",
      "fold-0\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [01:24<00:00, 53.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:02<00:00, 1503.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4704687141506022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 629.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:03<00:00, 1159.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4404575174928806\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 588.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:18<00:00, 61.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 64.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-1\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [01:25<00:00, 52.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:02<00:00, 1506.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.47742000077297625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 364.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:04<00:00, 1110.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.44698345618025653\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 580.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:18<00:00, 62.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 62.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-2\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [01:22<00:00, 54.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:02<00:00, 1506.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.47448571643374976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 605.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:03<00:00, 1139.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4434671861528761\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 591.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:17<00:00, 62.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 62.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-3\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [01:24<00:00, 52.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:03<00:00, 1468.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4784192118613257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 649.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:04<00:00, 1056.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4469319565605302\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 581.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:18<00:00, 62.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 64.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-4\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [01:23<00:00, 53.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:02<00:00, 1508.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.474465563293773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 649.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:03<00:00, 1152.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4442217812374032\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 587.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:17<00:00, 62.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-5\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [01:23<00:00, 53.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:02<00:00, 1528.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4717592148265669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 626.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4487/4487 [00:03<00:00, 1156.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4390752340438459\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 591.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:18<00:00, 62.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-6\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [01:23<00:00, 53.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:02<00:00, 1526.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4754358005590211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 650.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:03<00:00, 1133.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.44295315279661407\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 596.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:17<00:00, 62.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-7\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [01:23<00:00, 53.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:03<00:00, 1442.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.47557511365115634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 633.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:04<00:00, 1084.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.44337905624213164\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 590.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:17<00:00, 62.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 61.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-8\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [01:24<00:00, 53.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:03<00:00, 1423.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4794656604971294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 635.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:03<00:00, 1170.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4489498006539761\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 592.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:18<00:00, 62.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 63.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-9\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [01:22<00:00, 54.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:02<00:00, 1510.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.46870250510898176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 625.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [00:04<00:00, 1063.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4409309824844639\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 498/498 [00:00<00:00, 597.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1122/1122 [00:17<00:00, 62.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:02<00:00, 58.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "Semantic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              73.8 ±  2.2  -            -            \n",
      "  Balanced Accuracy     70.8 ±  2.4  -            -            \n",
      "  F1                    73.0 ±  2.4  63.0 ±  3.5  79.8 ±  1.6  \n",
      "  G-mean                70.8 ±  2.5  69.1 ±  2.8  69.1 ±  2.8  \n",
      "  Precision             73.8 ±  2.3  73.4 ±  3.1  74.0 ±  1.9  \n",
      "  Recall                73.8 ±  2.2  55.2 ±  3.9  86.4 ±  1.5  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              73.4 ±  1.7  -            -            \n",
      "  Balanced Accuracy     70.3 ±  1.9  -            -            \n",
      "  F1                    72.4 ±  1.9  62.1 ±  2.9  79.5 ±  1.2  \n",
      "  G-mean                70.2 ±  2.0  68.3 ±  2.4  68.3 ±  2.4  \n",
      "  Precision             73.3 ±  1.8  73.0 ±  2.6  73.6 ±  1.6  \n",
      "  Recall                73.4 ±  1.7  54.1 ±  3.8  86.4 ±  1.7  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              76.0 ±  1.3  -            -            \n",
      "  Balanced Accuracy     74.2 ±  1.4  -            -            \n",
      "  F1                    75.7 ±  1.3  68.5 ±  1.8  80.6 ±  1.2  \n",
      "  G-mean                74.2 ±  1.4  73.5 ±  1.5  73.5 ±  1.5  \n",
      "  Precision             75.8 ±  1.3  72.9 ±  2.3  77.8 ±  1.2  \n",
      "  Recall                76.0 ±  1.3  64.7 ±  2.9  83.6 ±  2.2  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symb+Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              77.3 ±  1.4  -            -            \n",
      "  Balanced Accuracy     75.6 ±  1.6  -            -            \n",
      "  F1                    77.0 ±  1.5  70.3 ±  2.2  81.6 ±  1.1  \n",
      "  G-mean                75.5 ±  1.6  75.0 ±  1.8  75.0 ±  1.8  \n",
      "  Precision             77.1 ±  1.5  74.4 ±  1.8  78.9 ±  1.5  \n",
      "  Recall                77.3 ±  1.4  66.7 ±  3.2  84.4 ±  1.4  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "ALL:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              77.4 ±  1.4  -            -            \n",
      "  Balanced Accuracy     75.7 ±  1.6  -            -            \n",
      "  F1                    77.1 ±  1.5  70.4 ±  2.2  81.7 ±  1.1  \n",
      "  G-mean                75.6 ±  1.6  75.1 ±  1.8  75.1 ±  1.8  \n",
      "  Precision             77.2 ±  1.4  74.6 ±  1.7  79.0 ±  1.6  \n",
      "  Recall                77.4 ±  1.4  66.7 ±  3.2  84.6 ±  1.3  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"fnkdd\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    semantics_quality = Metrics()\n",
    "    inf3_quality = Metrics()\n",
    "    inf2_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        bootstrap_size = 0\n",
    "        if bootstrap_size != 0:\n",
    "            bootstrap_factor = bootstrap_size / len(train)\n",
    "            bootstrap = np.random.uniform(size=len(train)) < bootstrap_factor\n",
    "            train=train[bootstrap]\n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference2 = Inference()\n",
    "        inference3 = Inference()\n",
    "        semantics = Semantics()\n",
    "        \n",
    "        print(\"semantic\")\n",
    "        semantics.train(body[train],labels[train])\n",
    "        ypred = semantics.predict(body[test])\n",
    "        semantics_quality.update(labels[test], ypred)\n",
    "    \n",
    "        print(\"trustex\")\n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        print(\"swarog\")\n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "\n",
    "        print(\"inference\")\n",
    "        inference2.train(trustex.graph_knn, swarog.train_prob, None, labels[train])\n",
    "        newpred = inference2.predict(trustex.graph_test_knn, swarog.test_prob, None)\n",
    "        inf2_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "        inference3.train(trustex.graph_knn, swarog.train_prob, semantics.trainX, labels[train])\n",
    "        newpred = inference3.predict(trustex.graph_test_knn, swarog.test_prob, semantics.testX)\n",
    "        inf3_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Semantic:\")\n",
    "semantics_quality.print_table()\n",
    "\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "\n",
    "print(\"Symb+Deep:\")\n",
    "inf2_quality.print_table()\n",
    "\n",
    "print(\"ALL:\")\n",
    "inf3_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279a4e6",
   "metadata": {},
   "source": [
    "## covidfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "7a564350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 10201\n",
      "labels fake= 9727 real= 474\n",
      "fold-0\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [00:11<00:00, 804.83it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [00:02<00:00, 4027.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6416482573104035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:01<00:00, 579.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [00:02<00:00, 3840.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6022464158873989\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:01<00:00, 577.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2295/2295 [00:11<00:00, 199.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:01<00:00, 195.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-1\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 783.91it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 4052.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6426120840712041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 573.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3826.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.603074574745465\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 579.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 201.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 199.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-2\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 785.34it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 4026.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6433784806689707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 586.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3817.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6039246746211283\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 580.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 199.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 196.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-3\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 775.81it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 4013.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6427887549912531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:03<00:00, 304.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3706.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.60307345211235\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 558.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 199.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 197.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-4\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:12<00:00, 758.82it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3970.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6434609272152707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 578.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3705.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6041686621461979\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 574.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 199.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 197.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-5\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 769.44it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3984.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6436223643698211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 577.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3674.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6038904451770066\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 575.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 201.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 200.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-6\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 784.47it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3933.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6437427049479205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 583.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3680.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6040509712926767\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 573.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 201.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 199.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-7\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 765.74it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3959.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6446890674715492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 563.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3695.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6054183837792178\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 569.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 201.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 199.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-8\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:12<00:00, 764.48it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3959.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6410904825624055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 580.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3688.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6011515437532513\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 582.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 199.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 196.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-9\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:11<00:00, 775.08it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 4010.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.641293988939949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 576.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9181/9181 [00:02<00:00, 3654.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6016977018324124\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1020/1020 [00:01<00:00, 565.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2296/2296 [00:11<00:00, 201.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:01<00:00, 200.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "Semantic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              97.4 ±  0.3  -            -            \n",
      "  Balanced Accuracy     74.0 ±  2.0  -            -            \n",
      "  F1                    97.0 ±  0.3  63.4 ±  4.0  98.7 ±  0.1  \n",
      "  G-mean                70.1 ±  2.7  69.2 ±  2.8  69.2 ±  2.8  \n",
      "  Precision             97.3 ±  0.4  93.4 ±  6.1  97.5 ±  0.2  \n",
      "  Recall                97.4 ±  0.3  48.1 ±  4.0  99.8 ±  0.2  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              97.4 ±  0.4  -            -            \n",
      "  Balanced Accuracy     73.1 ±  3.5  -            -            \n",
      "  F1                    96.9 ±  0.5  61.9 ±  6.7  98.6 ±  0.2  \n",
      "  G-mean                68.8 ±  4.8  67.9 ±  5.1  67.9 ±  5.1  \n",
      "  Precision             97.3 ±  0.4  93.8 ±  3.1  97.5 ±  0.3  \n",
      "  Recall                97.4 ±  0.4  46.4 ±  7.0  99.9 ±  0.1  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              97.6 ±  0.3  -            -            \n",
      "  Balanced Accuracy     80.1 ±  3.1  -            -            \n",
      "  F1                    97.4 ±  0.3  70.2 ±  4.4  98.8 ±  0.2  \n",
      "  G-mean                78.0 ±  3.8  77.6 ±  4.0  77.6 ±  4.0  \n",
      "  Precision             97.5 ±  0.3  84.0 ±  5.6  98.1 ±  0.3  \n",
      "  Recall                97.6 ±  0.3  60.7 ±  6.3  99.4 ±  0.3  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symb+Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.3 ±  0.3  -            -            \n",
      "  Balanced Accuracy     86.9 ±  2.8  -            -            \n",
      "  F1                    98.2 ±  0.3  79.8 ±  3.8  99.1 ±  0.2  \n",
      "  G-mean                86.1 ±  3.2  86.0 ±  3.2  86.0 ±  3.2  \n",
      "  Precision             98.2 ±  0.3  86.4 ±  5.0  98.8 ±  0.3  \n",
      "  Recall                98.3 ±  0.3  74.4 ±  5.7  99.4 ±  0.2  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "ALL:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.3 ±  0.4  -            -            \n",
      "  Balanced Accuracy     87.0 ±  2.8  -            -            \n",
      "  F1                    98.2 ±  0.4  80.2 ±  4.1  99.1 ±  0.2  \n",
      "  G-mean                86.3 ±  3.1  86.1 ±  3.1  86.1 ±  3.1  \n",
      "  Precision             98.2 ±  0.4  87.1 ±  5.4  98.8 ±  0.3  \n",
      "  Recall                98.3 ±  0.4  74.6 ±  5.5  99.4 ±  0.3  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"covidfn\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    semantics_quality = Metrics()\n",
    "    inf3_quality = Metrics()\n",
    "    inf2_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        bootstrap_size = 0\n",
    "        if bootstrap_size != 0:\n",
    "            bootstrap_factor = bootstrap_size / len(train)\n",
    "            bootstrap = np.random.uniform(size=len(train)) < bootstrap_factor\n",
    "            train=train[bootstrap]\n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference2 = Inference()\n",
    "        inference3 = Inference()\n",
    "        semantics = Semantics()\n",
    "        \n",
    "        print(\"semantic\")\n",
    "        semantics.train(body[train],labels[train])\n",
    "        ypred = semantics.predict(body[test])\n",
    "        semantics_quality.update(labels[test], ypred)\n",
    "    \n",
    "        print(\"trustex\")\n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        print(\"swarog\")\n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "\n",
    "        print(\"inference\")\n",
    "        inference2.train(trustex.graph_knn, swarog.train_prob, None, labels[train])\n",
    "        newpred = inference2.predict(trustex.graph_test_knn, swarog.test_prob, None)\n",
    "        inf2_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "        inference3.train(trustex.graph_knn, swarog.train_prob, semantics.trainX, labels[train])\n",
    "        newpred = inference3.predict(trustex.graph_test_knn, swarog.test_prob, semantics.testX)\n",
    "        inf3_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Semantic:\")\n",
    "semantics_quality.print_table()\n",
    "\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "\n",
    "print(\"Symb+Deep:\")\n",
    "inf2_quality.print_table()\n",
    "\n",
    "print(\"ALL:\")\n",
    "inf3_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8432ab6",
   "metadata": {},
   "source": [
    "## liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "6ae6bcea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading liar\n",
      "total_number_of_claims= 8061\n",
      "labels fake= 3554 real= 4507\n",
      "fold-0\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7254/7254 [00:09<00:00, 740.35it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7254/7254 [00:02<00:00, 3364.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.435177912552006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 807/807 [00:01<00:00, 673.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7254/7254 [00:01<00:00, 3824.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3702723209734244\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 807/807 [00:01<00:00, 664.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:08<00:00, 201.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 199.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-1\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:09<00:00, 740.20it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3718.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.43620986814729923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 700.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3841.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3719920066395006\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 668.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:08<00:00, 201.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 199.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-2\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:09<00:00, 730.00it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3821.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.43608186766336027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 649.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3865.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3715081571067758\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 688.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:09<00:00, 199.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 197.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-3\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:09<00:00, 750.72it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3755.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4361113804485705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 682.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3918.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3715639727317964\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 683.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:09<00:00, 200.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 197.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-4\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:10<00:00, 709.52it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3759.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.43717518664249316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 693.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3832.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.37196019611223585\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 688.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:09<00:00, 201.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 199.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-5\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:10<00:00, 722.46it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3741.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.43579897152982655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 685.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3817.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.37197240746819477\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 683.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:09<00:00, 200.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 196.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-6\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:09<00:00, 734.68it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3706.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4367373158491995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 686.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3812.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.37121661674174894\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 686.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:09<00:00, 199.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 197.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-7\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:10<00:00, 706.53it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3751.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.43573975920937186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 682.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3781.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3718538508523232\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 682.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:09<00:00, 201.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 199.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-8\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:10<00:00, 712.11it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3734.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.43592668599671913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 673.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3867.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.37196753984636455\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 542.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:08<00:00, 201.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 199.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-9\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:09<00:00, 748.41it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3733.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.4367811032652332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 669.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7255/7255 [00:01<00:00, 3841.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.37203080815053874\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:01<00:00, 699.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1814/1814 [00:08<00:00, 201.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:01<00:00, 198.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "Semantic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              63.3 ±  1.6  -            -            \n",
      "  Balanced Accuracy     62.0 ±  1.6  -            -            \n",
      "  F1                    62.9 ±  1.6  68.9 ±  1.4  55.2 ±  2.0  \n",
      "  G-mean                62.0 ±  1.6  61.1 ±  1.7  61.1 ±  1.7  \n",
      "  Precision             63.0 ±  1.6  65.4 ±  1.2  59.8 ±  2.2  \n",
      "  Recall                63.3 ±  1.6  72.8 ±  1.8  51.2 ±  2.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              62.6 ±  0.8  -            -            \n",
      "  Balanced Accuracy     61.4 ±  0.7  -            -            \n",
      "  F1                    62.2 ±  0.8  68.3 ±  0.9  54.5 ±  0.9  \n",
      "  G-mean                61.4 ±  0.7  60.5 ±  0.7  60.5 ±  0.7  \n",
      "  Precision             62.3 ±  0.8  65.0 ±  0.5  58.8 ±  1.2  \n",
      "  Recall                62.6 ±  0.8  71.9 ±  1.6  50.8 ±  1.3  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              65.6 ±  1.9  -            -            \n",
      "  Balanced Accuracy     64.6 ±  1.9  -            -            \n",
      "  F1                    65.3 ±  1.9  70.3 ±  1.8  58.9 ±  2.1  \n",
      "  G-mean                64.5 ±  1.9  64.0 ±  1.9  64.0 ±  1.9  \n",
      "  Precision             65.3 ±  2.0  67.8 ±  1.4  62.2 ±  2.7  \n",
      "  Recall                65.6 ±  1.9  73.1 ±  2.4  56.0 ±  1.9  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symb+Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              66.3 ±  1.8  -            -            \n",
      "  Balanced Accuracy     65.4 ±  1.8  -            -            \n",
      "  F1                    66.1 ±  1.8  70.9 ±  1.5  60.0 ±  2.3  \n",
      "  G-mean                65.4 ±  1.8  64.9 ±  1.9  64.9 ±  1.9  \n",
      "  Precision             66.1 ±  1.8  68.6 ±  1.6  63.0 ±  2.2  \n",
      "  Recall                66.3 ±  1.8  73.4 ±  2.0  57.4 ±  2.7  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "ALL:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              66.8 ±  2.0  -            -            \n",
      "  Balanced Accuracy     65.8 ±  2.0  -            -            \n",
      "  F1                    66.5 ±  2.0  71.3 ±  1.8  60.4 ±  2.4  \n",
      "  G-mean                65.8 ±  2.0  65.3 ±  2.0  65.3 ±  2.0  \n",
      "  Precision             66.6 ±  2.0  68.9 ±  1.6  63.6 ±  2.7  \n",
      "  Recall                66.8 ±  2.0  74.0 ±  2.3  57.6 ±  2.6  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"liar\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    semantics_quality = Metrics()\n",
    "    inf3_quality = Metrics()\n",
    "    inf2_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        bootstrap_size = 0\n",
    "        if bootstrap_size != 0:\n",
    "            bootstrap_factor = bootstrap_size / len(train)\n",
    "            bootstrap = np.random.uniform(size=len(train)) < bootstrap_factor\n",
    "            train=train[bootstrap]\n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference2 = Inference()\n",
    "        inference3 = Inference()\n",
    "        semantics = Semantics()\n",
    "        \n",
    "        print(\"semantic\")\n",
    "        semantics.train(body[train],labels[train])\n",
    "        ypred = semantics.predict(body[test])\n",
    "        semantics_quality.update(labels[test], ypred)\n",
    "    \n",
    "        print(\"trustex\")\n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        print(\"swarog\")\n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "\n",
    "        print(\"inference\")\n",
    "        inference2.train(trustex.graph_knn, swarog.train_prob, None, labels[train])\n",
    "        newpred = inference2.predict(trustex.graph_test_knn, swarog.test_prob, None)\n",
    "        inf2_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "        inference3.train(trustex.graph_knn, swarog.train_prob, semantics.trainX, labels[train])\n",
    "        newpred = inference3.predict(trustex.graph_test_knn, swarog.test_prob, semantics.testX)\n",
    "        inf3_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Semantic:\")\n",
    "semantics_quality.print_table()\n",
    "\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "\n",
    "print(\"Symb+Deep:\")\n",
    "inf2_quality.print_table()\n",
    "\n",
    "print(\"ALL:\")\n",
    "inf3_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389f4fb",
   "metadata": {},
   "source": [
    "## mmcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "9d93281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mmcovid\n",
      "total_number_of_claims= 7332\n",
      "labels fake= 2028 real= 5304\n",
      "fold-0\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6598/6598 [00:38<00:00, 172.57it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6598/6598 [00:03<00:00, 2192.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32675068766867355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 734/734 [00:01<00:00, 641.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6598/6598 [00:04<00:00, 1404.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5550252361631562\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 734/734 [00:01<00:00, 579.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 119.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 116.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-1\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6598/6598 [00:38<00:00, 170.96it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6598/6598 [00:02<00:00, 2267.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32444689380417124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 734/734 [00:01<00:00, 617.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6598/6598 [00:04<00:00, 1516.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5565031603947783\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 734/734 [00:01<00:00, 591.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 120.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 116.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-2\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:38<00:00, 169.77it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2262.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3234717951063957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 639.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1500.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5568051819431932\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 609.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 119.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 117.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-3\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:39<00:00, 165.99it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2209.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3248138965850563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 648.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1522.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5572621228321133\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 596.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 118.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 124.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-4\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:38<00:00, 170.13it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2264.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32757259560131147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 634.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1455.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5552816434285384\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 592.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 119.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 118.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-5\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:38<00:00, 170.72it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2281.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32241555395588106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 635.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1388.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5578110461229404\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 590.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 119.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 117.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-6\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:38<00:00, 171.58it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2271.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32769245146765613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 628.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1514.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.5574498510708638\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 596.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 119.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 116.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-7\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:38<00:00, 170.67it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2222.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32016774306164425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 630.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1512.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.547680591730261\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 591.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 119.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 115.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-8\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:38<00:00, 169.80it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2231.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.3259436177244116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 633.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1490.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.557178621463908\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 463.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 120.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 116.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "fold-9\n",
      "semantic\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:39<00:00, 165.36it/s]\n",
      "/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['because', 'couldn', 'didn', 'doesn', 'down', 'here', 'hers', 'ma', 'mightn', 'most', 'my', 'ourselves', 'should', 'some', 'such', 'themselves', 'those', 'wasn', 'wouldn', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:02<00:00, 2310.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.32504011167114216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 631.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustex\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6599/6599 [00:04<00:00, 1465.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.55790444409433\n",
      "Create test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733/733 [00:01<00:00, 596.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swarog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1650/1650 [00:13<00:00, 118.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:01<00:00, 121.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "Semantic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              81.6 ±  1.0  -            -            \n",
      "  Balanced Accuracy     68.8 ±  1.6  -            -            \n",
      "  F1                    79.1 ±  1.3  88.4 ±  0.6  54.6 ±  3.2  \n",
      "  G-mean                67.6 ±  1.8  62.5 ±  2.5  62.5 ±  2.5  \n",
      "  Precision             82.3 ±  1.2  81.0 ±  0.8  85.7 ±  2.7  \n",
      "  Recall                81.6 ±  1.0  97.4 ±  0.5  40.1 ±  3.1  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              90.8 ±  1.0  -            -            \n",
      "  Balanced Accuracy     86.5 ±  1.4  -            -            \n",
      "  F1                    90.5 ±  1.1  93.8 ±  0.7  82.1 ±  2.0  \n",
      "  G-mean                86.3 ±  1.5  85.9 ±  1.6  85.9 ±  1.6  \n",
      "  Precision             90.6 ±  1.0  91.6 ±  0.9  88.3 ±  2.2  \n",
      "  Recall                90.8 ±  1.0  96.1 ±  0.8  76.8 ±  2.6  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              93.0 ±  1.0  -            -            \n",
      "  Balanced Accuracy     88.8 ±  1.8  -            -            \n",
      "  F1                    92.8 ±  1.1  95.3 ±  0.6  86.2 ±  2.2  \n",
      "  G-mean                88.7 ±  1.8  88.2 ±  2.0  88.2 ±  2.0  \n",
      "  Precision             93.1 ±  0.9  92.5 ±  1.2  94.7 ±  1.3  \n",
      "  Recall                93.0 ±  1.0  98.3 ±  0.5  79.2 ±  3.6  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symb+Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              94.5 ±  0.6  -            -            \n",
      "  Balanced Accuracy     91.7 ±  1.2  -            -            \n",
      "  F1                    94.4 ±  0.7  96.3 ±  0.4  89.6 ±  1.3  \n",
      "  G-mean                91.7 ±  1.2  91.5 ±  1.3  91.5 ±  1.3  \n",
      "  Precision             94.5 ±  0.6  94.7 ±  0.9  94.1 ±  1.7  \n",
      "  Recall                94.5 ±  0.6  97.9 ±  0.7  85.5 ±  2.6  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "ALL:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              94.5 ±  0.6  -            -            \n",
      "  Balanced Accuracy     91.7 ±  1.0  -            -            \n",
      "  F1                    94.4 ±  0.6  96.2 ±  0.4  89.5 ±  1.1  \n",
      "  G-mean                91.6 ±  1.0  91.5 ±  1.1  91.5 ±  1.1  \n",
      "  Precision             94.5 ±  0.6  94.6 ±  0.8  94.1 ±  1.9  \n",
      "  Recall                94.5 ±  0.6  97.9 ±  0.7  85.5 ±  2.3  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"mmcovid\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    semantics_quality = Metrics()\n",
    "    inf3_quality = Metrics()\n",
    "    inf2_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        bootstrap_size = 0\n",
    "        if bootstrap_size != 0:\n",
    "            bootstrap_factor = bootstrap_size / len(train)\n",
    "            bootstrap = np.random.uniform(size=len(train)) < bootstrap_factor\n",
    "            train=train[bootstrap]\n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference2 = Inference()\n",
    "        inference3 = Inference()\n",
    "        semantics = Semantics()\n",
    "        \n",
    "        print(\"semantic\")\n",
    "        semantics.train(body[train],labels[train])\n",
    "        ypred = semantics.predict(body[test])\n",
    "        semantics_quality.update(labels[test], ypred)\n",
    "    \n",
    "        print(\"trustex\")\n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        print(\"swarog\")\n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "\n",
    "        print(\"inference\")\n",
    "        inference2.train(trustex.graph_knn, swarog.train_prob, None, labels[train])\n",
    "        newpred = inference2.predict(trustex.graph_test_knn, swarog.test_prob, None)\n",
    "        inf2_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "        inference3.train(trustex.graph_knn, swarog.train_prob, semantics.trainX, labels[train])\n",
    "        newpred = inference3.predict(trustex.graph_test_knn, swarog.test_prob, semantics.testX)\n",
    "        inf3_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Semantic:\")\n",
    "semantics_quality.print_table()\n",
    "\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "\n",
    "print(\"Symb+Deep:\")\n",
    "inf2_quality.print_table()\n",
    "\n",
    "print(\"ALL:\")\n",
    "inf3_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d63bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env.hator",
   "language": "python",
   "name": "env.hator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "375.498px",
    "width": "241.499px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "788px",
    "left": "10px",
    "top": "150px",
    "width": "158.95px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

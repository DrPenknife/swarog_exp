{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c253dd-4f4d-4d21-95ad-b13158f36ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b8c529a-ae5c-4661-8a68-f4e3bb333b80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1410)\n",
    "\n",
    "class DatasetLoader:\n",
    "    def get(self, name):\n",
    "        print(f\"loading {name}\")\n",
    "        if name == \"fnkdd\":\n",
    "            data = pd.read_csv(\"/home/rkozik/Desktop/swarog_exp_disk/datasets/fakenewskdd/train.csv\",sep=\"\\t\")\n",
    "            data.head()\n",
    "            body = data[\"text\"].values\n",
    "            labels = 1-data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"mmcovid\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/mmcovid/news_collection.csv\",sep=\"\\t\")\n",
    "            data[\"label\"] = [ 1 if v ==\"fake\" else 0 for v in data[\"label\"]]\n",
    "            data[\"text\"] = [ str(v) for v in data[\"text\"]]\n",
    "            data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"liar\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/liar.csv\", sep=\"\\t\",encoding=\"utf-8\")\n",
    "            def mpx(x):\n",
    "                if x in [0,2]:\n",
    "                    return 0\n",
    "                elif x in [4,5]:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            data[\"text\"] = data[\"statement\"]\n",
    "            data[\"label\"] = [mpx(x) for x in data[\"label\"]]\n",
    "            data=data[ data[\"label\"] != -1] \n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "\n",
    "        if name == \"covidfn\":\n",
    "            data = pd.read_csv(\"covid_fake_news.csv\", sep=\",\")\n",
    "            body = data[\"headlines\"].values\n",
    "            labels = data[\"outcome\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        return body, labels, total_number_of_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec5cb3-4b6a-42d7-85ee-b5af2327ed5b",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d1d03d0-9868-4c57-8677-c8ccb6f2a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.scores = {\n",
    "            'Accuracy': {'func': accuracy_score},\n",
    "            'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "            'F1': {'func': f1_score},\n",
    "            'Precision': {'func': precision_score},\n",
    "            'Recall': {'func': recall_score},\n",
    "            'G-mean': {'func': geometric_mean_score}\n",
    "        }\n",
    "        \n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            score_dict[\"list\"] = []\n",
    "            score_dict[\"lab\"] = []\n",
    "\n",
    "    def update(self, actual, prediction):\n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\",\"G-mean\"]:\n",
    "                scorvaln = score_dict['func'](actual, prediction, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](actual, prediction, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](actual, prediction)\n",
    "                score_dict['list'].append(scorval)\n",
    "                \n",
    "    def print_table(self, labels=None):\n",
    "        # Print stats\n",
    "        scores = self.scores\n",
    "        numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "        scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "        head = \"  %-20s  %-10s  \" +  numlabels * \" %-10s  \" \n",
    "        headv = [\"Score\", \"Average\"]\n",
    "        if labels:\n",
    "            headv.extend([labels[i] for i in range(numlabels)])\n",
    "        else:\n",
    "            headv.extend([\"Lab:\"+str(i+1) for i in range(numlabels)])\n",
    "        row=head % tuple(headv)\n",
    "        # table header\n",
    "        print(\"―\"*len(row))\n",
    "        print(row)\n",
    "        print(\"―\"*len(row))\n",
    "        # table rows\n",
    "        for score_name, score_dict in sorted(scores.items()) :\n",
    "            headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "            for i in range(numlabels):\n",
    "                if score_name in [\"F1\",\"Precision\",\"Recall\", \"G-mean\"]:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels* \"%4.1f ± %4.1f  \"\n",
    "                    vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                    headv.append(np.mean(vals)*100)\n",
    "                    headv.append(np.std(vals)*100)\n",
    "                else:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels * \"%-11s  \" \n",
    "                    headv.append(\"-\")\n",
    "            print(head % tuple(headv))\n",
    "        print(\"―\"*len(row))\n",
    "\n",
    "\n",
    "def get_graph_node_stats(vec, nearestDocIDs, y_train, bodyTrainTFIDF):   \n",
    "    vecdense = vec.toarray()[0]\n",
    "    docids = nearestDocIDs\n",
    "    trlabels = np.array(y_train)\n",
    "    labsum = trlabels[docids].sum()\n",
    "    \n",
    "    ivec = []\n",
    "    labmask = []\n",
    "    for hitdocid in docids:\n",
    "        value=bodyTrainTFIDF[hitdocid].toarray()[0]\n",
    "        intersection = (vecdense>0)*(value>0)\n",
    "        ivec.append(intersection.sum())\n",
    "        labmask.append(trlabels[hitdocid])\n",
    "        \n",
    "    masked_ivec =  np.array(ivec)*np.array(labmask)   \n",
    "    masked_ivec_neg =  np.array(ivec)*(-1*(np.array(labmask)-1)) \n",
    "    ivec = np.array(ivec)\n",
    "    masked_ivec = np.array(masked_ivec)\n",
    "    masked_ivec_neg = np.array(masked_ivec_neg)\n",
    "    \n",
    "    newvec = [labsum, (vecdense>0).sum(),ivec.max(), ivec.max(), masked_ivec.max(), masked_ivec.min(), masked_ivec_neg.max(), masked_ivec_neg.min()]\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842a6fe-2dea-45f6-91c0-f98d14065865",
   "metadata": {},
   "source": [
    "# Swarog Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "551840b1-0372-4cf3-be75-520adda5ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "from bentoml.io import JSON\n",
    "from annoy import AnnoyIndex\n",
    "import re\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle5 as pickle\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    " \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "class SwarogModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=4, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, \n",
    "                                           truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, \n",
    "                                       output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return np.array(allembeds)\n",
    "    \n",
    "    def train(self, body, labels):\n",
    "        embeddings = self.transform(body)\n",
    "        self.cls = LogisticRegression(max_iter=1000)\n",
    "        self.cls.fit(embeddings, labels)\n",
    "        self.train_prob = self.cls.predict_proba(embeddings)\n",
    "        \n",
    "    def predict(self, body):\n",
    "        embeddings = self.transform(body)\n",
    "        self.test_prob = self.cls.predict_proba(embeddings)\n",
    "        return  self.cls.predict(embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49565d-f7a3-4ebf-ba03-7bb29d13a3d8",
   "metadata": {},
   "source": [
    "# Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "958f52d6-f007-4eb4-bec6-ec915a22bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', \"'\"]\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "    \n",
    "\n",
    "class TrustexModel:\n",
    "    def __init__(self):\n",
    "        # Lemmatize the stop words\n",
    "        self.tokenizer=LemmaTokenizer()\n",
    "        self.token_stop = self.tokenizer(' '.join(stop_words))\n",
    "        \n",
    "    def tfidf(self,body):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=self.token_stop)\n",
    "        self.tfidf_vectorizer.fit(body)\n",
    "        self.vocabulary_tfidf_words = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.graph_knn = []\n",
    "        self.train_labels = labels\n",
    "        from tqdm import tqdm\n",
    "        for id, topIDs in tqdm(enumerate(knn_idx), total=knn_idx.shape[0]):\n",
    "            vec = self.bodyTrainTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], labels, self.bodyTrainTFIDF)\n",
    "            self.graph_knn.append(newvec)\n",
    "        print(\"avg. nodes sim.=\",np.mean([x[2]/x[1] for x in self.graph_knn]))\n",
    "\n",
    "    def graph_transform_test_data(self, body):\n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        self.graph_test_knn = []\n",
    "        for id, topIDs in tqdm(enumerate(knn_test_idx), total=knn_test_idx.shape[0]):\n",
    "            vec = self.bodyTestTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], self.train_labels, self.bodyTrainTFIDF)\n",
    "            self.graph_test_knn.append(newvec)        \n",
    "              \n",
    "    def train(self, body, labels):\n",
    "        print(\"Building similarity graph\")\n",
    "        self.tfidf(body)\n",
    "        self.create_graph(body, labels)\n",
    "          \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.graph_knn, labels)\n",
    "\n",
    "    def predict(self, body):\n",
    "        self.graph_transform_test_data(body)\n",
    "        y_pred = self.cls.predict(self.graph_test_knn)\n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ec0b7",
   "metadata": {},
   "source": [
    "# FTS Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "fcc807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class FTSSemantic:\n",
    "    def __init__(self):\n",
    "        self.con=sqlite3.connect(\"svo.db\")\n",
    "        \n",
    "    def build_index(self, body, labels, tblname=\"train\"): \n",
    "        print(\"Building index\")\n",
    "        self.con.execute(f\"drop table if exists {tblname}_fts\")\n",
    "        self.con.execute(f\"create VIRTUAL table if not exists {tblname}_fts USING fts5(txt, original, label)\")\n",
    "        \n",
    "        for docid, txt in tqdm(enumerate(body), total=len(labels)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            vecs = []\n",
    "            newtext=txt\n",
    "            for xsvo in svos:\n",
    "                svo = xsvo\n",
    "                if len(svo) < 3 and len(svo) > 1:\n",
    "                    svos = [svo[0],svo[1],\"\"]\n",
    "                if len(svo) == 3:\n",
    "                    line = \" \".join(svo)\n",
    "                    sentence = sp(line)\n",
    "                    w = \" \".join([word.lemma_ for word in sentence])\n",
    "                    vecs.append(w)\n",
    "            if len(vecs) > 0:\n",
    "                newtext=\" \".join(vecs)\n",
    "                \n",
    "            self.con.execute(f\"insert into {tblname}_fts values(?,?,?)\", \n",
    "                        [newtext,txt, str(labels[docid])])\n",
    "            \n",
    "        self.con.commit() \n",
    "        \n",
    "    def transformData(self, body):\n",
    "        X = []\n",
    "        for docid, txt in tqdm(enumerate(body), total=len(body)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            svos_vec=self.svos_features(svos)\n",
    "            X.append(svos_vec)\n",
    "        return X\n",
    "            \n",
    "    def createTrainData(self, body, labels):\n",
    "        print(\"Creating trainX\")\n",
    "        X = self.transformData(body)\n",
    "        with open(\"fts_train_X.pickle\",\"wb\") as fp:\n",
    "            pickle.dump(X, fp)\n",
    "        return X\n",
    "        \n",
    "    def train(self, body, labels, tblname=\"train\"):    \n",
    "        #self.build_index(body,labels, tblname)\n",
    "\n",
    "        #self.createTrainData(body,labels)\n",
    "        \n",
    "        with open(\"fts_train_X.pickle\",\"rb\") as fp:\n",
    "            X=pickle.load(fp)\n",
    "        \n",
    "        #self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls = RandomForestClassifier(max_depth=12)\n",
    "        self.cls.fit(X, labels)\n",
    "        \n",
    "    def svo2query(self,svo):\n",
    "        line = \" \".join(svo)\n",
    "        sentence = sp(line)\n",
    "        zlepek = [re.sub(r'[^a-zA-Z0-9]', ' ', word.lemma_) for word in sentence]\n",
    "        zlepek = [\" \".join(z.split()) for z in zlepek]\n",
    "        zlepek = [z for z in zlepek if len(z) > 1]\n",
    "        w = \" OR \".join(zlepek)\n",
    "        return w\n",
    "    \n",
    "    def svos_features(self,svos):\n",
    "        result = []\n",
    "        for xsvo in svos:\n",
    "            svo = xsvo\n",
    "            if len(svo) < 3 and len(svo) > 1:\n",
    "                svos = [svo[0],svo[1],\"\"]\n",
    "\n",
    "            if len(svo) == 3:\n",
    "                w = self.svo2query(svo)\n",
    "                #print(\"   \",w)\n",
    "                try:\n",
    "                    q=f\"\"\"\n",
    "                        with top10 as (\n",
    "                            select bm25(train_fts) as score, cast(label as int) as pred from train_fts  \n",
    "                             where txt  match \"{w}\" order by bm25(train_fts) limit 5\n",
    "                        )\n",
    "                        select min(score), avg(score), max(score), avg(pred), count(*), sum(pred) from top10\n",
    "                    \"\"\"\n",
    "                    fvector = self.con.execute(q).fetchall()[0]\n",
    "                    result.append(fvector)\n",
    "                    #print(w)\n",
    "                    #print(matchscore)\n",
    "                except (sqlite3.OperationalError):\n",
    "                    #print(w)\n",
    "                    result.append([0]*6)\n",
    "        if len(result) > 0:\n",
    "            return np.mean(result, axis=0)\n",
    "        else:\n",
    "            return [0]*6\n",
    "\n",
    "    def predict(self, body, labels):\n",
    "        pred = []\n",
    "        X = self.transformData(body)\n",
    "        return self.cls.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "6fba1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = FTSSemantic()\n",
    "sem.train(body[train],labels[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "71d2062e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:11<00:00, 89.83it/s]\n"
     ]
    }
   ],
   "source": [
    "ypred=sem.predict(body[test],labels[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "107d2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              95.9 ±  0.0  -            -            \n",
      "  Balanced Accuracy     60.2 ±  0.0  -            -            \n",
      "  F1                    94.8 ±  0.0  97.9 ±  0.0  32.3 ±  0.0  \n",
      "  G-mean                48.5 ±  0.0  45.5 ±  0.0  45.5 ±  0.0  \n",
      "  Precision             95.1 ±  0.0  96.2 ±  0.0  71.4 ±  0.0  \n",
      "  Recall                95.9 ±  0.0  99.6 ±  0.0  20.8 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "met = Metrics()\n",
    "met.update(labels[test], ypred)\n",
    "met.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f556a4b-df4e-454d-bd23-4ac3f3d87b22",
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "source": [
    "# Semantic relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cee2e4a1-1388-4769-a855-94eb6e61882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import pickle \n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "        \n",
    "class Semantic:\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.tokenizer=LemmaTokenizer()\n",
    "        self.token_stop = self.tokenizer(' '.join(stop_words))\n",
    "        self.con=sqlite3.connect(\"svo.db\")\n",
    "        \n",
    "    def extract_svo(self, body, labels, tblname=\"train\"):\n",
    "        self.con.execute(f\"drop table if exists {tblname}_svo\")\n",
    "        self.con.execute(f\"create table if not exists {tblname}_svo(sub, verb, obj, docid, label)\")\n",
    "        self.con.commit()\n",
    "        \n",
    "        for docid, txt in tqdm(enumerate(body), total=len(labels)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            for svo in svos:\n",
    "                if len(svo) == 3:\n",
    "                    self.con.execute(f\"insert into {tblname}_svo values(?,?,?,?,?)\", \n",
    "                        [svo[0],self.ps.stem(svo[1]),svo[2], docid, int(labels[docid])])\n",
    "            self.con.commit()\n",
    "            \n",
    "    def svo_lema(self, tblname=\"train\"):\n",
    "        self.con.execute(f\"drop table if exists {tblname}_svo_lema\")\n",
    "        self.con.execute(f\"create table if not exists {tblname}_svo_lema(lema, docid, label)\")\n",
    "        self.con.commit()\n",
    "        \n",
    "        redux = pd.read_sql(f\"\"\"\n",
    "        select sub || \" \" || verb || \" \" || obj as redux, docid, label from {tblname}_svo \n",
    "        -- limit 10\n",
    "        \"\"\", sem.con).values\n",
    "\n",
    "        for line in tqdm(redux,total=redux.shape[0]):\n",
    "            sentence = sp(line[0])\n",
    "            w = \" \".join([word.lemma_ for word in sentence])\n",
    "            self.con.execute(f\"insert into {tblname}_svo_lema values(?,?,?)\", [w, line[1],line[2]]) \n",
    "            \n",
    "        self.con.commit()\n",
    "        \n",
    "    def tfidf(self,body):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=self.token_stop)\n",
    "        self.tfidf_vectorizer.fit(body)\n",
    "        self.vocabulary_tfidf_words = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.graph_knn = []\n",
    "        self.train_labels = labels\n",
    "        from tqdm import tqdm\n",
    "        for id, topIDs in tqdm(enumerate(knn_idx), total=knn_idx.shape[0]):\n",
    "            #print(body[id])\n",
    "            vec = self.bodyTrainTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], labels, self.bodyTrainTFIDF)\n",
    "            #print(\"  \",newvec)\n",
    "            self.graph_knn.append(newvec)\n",
    "        print(\"avg. nodes sim.=\",np.mean([x[2]/(x[1]+0.000001) for x in self.graph_knn]))\n",
    "        \n",
    "        with open(\"sem_graph_knn.pickle\",\"wb\") as fp:\n",
    "            pickle.dump(sem.graph_knn, fp)\n",
    "        \n",
    "    def transform_test_data(self, body):\n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        rsvo_body = self.right_svo[\"redux\"].values\n",
    "        return knn_test_d,knn_test_idx\n",
    "              \n",
    "    def train(self, body, labels):\n",
    "        print(\"Extracting SVO\")\n",
    "        #self.extract_svo(body,labels) \n",
    "        print(\"Lematizing SVO\")\n",
    "        #self.svo_lema()\n",
    "  \n",
    "    \n",
    "        rexux=pd.read_sql(f\"\"\"\n",
    "        select lema, label from train_svo_lema \n",
    "        --limit 10\n",
    "        \"\"\", sem.con)\n",
    "    \n",
    "        self.tfidf(rexux[\"lema\"].values)\n",
    "        #self.create_graph(rexux[\"lema\"].values,rexux[\"label\"].values)\n",
    "        \n",
    "        with open(\"sem_graph_knn.pickle\",\"rb\") as fp:\n",
    "            sem.graph_knn=pickle.load(fp)\n",
    "        \n",
    "        \n",
    "                  \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.graph_knn, rexux[\"label\"].values)\n",
    "        \n",
    "            \n",
    "#         rsvo_body = self.right_svo[\"redux\"].values\n",
    "#         print(self.right_svo.head())\n",
    "        \n",
    "#         print(\"Building similarity graph\")\n",
    "#         self.tfidf(rsvo_body)\n",
    "#         self.create_graph(rsvo_body, labels)\n",
    "          \n",
    "#         self.cls = LogisticRegression(max_iter=10000)\n",
    "#         self.cls.fit(self.graph_knn, labels)\n",
    "\n",
    "    def predict(self, body, labels):\n",
    "        print(\"Extracting SVO\")\n",
    "        #self.extract_svo(body,labels,\"test\") \n",
    "        print(\"Lematizing SVO\")\n",
    "        #self.svo_lema(\"test\")\n",
    "        \n",
    "            \n",
    "        rexux=pd.read_sql(f\"\"\"\n",
    "        select lema, label from train_svo_lema \n",
    "        --limit 10\n",
    "        \"\"\", sem.con)\n",
    "    \n",
    "        self.tfidf(rexux[\"lema\"].values)\n",
    "        \n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        \n",
    "        with open(\"sem_graph_knn.pickle\",\"rb\") as fp:\n",
    "            sem.graph_knn=pickle.load(fp)\n",
    "            \n",
    "        sem.graph_knn=np.array(sem.graph_knn)\n",
    "        \n",
    "        preds=[]\n",
    "        \n",
    "        for docid, txt in tqdm(enumerate(body), total=len(labels)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            vecs = []\n",
    "\n",
    "            for xsvo in svos:\n",
    "                svo = xsvo\n",
    "                if len(svo) < 3 and len(svo) > 1:\n",
    "                    svos = [svo[0],svo[1],\"\"]\n",
    "                \n",
    "                if len(svo) == 3:\n",
    "                    line = \" \".join([svo[0],self.ps.stem(svo[1]),svo[2]])\n",
    "                    sentence = sp(line)\n",
    "                    w = \" \".join([word.lemma_ for word in sentence])\n",
    "                    densev = self.tfidf_vectorizer.transform([w]) \n",
    "                    knn_test_d,knn_test_idx = self.nn.kneighbors(densev)\n",
    "                    nearest = knn_test_idx[0][0]\n",
    "                    nv = sem.graph_knn[nearest]\n",
    "                    vecs.append(self.cls.predict([nv])[0])\n",
    "            if len(vecs) > 0:\n",
    "                preds.append(np.mean(vecs))\n",
    "            else:\n",
    "                preds.append(0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "938df657-ee63-4388-b674-4da5c80aed4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SVO\n",
      "Lematizing SVO\n"
     ]
    }
   ],
   "source": [
    "sem = Semantic()\n",
    "sem.train(body[train],labels[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d3f9394c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SVO\n",
      "Lematizing SVO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:11<00:00, 90.24it/s]\n"
     ]
    }
   ],
   "source": [
    " pred = sem.predict(body[test], labels[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "37e569d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [1 if x > 0.5 else 0 for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ab449948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              95.5 ±  0.0  -            -            \n",
      "  Balanced Accuracy     53.1 ±  0.0  -            -            \n",
      "  F1                    93.6 ±  0.0  97.7 ±  0.0  11.5 ±  0.0  \n",
      "  G-mean                31.9 ±  0.0  25.0 ±  0.0  25.0 ±  0.0  \n",
      "  Precision             94.6 ±  0.0  95.6 ±  0.0  75.0 ±  0.0  \n",
      "  Recall                95.5 ±  0.0  99.9 ±  0.0   6.2 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "met = Metrics()\n",
    "met.update(labels[test], ypred)\n",
    "met.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8950be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3026a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "827cd041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem.tfidf_vectorizer.transform([\"the principle of , violat compulsory vacination\"]).todense()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "931d6b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     the principle of , violat compulsory vacination\n",
       "1             many false positive return the PCR test\n",
       "2                    COVID-19 relat influenza vaccine\n",
       "3    COVID-19 regist all death by respiratory failure\n",
       "4                there be a lot of PCR false positive\n",
       "5                         a lot of spread coronavirus\n",
       "6           a red ribbon put Households with , Grande\n",
       "7      their garbage bag put Households with , Grande\n",
       "8                               they handl Garbagemen\n",
       "9                         a safe way handl Garbagemen\n",
       "Name: lema, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"\"\"\n",
    "select lema, label from svo_lema \n",
    "limit 10\n",
    "\"\"\", sem.con)[\"lema\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e40af523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11977\n"
     ]
    }
   ],
   "source": [
    "redux_lema_labels = [x[0] for x in pd.read_sql(\"\"\"\n",
    "select label from svo \n",
    "\"\"\", sem.con).values]\n",
    "print(len(redux_lema_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2615fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 11977/11977 [00:14<00:00, 837.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_15611/1074183604.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  print(\"avg. nodes sim.=\",np.mean([x[2]/x[1] for x in self.graph_knn]))\n"
     ]
    }
   ],
   "source": [
    "pd.read_sql(\"\"\"\n",
    "select sub || \" \" || verb || \" \" || obj as redux from svo \n",
    "-- limit 10\n",
    "\"\"\", sem.con).values\n",
    "\n",
    "tm = TrustexModel()\n",
    "tm.train(redux_lema,redux_lema_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b992a48-181f-4b67-a320-939543e855ff",
   "metadata": {},
   "source": [
    "# Infrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5526269d-b19e-46e7-803f-8eada642f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Inference:\n",
    "    def train(self,graph, content, labels):\n",
    "        newX=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i])\n",
    "            newX.append(v2)\n",
    "            \n",
    "        self.inf = RandomForestClassifier(max_depth=12)\n",
    "        self.inf.fit(newX, labels)\n",
    "        \n",
    "    def predict(self, graph, content):\n",
    "        newTest=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i])\n",
    "            newTest.append(v2)\n",
    "    \n",
    "        return self.inf.predict(newTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baccae2-b3bd-4e07-a8cd-e55fe8801382",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827802d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 10201\n",
      "labels fake= 474 real= 9727\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "body, labels, total_number_of_claims = loader.get(\"covidfn\")\n",
    "X=range(0,total_number_of_claims)\n",
    "train, test = list(rskf.split(X, labels))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "315f84fb-918d-4814-9e04-3601b4f3fe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 10201\n",
      "labels fake= 474 real= 9727\n",
      "fold-0\n",
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [00:08<00:00, 1093.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6016417403761986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:00<00:00, 1091.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2295/2295 [00:12<00:00, 181.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:01<00:00, 189.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              97.4 ±  0.0  -            -            \n",
      "  Balanced Accuracy     75.8 ±  0.0  -            -            \n",
      "  F1                    97.0 ±  0.0  98.6 ±  0.0  64.9 ±  0.0  \n",
      "  G-mean                72.7 ±  0.0  72.0 ±  0.0  72.0 ±  0.0  \n",
      "  Precision             97.1 ±  0.0  97.7 ±  0.0  86.2 ±  0.0  \n",
      "  Recall                97.4 ±  0.0  99.6 ±  0.0  52.1 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.2 ±  0.0  -            -            \n",
      "  Balanced Accuracy     85.2 ±  0.0  -            -            \n",
      "  F1                    98.1 ±  0.0  99.1 ±  0.0  79.1 ±  0.0  \n",
      "  G-mean                84.2 ±  0.0  84.0 ±  0.0  84.0 ±  0.0  \n",
      "  Precision             98.1 ±  0.0  98.6 ±  0.0  89.5 ±  0.0  \n",
      "  Recall                98.2 ±  0.0  99.6 ±  0.0  70.8 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Both:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.3 ±  0.0  -            -            \n",
      "  Balanced Accuracy     90.2 ±  0.0  -            -            \n",
      "  F1                    98.3 ±  0.0  99.1 ±  0.0  82.1 ±  0.0  \n",
      "  G-mean                89.8 ±  0.0  89.8 ±  0.0  89.8 ±  0.0  \n",
      "  Precision             98.3 ±  0.0  99.1 ±  0.0  83.0 ±  0.0  \n",
      "  Recall                98.3 ±  0.0  99.2 ±  0.0  81.2 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"covidfn\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    inf_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference = Inference()\n",
    "    \n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "        \n",
    "        inference.train(trustex.graph_knn, swarog.train_prob, labels[train])\n",
    "        newpred = inference.predict(trustex.graph_test_knn, swarog.test_prob)\n",
    "        inf_quality.update(labels[test], newpred)\n",
    "\n",
    "        break\n",
    "\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "print(\"Both:\")\n",
    "inf_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a15b3-d116-4d04-af2f-d87dcf77ed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env.hator",
   "language": "python",
   "name": "env.hator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "375.498px",
    "width": "241.499px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "788px",
    "left": "10px",
    "top": "150px",
    "width": "158.95px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

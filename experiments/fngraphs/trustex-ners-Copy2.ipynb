{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c253dd-4f4d-4d21-95ad-b13158f36ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8c529a-ae5c-4661-8a68-f4e3bb333b80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1410)\n",
    "\n",
    "class DatasetLoader:\n",
    "    def get(self, name):\n",
    "        print(f\"loading {name}\")\n",
    "        if name == \"fnkdd\":\n",
    "            data = pd.read_csv(\"/home/rkozik/Desktop/swarog_exp_disk/datasets/fakenewskdd/train.csv\",sep=\"\\t\")\n",
    "            data.head()\n",
    "            body = data[\"text\"].values\n",
    "            labels = 1-data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"mmcovid\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/mmcovid/news_collection.csv\",sep=\"\\t\")\n",
    "            data[\"label\"] = [ 1 if v ==\"fake\" else 0 for v in data[\"label\"]]\n",
    "            data[\"text\"] = [ str(v) for v in data[\"text\"]]\n",
    "            data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        if name == \"liar\":\n",
    "            data = pd.read_csv(\"/media/rkozik/02FF-A831/data/swarog/datasets/liar.csv\", sep=\"\\t\",encoding=\"utf-8\")\n",
    "            def mpx(x):\n",
    "                if x in [0,2]:\n",
    "                    return 0\n",
    "                elif x in [4,5]:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            data[\"text\"] = data[\"statement\"]\n",
    "            data[\"label\"] = [mpx(x) for x in data[\"label\"]]\n",
    "            data=data[ data[\"label\"] != -1] \n",
    "            body = data[\"text\"].values\n",
    "            labels = data[\"label\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "\n",
    "        if name == \"covidfn\":\n",
    "            data = pd.read_csv(\"covid_fake_news.csv\", sep=\",\")\n",
    "            body = data[\"headlines\"].values\n",
    "            labels = 1 - data[\"outcome\"].values\n",
    "            total_number_of_claims = data.shape[0]\n",
    "            print(\"total_number_of_claims=\",total_number_of_claims)\n",
    "            print(\"labels fake=\",sum(labels),\"real=\", len(labels)-sum(labels))\n",
    "        \n",
    "        return body, labels, total_number_of_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec5cb3-4b6a-42d7-85ee-b5af2327ed5b",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d1d03d0-9868-4c57-8677-c8ccb6f2a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.scores = {\n",
    "            'Accuracy': {'func': accuracy_score},\n",
    "            'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "            'F1': {'func': f1_score},\n",
    "            'Precision': {'func': precision_score},\n",
    "            'Recall': {'func': recall_score},\n",
    "            'G-mean': {'func': geometric_mean_score}\n",
    "        }\n",
    "        \n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            score_dict[\"list\"] = []\n",
    "            score_dict[\"lab\"] = []\n",
    "\n",
    "    def update(self, actual, prediction):\n",
    "        for score_name, score_dict in self.scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\",\"G-mean\"]:\n",
    "                scorvaln = score_dict['func'](actual, prediction, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](actual, prediction, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](actual, prediction)\n",
    "                score_dict['list'].append(scorval)\n",
    "                \n",
    "    def print_table(self, labels=None):\n",
    "        # Print stats\n",
    "        scores = self.scores\n",
    "        numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "        scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "        head = \"  %-20s  %-10s  \" +  numlabels * \" %-10s  \" \n",
    "        headv = [\"Score\", \"Average\"]\n",
    "        if labels:\n",
    "            headv.extend([labels[i] for i in range(numlabels)])\n",
    "        else:\n",
    "            headv.extend([\"Lab:\"+str(i+1) for i in range(numlabels)])\n",
    "        row=head % tuple(headv)\n",
    "        # table header\n",
    "        print(\"―\"*len(row))\n",
    "        print(row)\n",
    "        print(\"―\"*len(row))\n",
    "        # table rows\n",
    "        for score_name, score_dict in sorted(scores.items()) :\n",
    "            headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "            for i in range(numlabels):\n",
    "                if score_name in [\"F1\",\"Precision\",\"Recall\", \"G-mean\"]:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels* \"%4.1f ± %4.1f  \"\n",
    "                    vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                    headv.append(np.mean(vals)*100)\n",
    "                    headv.append(np.std(vals)*100)\n",
    "                else:\n",
    "                    head = \"  %-20s  %4.1f ± %4.1f  \" + numlabels * \"%-11s  \" \n",
    "                    headv.append(\"-\")\n",
    "            print(head % tuple(headv))\n",
    "        print(\"―\"*len(row))\n",
    "\n",
    "\n",
    "def get_graph_node_stats(vec, nearestDocIDs, y_train, bodyTrainTFIDF):   \n",
    "    vecdense = vec.toarray()[0]\n",
    "    docids = nearestDocIDs\n",
    "    trlabels = np.array(y_train)\n",
    "    labsum = trlabels[docids].sum()\n",
    "    \n",
    "    ivec = []\n",
    "    labmask = []\n",
    "    for hitdocid in docids:\n",
    "        value=bodyTrainTFIDF[hitdocid].toarray()[0]\n",
    "        intersection = (vecdense>0)*(value>0)\n",
    "        ivec.append(intersection.sum())\n",
    "        labmask.append(trlabels[hitdocid])\n",
    "        \n",
    "    masked_ivec =  np.array(ivec)*np.array(labmask)   \n",
    "    masked_ivec_neg =  np.array(ivec)*(-1*(np.array(labmask)-1)) \n",
    "    ivec = np.array(ivec)\n",
    "    masked_ivec = np.array(masked_ivec)\n",
    "    masked_ivec_neg = np.array(masked_ivec_neg)\n",
    "    \n",
    "    newvec = [labsum, (vecdense>0).sum(),ivec.max(), ivec.max(), masked_ivec.max(), masked_ivec.min(), masked_ivec_neg.max(), masked_ivec_neg.min()]\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842a6fe-2dea-45f6-91c0-f98d14065865",
   "metadata": {},
   "source": [
    "# Swarog Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "551840b1-0372-4cf3-be75-520adda5ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "from bentoml.io import JSON\n",
    "from annoy import AnnoyIndex\n",
    "import re\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle5 as pickle\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    " \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "class SwarogModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=4, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, \n",
    "                                           truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, \n",
    "                                       output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return np.array(allembeds)\n",
    "    \n",
    "    def train(self, body, labels):\n",
    "        embeddings = self.transform(body)\n",
    "        self.cls = LogisticRegression(max_iter=1000)\n",
    "        self.cls.fit(embeddings, labels)\n",
    "        self.train_prob = self.cls.predict_proba(embeddings)\n",
    "        \n",
    "    def predict(self, body):\n",
    "        embeddings = self.transform(body)\n",
    "        self.test_prob = self.cls.predict_proba(embeddings)\n",
    "        return  self.cls.predict(embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49565d-f7a3-4ebf-ba03-7bb29d13a3d8",
   "metadata": {},
   "source": [
    "# Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "958f52d6-f007-4eb4-bec6-ec915a22bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rkozik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', \"'\"]\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "    \n",
    "\n",
    "class TrustexModel:\n",
    "    def __init__(self):\n",
    "        # Lemmatize the stop words\n",
    "        self.tokenizer=LemmaTokenizer()\n",
    "        self.token_stop = self.tokenizer(' '.join(stop_words))\n",
    "        \n",
    "    def tfidf(self,body):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=self.token_stop)\n",
    "        self.tfidf_vectorizer.fit(body)\n",
    "        self.vocabulary_tfidf_words = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        self.bodyTrainTFIDF = self.tfidf_vectorizer.transform(body)\n",
    "        \n",
    "    def create_graph(self, body, labels):\n",
    "        self.nn = NearestNeighbors(n_neighbors=10)\n",
    "        self.nn.fit(self.bodyTrainTFIDF)\n",
    "        knn_d,knn_idx = self.nn.kneighbors(self.bodyTrainTFIDF)\n",
    "        self.graph_knn = []\n",
    "        self.train_labels = labels\n",
    "        from tqdm import tqdm\n",
    "        for id, topIDs in tqdm(enumerate(knn_idx), total=knn_idx.shape[0]):\n",
    "            vec = self.bodyTrainTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], labels, self.bodyTrainTFIDF)\n",
    "            self.graph_knn.append(newvec)\n",
    "        print(\"avg. nodes sim.=\",np.mean([x[2]/x[1] for x in self.graph_knn]))\n",
    "\n",
    "    def graph_transform_test_data(self, body):\n",
    "        self.bodyTestTFIDF = self.tfidf_vectorizer.transform(body) \n",
    "        knn_test_d,knn_test_idx = self.nn.kneighbors(self.bodyTestTFIDF)\n",
    "        self.graph_test_knn = []\n",
    "        for id, topIDs in tqdm(enumerate(knn_test_idx), total=knn_test_idx.shape[0]):\n",
    "            vec = self.bodyTestTFIDF[id]\n",
    "            newvec = get_graph_node_stats(vec, topIDs[1:], self.train_labels, self.bodyTrainTFIDF)\n",
    "            self.graph_test_knn.append(newvec)        \n",
    "              \n",
    "    def train(self, body, labels):\n",
    "        print(\"Building similarity graph\")\n",
    "        self.tfidf(body)\n",
    "        self.create_graph(body, labels)\n",
    "          \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.graph_knn, labels)\n",
    "\n",
    "    def predict(self, body):\n",
    "        self.graph_transform_test_data(body)\n",
    "        y_pred = self.cls.predict(self.graph_test_knn)\n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47dab7",
   "metadata": {},
   "source": [
    "# FTS Semantics 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09453842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import traceback\n",
    "from rank_bm25 import BM25Okapi\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import traceback\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from subject_verb_object_extract import findSVOs, nlp\n",
    "import os\n",
    "import spacy\n",
    "import pickle \n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def call_it2(instance, name, arg):\n",
    "    \"indirect caller for instance methods and multiprocessing\"\n",
    "    return getattr(instance, name)(arg)\n",
    "\n",
    "\n",
    "class FTSSemantic2:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def tokenize_single(self, e):\n",
    "        docid, txt = e\n",
    "        tokens = nlp(txt)\n",
    "        svos = findSVOs(tokens)\n",
    "        proc_text = []\n",
    "        for xsvo in svos:\n",
    "            svo = xsvo\n",
    "            if len(svo) < 3 and len(svo) > 1:\n",
    "                svos = [svo[0],svo[1],\"\"]\n",
    "            if len(svo) == 3:\n",
    "                line = \" \".join(svo)\n",
    "                sentence = sp(line)\n",
    "                lemas =  [word.lemma_ for word in sentence]\n",
    "#                 tokenized_corpus.append(lemas)\n",
    "                proc_text.append((docid,lemas))\n",
    "#                 tokenized_corpus_i.append(docid)\n",
    "        return proc_text\n",
    "\n",
    "\n",
    "    def tokenize(self, body,batch=32): \n",
    "        print(\"Extracting SVO Triples\")\n",
    "        tokenized_corpus = []\n",
    "        tokenized_corpus_i = []\n",
    "        \n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'tokenize_single')\n",
    "            #vectors = pool.map(func_call_it, tqdm(enumerate(body), total=len(body)))\n",
    "            vectors = list(tqdm(pool.imap_unordered(func_call_it, enumerate(body),chunksize=batch), total=len(body)))\n",
    "        \n",
    "        \n",
    "        for e in vectors:\n",
    "            for docid, lema in e:\n",
    "                tokenized_corpus.append(lema)\n",
    "                tokenized_corpus_i.append(docid)\n",
    "                    \n",
    "        return tokenized_corpus, tokenized_corpus_i\n",
    "    \n",
    "    def transform_single(self, lema):\n",
    "        doc_scores = self.bm25.get_scores(lema)\n",
    "        topN = np.argsort(doc_scores)[::-1][:5]\n",
    "        sc = doc_scores[topN]\n",
    "        lab = np.array([self.training_labels[i] for i in topN])\n",
    "        v = [\n",
    "                np.sum(lab),\n",
    "                np.mean(sc),np.max(sc),np.min(sc)\n",
    "        ]\n",
    "        sc2 = [v if lab[i]==1 else 0 for i,v in enumerate(sc)]\n",
    "        v.extend([np.mean(sc2),np.max(sc2),np.sum(sc2)])\n",
    "        sc3 = [v if lab[i]==0 else 0 for i,v in enumerate(sc)]\n",
    "        v.extend([np.mean(sc3),np.max(sc3),np.sum(sc3)])\n",
    "        return v\n",
    "    \n",
    "    def transform(self, body, batch=32):\n",
    "        vectors = []\n",
    "        \n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            func_call_it = functools.partial(call_it2, self, 'transform_single')\n",
    "            #vectors = pool.map(func_call_it, tqdm(body,total=len(body)))\n",
    "            vectors = list(tqdm(pool.imap_unordered(func_call_it, body, chunksize=batch), total=len(body)))\n",
    "        \n",
    "        return vectors\n",
    "        \n",
    "    def train(self, body, labels):    \n",
    "        tokenized_corpus, tokenized_corpus_i = self.tokenize(body)\n",
    "        with open(\"tokenized_corpus.pickle\",\"wb\") as fp:\n",
    "             pickle.dump(zip(tokenized_corpus, tokenized_corpus_i), fp)\n",
    "        \n",
    "        \n",
    "        with open(\"tokenized_corpus.pickle\",\"rb\") as fp:\n",
    "            X=pickle.load(fp)\n",
    "        \n",
    "       \n",
    "        \n",
    "        tokenized_corpus, tokenized_corpus_i = zip(*X)\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        self.training_labels = labels[list(tokenized_corpus_i)]\n",
    "\n",
    "        self.trainX = self.transform(tokenized_corpus)\n",
    "        with open(\"fts2_trainX.pickle\",\"wb\") as fp:\n",
    "            pickle.dump(self.trainX , fp) \n",
    "        \n",
    "        with open(\"fts2_trainX.pickle\",\"rb\") as fp:\n",
    "            self.trainX =pickle.load(fp)\n",
    "            \n",
    "        y = labels[list(tokenized_corpus_i)]\n",
    "        #print( self.trainX)\n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        self.cls.fit(self.trainX, y)\n",
    "            \n",
    "    def predict(self, body, labels):\n",
    "        tokenized_corpus, tokenized_corpus_i = self.tokenize(body)\n",
    "        \n",
    "        self.testX = self.transform(tokenized_corpus)\n",
    "        preds = self.cls.predict(self.testX)\n",
    "        \n",
    "        \n",
    "        mp={}\n",
    "        for i,e in enumerate(tokenized_corpus_i):\n",
    "            if e not in mp:\n",
    "                mp[e]=[]\n",
    "            mp[e].append(preds[i])\n",
    "        \n",
    "        ypred=[]\n",
    "        for id,row in enumerate(body):\n",
    "            if id not in mp:\n",
    "                ypred.append(0)\n",
    "            else:\n",
    "                ypred.append(1 if np.mean(mp[e])>0 else 0)\n",
    "        \n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5a32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1defe08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SVO Triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/9180 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_29624/1307854309.py\", line 16, in call_it2\n    return getattr(instance, name)(arg)\n  File \"/tmp/ipykernel_29624/1307854309.py\", line 34, in tokenize_single\n    sentence = sp(line)\nNameError: name 'sp' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fts2 \u001b[38;5;241m=\u001b[39m FTSSemantic2()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfts2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 86\u001b[0m, in \u001b[0;36mFTSSemantic2.train\u001b[0;34m(self, body, labels)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, body, labels):    \n\u001b[0;32m---> 86\u001b[0m     tokenized_corpus, tokenized_corpus_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_corpus.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m     88\u001b[0m          pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mzip\u001b[39m(tokenized_corpus, tokenized_corpus_i), fp)\n",
      "Cell \u001b[0;32mIn [9], line 50\u001b[0m, in \u001b[0;36mFTSSemantic2.tokenize\u001b[0;34m(self, body, batch)\u001b[0m\n\u001b[1;32m     48\u001b[0m     func_call_it \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(call_it2, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize_single\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#vectors = pool.map(func_call_it, tqdm(enumerate(body), total=len(body)))\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_call_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m vectors:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m docid, lema \u001b[38;5;129;01min\u001b[39;00m e:\n",
      "File \u001b[0;32m/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:448\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    440\u001b[0m result \u001b[38;5;241m=\u001b[39m IMapUnorderedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taskqueue\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m    442\u001b[0m     (\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guarded_task_generation(result\u001b[38;5;241m.\u001b[39m_job,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         result\u001b[38;5;241m.\u001b[39m_set_length\n\u001b[1;32m    447\u001b[0m     ))\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (item \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m chunk)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "fts2 = FTSSemantic2()\n",
    "fts2.train(body[train],labels[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "9a18e6e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SVO Triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████▊                                                                                                | 120/1021 [00:23<02:54,  5.16it/s]Process ForkPoolWorker-369:\n",
      " 12%|████████████▊                                                                                                | 120/1021 [00:23<02:56,  5.12it/s]Process ForkPoolWorker-372:\n",
      "Process ForkPoolWorker-365:\n",
      "Process ForkPoolWorker-370:\n",
      "Process ForkPoolWorker-368:\n",
      "Process ForkPoolWorker-366:\n",
      "Process ForkPoolWorker-371:\n",
      "Process ForkPoolWorker-367:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 366, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rkozik/anaconda3/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [726], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ypred \u001b[38;5;241m=\u001b[39m \u001b[43mfts2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [720], line 109\u001b[0m, in \u001b[0;36mFTSSemantic2.predict\u001b[0;34m(self, body, labels)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, body, labels):\n\u001b[0;32m--> 109\u001b[0m     tokenized_corpus, tokenized_corpus_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(tokenized_corpus)\n\u001b[1;32m    112\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestX)\n",
      "Cell \u001b[0;32mIn [720], line 45\u001b[0m, in \u001b[0;36mFTSSemantic2.tokenize\u001b[0;34m(self, body)\u001b[0m\n\u001b[1;32m     43\u001b[0m     func_call_it \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(call_it2, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize_single\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#vectors = pool.map(func_call_it, tqdm(enumerate(body), total=len(body)))\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_call_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m vectors:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m docid, lema \u001b[38;5;129;01min\u001b[39;00m e:\n",
      "File \u001b[0;32m/media/rkozik/02FF-A831/repos/swarog_exp/env.hator/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ypred = fts2.predict(body[test],labels[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "met = Metrics()\n",
    "met.update(labels[test], ypred)\n",
    "met.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b6024",
   "metadata": {},
   "source": [
    "# FTS Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "b5698c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import traceback\n",
    "\n",
    "class FTSSemantic:\n",
    "    def __init__(self):\n",
    "        self.con=sqlite3.connect(\"svo.db\")\n",
    "        \n",
    "    def build_index(self, body, labels, tblname=\"train\"): \n",
    "        print(\"Building index\")\n",
    "        self.con.execute(f\"drop table if exists {tblname}_fts\")\n",
    "        self.con.execute(f\"create VIRTUAL table if not exists {tblname}_fts USING fts5(txt, original, docid, label)\")\n",
    "        for docid, txt in tqdm(enumerate(body), total=len(labels)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            vecs = []\n",
    "            newtext=txt\n",
    "            for xsvo in svos:\n",
    "                svo = xsvo\n",
    "                if len(svo) < 3 and len(svo) > 1:\n",
    "                    svos = [svo[0],svo[1],\"\"]\n",
    "                if len(svo) == 3:\n",
    "                    line = \" \".join(svo)\n",
    "                    sentence = sp(line)\n",
    "                    w = \" \".join([word.lemma_ for word in sentence])\n",
    "                    vecs.append(w)\n",
    "            if len(vecs) > 0:\n",
    "                newtext=\" \".join(vecs)\n",
    "                \n",
    "            self.con.execute(f\"insert into {tblname}_fts values(?,?,?,?)\", \n",
    "                        [newtext,txt, docid, str(labels[docid])])\n",
    "            \n",
    "        self.con.commit() \n",
    "        \n",
    "    def transformData(self, body, skip_self=False):\n",
    "        X = []\n",
    "        for docid, txt in tqdm(enumerate(body), total=len(body)):\n",
    "            tokens = nlp(txt)\n",
    "            svos = findSVOs(tokens)\n",
    "            svos_vec=self.svos_features(svos, docid if skip_self else -1 )\n",
    "            X.append(svos_vec)\n",
    "        return X\n",
    "            \n",
    "    def createTrainData(self, body, labels):\n",
    "        print(\"Creating trainX\")\n",
    "        X = self.transformData(body)\n",
    "        with open(\"fts_train_X.pickle\",\"wb\") as fp:\n",
    "            pickle.dump(X, fp)\n",
    "        return X\n",
    "        \n",
    "\n",
    "        \n",
    "    def svo2query(self,svo):\n",
    "        line = \" \".join(svo)\n",
    "        sentence = sp(line)\n",
    "        zlepek = [re.sub(r'[^a-zA-Z0-9]', ' ', word.lemma_) for word in sentence]\n",
    "        zlepek = [\" \".join(z.split()) for z in zlepek]\n",
    "        zlepek = [z for z in zlepek if len(z) > 1]\n",
    "        w = \" OR \".join(zlepek)\n",
    "        return w\n",
    "    \n",
    "    def svos_features(self,svos, skip_docid):\n",
    "        result = []\n",
    "        for xsvo in svos:\n",
    "            svo = xsvo\n",
    "            if len(svo) < 3 and len(svo) > 1:\n",
    "                svos = [svo[0],svo[1],\"\"]\n",
    "\n",
    "            if len(svo) == 3:\n",
    "                w = self.svo2query(svo)\n",
    "                try:\n",
    "                    q=f\"\"\"\n",
    "                        with top10 as (\n",
    "                            select bm25(train_fts) as score, cast(label as int) as pred from train_fts  \n",
    "                             where txt  match \"{w}\" order by bm25(train_fts) limit 5\n",
    "                        )\n",
    "                        select min(score), avg(score), max(score), avg(pred), count(*), sum(pred) from top10\n",
    "                    \"\"\"\n",
    "                    fvector = self.con.execute(q).fetchall()[0]\n",
    "                    if fvector[-2] == 0:\n",
    "                        result.append([0]*6)\n",
    "                    else:    \n",
    "                        result.append(fvector)\n",
    "                    #print(skip_docid,w)\n",
    "                    #print(fvector)\n",
    "                except (sqlite3.OperationalError):\n",
    "                    #print(\"error,\")\n",
    "                    traceback.print_exc()\n",
    "                    result.append([0]*6)\n",
    "        if len(result) > 0:\n",
    "            return np.mean(result, axis=0)\n",
    "        else:\n",
    "            return [0]*6\n",
    "        \n",
    "        \n",
    "    def train(self, body, labels, tblname=\"train\"):    \n",
    "        self.build_index(body,labels, tblname)\n",
    "\n",
    "        self.createTrainData(body,labels)\n",
    "        \n",
    "        with open(\"fts_train_X.pickle\",\"rb\") as fp:\n",
    "            X=pickle.load(fp)\n",
    "            \n",
    "        self.trainX = X\n",
    "        \n",
    "        self.cls = LogisticRegression(max_iter=10000)\n",
    "        #self.cls = RandomForestClassifier(max_depth=12)\n",
    "        self.cls.fit(X, labels)\n",
    "\n",
    "    def predict(self, body):\n",
    "        pred = []\n",
    "        X = self.transformData(body)\n",
    "        self.testX=X\n",
    "        return self.cls.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "c6bb8f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [01:24<00:00, 108.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [01:50<00:00, 83.35it/s]\n"
     ]
    }
   ],
   "source": [
    "sem = FTSSemantic()\n",
    "sem.train(body[train],labels[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "284ee47b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:11<00:00, 89.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ypred=sem.predict(body[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "2629f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              95.5 ±  0.0  -            -            \n",
      "  Balanced Accuracy     61.0 ±  0.0  -            -            \n",
      "  F1                    94.6 ±  0.0  32.4 ±  0.0  97.7 ±  0.0  \n",
      "  G-mean                50.3 ±  0.0  47.6 ±  0.0  47.6 ±  0.0  \n",
      "  Precision             94.4 ±  0.0  55.0 ±  0.0  96.3 ±  0.0  \n",
      "  Recall                95.5 ±  0.0  22.9 ±  0.0  99.1 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "met = Metrics()\n",
    "met.update(labels[test], ypred)\n",
    "met.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b992a48-181f-4b67-a320-939543e855ff",
   "metadata": {},
   "source": [
    "\n",
    "# Infrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "5526269d-b19e-46e7-803f-8eada642f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Inference:\n",
    "    def train(self,graph, content, semantics, labels):\n",
    "        newX=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i]) \n",
    "            if semantics:\n",
    "                v2 = np.append(v2, semantics[i])\n",
    "            newX.append(v2)\n",
    "            \n",
    "        self.inf = RandomForestClassifier(max_depth=12)\n",
    "        self.inf.fit(newX, labels)\n",
    "        \n",
    "    def predict(self, graph, content, semantics):\n",
    "        newTest=[]\n",
    "        for i,vec in enumerate(content):\n",
    "            v2 = np.append(content[i], graph[i]) \n",
    "            if semantics:\n",
    "                v2 = np.append(v2, semantics[i])\n",
    "            newTest.append(v2)\n",
    "    \n",
    "        return self.inf.predict(newTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baccae2-b3bd-4e07-a8cd-e55fe8801382",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827802d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 10201\n",
      "labels fake= 9727 real= 474\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "body, labels, total_number_of_claims = loader.get(\"covidfn\")\n",
    "X=range(0,total_number_of_claims)\n",
    "train, test = list(rskf.split(X, labels))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "315f84fb-918d-4814-9e04-3601b4f3fe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading covidfn\n",
      "total_number_of_claims= 10201\n",
      "labels fake= 9727 real= 474\n",
      "fold-0\n",
      "Building index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [01:24<00:00, 108.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [02:08<00:00, 71.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:13<00:00, 78.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 9180/9180 [00:08<00:00, 1052.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. nodes sim.= 0.6016417403761986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:00<00:00, 1050.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2295/2295 [00:12<00:00, 185.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:01<00:00, 183.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              95.5 ±  0.0  -            -            \n",
      "  Balanced Accuracy     61.0 ±  0.0  -            -            \n",
      "  F1                    94.6 ±  0.0  32.4 ±  0.0  97.7 ±  0.0  \n",
      "  G-mean                50.3 ±  0.0  47.6 ±  0.0  47.6 ±  0.0  \n",
      "  Precision             94.4 ±  0.0  55.0 ±  0.0  96.3 ±  0.0  \n",
      "  Recall                95.5 ±  0.0  22.9 ±  0.0  99.1 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symbolic:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              97.4 ±  0.0  -            -            \n",
      "  Balanced Accuracy     75.8 ±  0.0  -            -            \n",
      "  F1                    97.0 ±  0.0  64.9 ±  0.0  98.6 ±  0.0  \n",
      "  G-mean                72.7 ±  0.0  72.0 ±  0.0  72.0 ±  0.0  \n",
      "  Precision             97.1 ±  0.0  86.2 ±  0.0  97.7 ±  0.0  \n",
      "  Recall                97.4 ±  0.0  52.1 ±  0.0  99.6 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.2 ±  0.0  -            -            \n",
      "  Balanced Accuracy     85.2 ±  0.0  -            -            \n",
      "  F1                    98.1 ±  0.0  79.1 ±  0.0  99.1 ±  0.0  \n",
      "  G-mean                84.2 ±  0.0  84.0 ±  0.0  84.0 ±  0.0  \n",
      "  Precision             98.1 ±  0.0  89.5 ±  0.0  98.6 ±  0.0  \n",
      "  Recall                98.2 ±  0.0  70.8 ±  0.0  99.6 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "Symb+Deep:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.4 ±  0.0  -            -            \n",
      "  Balanced Accuracy     91.3 ±  0.0  -            -            \n",
      "  F1                    98.4 ±  0.0  83.3 ±  0.0  99.2 ±  0.0  \n",
      "  G-mean                91.0 ±  0.0  90.9 ±  0.0  90.9 ±  0.0  \n",
      "  Precision             98.4 ±  0.0  83.3 ±  0.0  99.2 ±  0.0  \n",
      "  Recall                98.4 ±  0.0  83.3 ±  0.0  99.2 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "ALL:\n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Score                 Average      Lab:1        Lab:2       \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
      "  Accuracy              98.1 ±  0.0  -            -            \n",
      "  Balanced Accuracy     86.2 ±  0.0  -            -            \n",
      "  F1                    98.1 ±  0.0  78.7 ±  0.0  99.0 ±  0.0  \n",
      "  G-mean                85.3 ±  0.0  85.1 ±  0.0  85.1 ±  0.0  \n",
      "  Precision             98.0 ±  0.0  85.4 ±  0.0  98.7 ±  0.0  \n",
      "  Recall                98.1 ±  0.0  72.9 ±  0.0  99.4 ±  0.0  \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "for dataset in [\"fnkdd\"]:\n",
    "    body, labels, total_number_of_claims = loader.get(dataset)\n",
    "    X=range(0,total_number_of_claims)\n",
    "    \n",
    "    trustex_quality = Metrics()\n",
    "    swarog_quality = Metrics()\n",
    "    semantics_quality = Metrics()\n",
    "    inf3_quality = Metrics()\n",
    "    inf2_quality = Metrics()\n",
    "    \n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(rskf.split(X, labels)):\n",
    "        print(f\"fold-{fold_idx}\")    \n",
    "        \n",
    "        swarog = SwarogModel()\n",
    "        trustex = TrustexModel()\n",
    "        inference2 = Inference()\n",
    "        inference3 = Inference()\n",
    "        semantics = FTSSemantic()\n",
    "        \n",
    "        semantics.train(body[train],labels[train])\n",
    "        ypred = semantics.predict(body[test])\n",
    "        semantics_quality.update(labels[test], ypred)\n",
    "    \n",
    "        trustex.train(body[train],labels[train])\n",
    "        ypred = trustex.predict(body[test])\n",
    "        trustex_quality.update(labels[test], ypred)\n",
    "        \n",
    "        swarog.train(body[train],labels[train])\n",
    "        ypred = swarog.predict(body[test])\n",
    "        swarog_quality.update(labels[test], ypred)\n",
    "\n",
    "        inference2.train(trustex.graph_knn, swarog.train_prob, None, labels[train])\n",
    "        newpred = inference2.predict(trustex.graph_test_knn, swarog.test_prob, None)\n",
    "        inf2_quality.update(labels[test], newpred)\n",
    "\n",
    "        \n",
    "        inference3.train(trustex.graph_knn, swarog.train_prob, semantics.trainX, labels[train])\n",
    "        newpred = inference3.predict(trustex.graph_test_knn, swarog.test_prob, semantics.testX)\n",
    "        inf3_quality.update(labels[test], newpred)\n",
    "\n",
    "        break\n",
    "\n",
    "print(\"Semantic:\")\n",
    "semantics_quality.print_table()\n",
    "print(\"Symbolic:\")\n",
    "trustex_quality.print_table()\n",
    "print(\"Deep:\")\n",
    "swarog_quality.print_table()\n",
    "\n",
    "print(\"Symb+Deep:\")\n",
    "inf2_quality.print_table()\n",
    "\n",
    "print(\"ALL:\")\n",
    "inf3_quality.print_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a15b3-d116-4d04-af2f-d87dcf77ed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env.hator",
   "language": "python",
   "name": "env.hator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "375.498px",
    "width": "241.499px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "788px",
    "left": "10px",
    "top": "150px",
    "width": "158.95px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84036e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-25 12:33:21--  https://docs.google.com/uc?export=download&confirm=&id=18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.203.206, 2a00:1450:401b:810::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.203.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0k-34-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hu3hf9fb264l5mne6gmhhtkla9f5dc4e/1661423550000/05536367513713019519/*/18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV?e=download&uuid=a2cba134-b25d-4da1-a1bb-a764bceac670 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-08-25 12:33:24--  https://doc-0k-34-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hu3hf9fb264l5mne6gmhhtkla9f5dc4e/1661423550000/05536367513713019519/*/18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV?e=download&uuid=a2cba134-b25d-4da1-a1bb-a764bceac670\n",
      "Resolving doc-0k-34-docs.googleusercontent.com (doc-0k-34-docs.googleusercontent.com)... 142.250.75.1, 2a00:1450:401b:80d::2001\n",
      "Connecting to doc-0k-34-docs.googleusercontent.com (doc-0k-34-docs.googleusercontent.com)|142.250.75.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51522510 (49M) [text/csv]\n",
      "Saving to: ‘pubhealth.csv’\n",
      "\n",
      "pubhealth.csv       100%[===================>]  49,13M  1,79MB/s    in 14s     \n",
      "\n",
      "2022-08-25 12:33:39 (3,40 MB/s) - ‘pubhealth.csv’ saved [51522510/51522510]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!id=\"18E24-G2Xq-y5HfM6ZatUB_8WKL_r6HOV\"; conf=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$id -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p'); wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$conf&id=$id\" -O pubhealth.csv && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c26a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      " 0    494808\n",
      "1    262288\n",
      "Name: label, dtype: int64\n",
      "shape  \n",
      " (757096, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 37.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (757096,) y (757096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "data = pd.read_csv(\"../../raw/nela-gt-2020.csv\",sep=\",\")\n",
    "print(\"labels\\n\",data[\"label\"].value_counts())\n",
    "print(\"shape  \\n\",data.shape)\n",
    "\n",
    "bootstrap_size = 0\n",
    "\n",
    "if bootstrap_size != 0:\n",
    "    bootstrap_factor = bootstrap_size / data.shape[0]\n",
    "    bootstrap = np.random.uniform(size=data.shape[0]) < bootstrap_factor\n",
    "    data = data.iloc[bootstrap]\n",
    "\n",
    "data[\"text\"] = [ str(v) for v in data[\"text\"]]\n",
    "X, y = data[\"text\"].values, data[\"label\"].values\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b30aa57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'speaker', 'text', 'explanation', 'url', 'label'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe3e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# SAVE\n",
    "with open('nela-gt-X.pickle', 'wb') as handle:\n",
    "    pickle.dump(X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('nela-gt-y.pickle', 'wb') as handle:\n",
    "    pickle.dump(y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014cbce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m rskf \u001b[38;5;241m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1410\u001b[39m)\n\u001b[1;32m     16\u001b[0m foldids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, (train, test) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(rskf\u001b[38;5;241m.\u001b[39msplit(\u001b[43mX\u001b[49m, y)), total\u001b[38;5;241m=\u001b[39mrskf\u001b[38;5;241m.\u001b[39mget_n_splits()):\n\u001b[1;32m     18\u001b[0m     foldids\u001b[38;5;241m.\u001b[39mappend((fold_idx,train,test))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes X\u001b[39m\u001b[38;5;124m\"\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "# with open('nela-gt-X.pickle', 'rb') as handle:\n",
    "#     X=pickle.load(handle)\n",
    "    \n",
    "with open('../../pickles/products/nela-gt-y.pickle', 'rb') as handle:\n",
    "    y=pickle.load(handle)\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aaaa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "\n",
    "class BERTEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=16, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return csr_matrix(allembeds)\n",
    "\n",
    "\n",
    "class BertHead(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.head = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        self.head.fit(X, y)\n",
    "\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X=None):    \n",
    "        return self.head.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af643837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 47319/47319 [24:03<00:00, 32.78it/s]\n"
     ]
    }
   ],
   "source": [
    "bert = BERTEmbeddings()\n",
    "X_dstil_numpy = bert.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0d9c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickles/bertcls_embeddings/nela-gt_BERTEmbeddings.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# SAVE\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickles/bertcls_embeddings/nela-gt_BERTEmbeddings.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(X_dstil_numpy, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickles/bertcls_embeddings/nela-gt_BERTEmbeddings.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# SAVE\n",
    "with open('pickles/bertcls_embeddings/nela-gt_BERTEmbeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_dstil_numpy, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143f4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 38.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X (757096, 768) y (757096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "with open('../../pickles/bertcls_embeddings/nela-gt_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    X_dstil_numpy = pickle.load(handle)\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X_dstil_numpy, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X_dstil_numpy.shape,\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc74e9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BAC= 0.8007864947600388\n",
      " BAC= 0.8005056569270981\n",
      " BAC= 0.8005197171602432\n",
      " BAC= 0.8003859727183564\n",
      " BAC= 0.8013379078784156\n",
      " BAC= 0.7998190364851836\n",
      " BAC= 0.8008250541308373\n",
      " BAC= 0.8015369305476252\n",
      " BAC= 0.8006174432269242\n",
      " BAC= 0.7999808147250818\n",
      "----------\n",
      "AVG. BAC= 0.8006315028559804 +/- 0.0005052330705177547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "arr = []\n",
    "\n",
    "\n",
    "\n",
    "def doTrain(X,y):\n",
    "    #clf = BaggingClassifier(base_estimator=LogisticRegression(class_weight='balanced', max_iter=10000),\n",
    "    #                        n_estimators=10, random_state=0, max_samples=0.42).fit(X, y)\n",
    "    #ens = []\n",
    "    cls = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "    cls.fit(X, y)\n",
    "    return cls\n",
    "\n",
    "def doTest(cls, X):\n",
    "    return cls.predict(X)\n",
    "\n",
    "for fold,j in enumerate(foldids):\n",
    "    train = foldids[fold][1]\n",
    "    test = foldids[fold][2]\n",
    "    \n",
    "    pca = PCA(n_components=512)\n",
    "    X_train_pca = pca.fit_transform(X_dstil_numpy[train])\n",
    "    X_test_pca = pca.transform(X_dstil_numpy[test])\n",
    "    \n",
    "    cls = doTrain(X_train_pca, y[train])\n",
    "    \n",
    "    y_pred = doTest(cls, X_test_pca)\n",
    "    \n",
    "    bac = balanced_accuracy_score(y[test], y_pred)\n",
    "    \n",
    "    arr.append(bac)\n",
    "\n",
    "    print(\" BAC=\", bac)\n",
    "\n",
    "print(10*\"-\")\n",
    "print(\"AVG. BAC=\",np.mean(arr),\"+/-\",np.std(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50a84f0-9e39-4c35-8434-bab20e6cff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "def experiment(foldids, X, y, cls = LogisticRegression(max_iter=10000), fit=True):\n",
    "\n",
    "    scores = {\n",
    "        'Accuracy': {'func': accuracy_score},\n",
    "        'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "        'F1': {'func': f1_score},\n",
    "        'Precision': {'func': precision_score},\n",
    "        'Recall': {'func': recall_score},\n",
    "        'G-mean': {'func': geometric_mean_score}\n",
    "    }\n",
    "\n",
    "    for score_name, score_dict in scores.items():\n",
    "        scores[score_name][\"list\"] = []\n",
    "        scores[score_name][\"lab\"] = []\n",
    "\n",
    "    for fold,j in enumerate(foldids):\n",
    "        train = foldids[fold][1]\n",
    "        test = foldids[fold][2]\n",
    "        xin, yin = X[train], np.array(y[train])\n",
    "        \n",
    "        pca = PCA(n_components=512)\n",
    "        pca.fit(xin)\n",
    "        \n",
    "        \n",
    "        if fit == True:\n",
    "            cls.fit(pca.transform(xin), yin)\n",
    "        y_pred = cls.predict(pca.transform(X[test]))\n",
    "        for score_name, score_dict in scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                scorvaln = score_dict['func'](y[test], y_pred, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](y[test], y_pred, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](y[test], y_pred)\n",
    "                score_dict['list'].append(scorval)\n",
    "                #print(score_name, scorval)\n",
    "        #print(\" \")\n",
    "\n",
    "    #clear_output()\n",
    "    for score_name, score_dict in scores.items():\n",
    "        score_dict['avg'] = np.mean(score_dict['list'])\n",
    "        score_dict['std'] = np.std(score_dict['list'])\n",
    " \n",
    "    # Print stats\n",
    "    numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "    scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "    head = \"| %-20s | %-10s |\" +  numlabels * \" %-10s |\" \n",
    "    headv = [\"Score\", \"Average\"]\n",
    "    headv.extend([\"Kat_\"+str(i+1) for i in range(numlabels)])\n",
    "    row=head % tuple(headv)\n",
    "    print(\"+\"*len(row))\n",
    "    print(row)\n",
    "    print(\"+\"*len(row))\n",
    "    for score_name, score_dict in sorted(scores.items()) :\n",
    "        headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "        for i in range(numlabels):\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels* \" %4.1f ± %3.1f |\"\n",
    "                vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                headv.append(np.mean(vals)*100)\n",
    "                headv.append(np.std(vals)*100)\n",
    "            else:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels * \" %-10s |\" \n",
    "                headv.append(\"-\")\n",
    "        print(head % tuple(headv))\n",
    "    print(\"+\"*len(row))\n",
    "    return cls, scores, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "052716a2-6b65-4a0e-a854-0fd749fb934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 81.8 ± 0.0 | -          | -          |\n",
      "| Balanced Accuracy    | 78.6 ± 0.0 | -          | -          |\n",
      "| F1                   | 81.5 ± 0.0 | 86.4 ± 0.0 | 72.2 ± 0.1 |\n",
      "| G-mean               | 77.9 ± 0.0 | -          | -          |\n",
      "| Precision            | 81.5 ± 0.0 | 84.1 ± 0.0 | 76.6 ± 0.1 |\n",
      "| Recall               | 81.8 ± 0.0 | 88.9 ± 0.1 | 68.3 ± 0.1 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "foldids = []\n",
    "for fold_idx, (train, test) in enumerate(rskf.split(X_dstil_numpy, y)):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "\n",
    "#print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "model1, scores1, pca = experiment(foldids, X_dstil_numpy, y, \n",
    "                             LogisticRegression(max_iter=10000))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env.hator",
   "language": "python",
   "name": "env.hator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

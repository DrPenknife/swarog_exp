{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cf8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if \"disilbert_model\" not in locals():\n",
    "    disilbert_tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    disilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    handle = disilbert_model.to(device)\n",
    "\n",
    "\n",
    "class BERTEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer =  disilbert_tokenizer\n",
    "        self.model = disilbert_tokenizer\n",
    "        self.max_length = 256\n",
    "        self.model_name = disilbert_model\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        return self.tokenizer(txt, max_length=self.max_length, \n",
    "                              truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        dataloader = DataLoader(X, batch_size=4, shuffle=False)\n",
    "        allembeds = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batchenc = disilbert_tokenizer(batch, max_length=256, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = batchenc['input_ids'].to(device)\n",
    "            attention_mask = batchenc['attention_mask'].to(device)\n",
    "            batchout = disilbert_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            embeds = [vec[0].cpu().detach().numpy() for vec in batchout[1][-1]]\n",
    "            allembeds.extend(embeds)\n",
    "        return csr_matrix(allembeds)\n",
    "\n",
    "\n",
    "class BigPappy(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        # load\n",
    "        self.bert = BERTEmbeddings()\n",
    "        \n",
    "        with open('domain_cls.pickle', 'rb') as handle:\n",
    "            self.pipe_domain = pickle.load(handle)\n",
    "\n",
    "        self.domain_model_pipe = []\n",
    "        for i in range(6):\n",
    "            with open(f'model_{i}.pickle', 'rb') as handle:\n",
    "                p=pickle.load(handle)\n",
    "                self.domain_model_pipe.append(p)\n",
    "\n",
    "    def predictw(self,  X_test):\n",
    "        domain_pred = self.pipe_domain.predict(X_test)\n",
    "        ypred = []\n",
    "        for i,dpred in enumerate(domain_pred):\n",
    "            model = self.domain_model_pipe[dpred] \n",
    "            ypred.append(model.predict(X_test[i:i+1])[0])\n",
    "        return ypred, domain_pred\n",
    "        \n",
    "    def predict(self,  X_raw):\n",
    "        X_test = self.bert.transform(X_raw).toarray()\n",
    "        return self.predictw(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a671cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 65.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0], array([5]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp = BigPappy()\n",
    "bp.predict([\"A hard story to tell. by EeffingT I loved this film. It is heartbreaking without a doubt, but anyone who has ever loved someone whose life fizzled out slowly like Alice’s will understand. It’s such a hard thing to see someone you love essentially die before they actually leave this earth, because the person you loved no longer exists. It’s so honest and real, and portrays how real families are changed when tragedy strikes. I loved how they ended with the line from Angels in America. So fitting especially given that the co-creator of this movie passed away before his time due to ALS. Julianna Moore really did deserve the Oscar for this film. Kristen Stewart should have won an award as well. ugh by serrata so, souls are going to form a net with hands grasping ankles and heal the ozone layer? and that’s “love”? ugh. tragic story by littleRedFiat Most of the characters in this movie, other than Julianne Moore and Kristen Stewart, were like placeholders. Not the fault of the actors but of the overall story, or lack thereof, I felt. The scenes between Julianne Moore and Kristen Stewart were wrenching, because it felt like there was some kind of real relationship there—the rest was just sketched in. The sketchiness and lack of depth that the rest of the story and characters had detracted from the power of the story of the main character, which is too bad, because I think the movie serves to bring the topic of Alzheimer’s disease to a larger audience. But I think as a movie, overall it didn’t quite succeed. Viewers Also Bought\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA = {\n",
    "    \"X\" : [],\n",
    "    \"category\" : [],\n",
    "    \"y\" : []\n",
    "}\n",
    "\n",
    "X=DATA[\"X\"]\n",
    "category=DATA[\"category\"]\n",
    "y=DATA[\"y\"]\n",
    "\n",
    "\n",
    "with open('covid_fake_news_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(0,len(dst)))\n",
    "    data = pd.read_csv(\"raw/covid_fake_news.csv\",sep=\"\\t\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "with open('mmcovid_en_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(1,len(dst)))\n",
    "    data = pd.read_csv(\"raw/mmcovid_en.csv\",sep=\",\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "    \n",
    "# with open('isot_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "#     dst = pickle.load(handle)\n",
    "#     X.extend(dst)\n",
    "#     category.extend(np.repeat(3,len(dst)))\n",
    "    \n",
    "# with open('grafn_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "#     dst = pickle.load(handle)\n",
    "#     X.extend(dst)\n",
    "#     category.extend(np.repeat(4,len(dst)))\n",
    "    \n",
    "with open('pubhealth_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "    dst = pickle.load(handle)\n",
    "    X.extend(dst)\n",
    "    category.extend(np.repeat(2,len(dst)))\n",
    "    data = pd.read_csv(\"raw/pubhealth.csv\",sep=\",\")\n",
    "    y.extend(data[\"label\"].values)\n",
    "\n",
    "    \n",
    "# with open('qprop_BERTEmbeddings.pickle', 'rb') as handle:\n",
    "#     dst = pickle.load(handle)\n",
    "#     print(dst.shape)\n",
    "#     X.extend(dst)\n",
    "#     category.extend(np.repeat(3,len(dst)))\n",
    "#     data = pd.read_csv(\"raw/qprop.csv\",sep=\"\\t\")\n",
    "#     y.extend(data[\"label\"].values)\n",
    "#     print(data.shape)\n",
    "    \n",
    "DATA[\"X\"]=np.array(X)\n",
    "DATA[\"category\"]=np.array(category)\n",
    "DATA[\"y\"]=np.array(y)\n",
    "DATA[\"folds\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def experiment(foldids, X, y, cls = LogisticRegression(max_iter=10000), fit=True):\n",
    "\n",
    "    scores = {\n",
    "        'Accuracy': {'func': accuracy_score},\n",
    "        'Balanced Accuracy': {'func': balanced_accuracy_score},\n",
    "        'F1': {'func': f1_score},\n",
    "        'Precision': {'func': precision_score},\n",
    "        'Recall': {'func': recall_score},\n",
    "        'G-mean': {'func': geometric_mean_score}\n",
    "    }\n",
    "\n",
    "    for score_name, score_dict in scores.items():\n",
    "        scores[score_name][\"list\"] = []\n",
    "        scores[score_name][\"lab\"] = []\n",
    "\n",
    "    for fold,j in enumerate(foldids):\n",
    "        train = foldids[fold][1]\n",
    "        test = foldids[fold][2]\n",
    "        xin, yin = X[train], np.array(y[train])\n",
    "        if fit == True:\n",
    "            cls.fit(xin, yin)\n",
    "        y_pred = cls.predict(X[test])\n",
    "        for score_name, score_dict in scores.items():\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                scorvaln = score_dict['func'](y[test], y_pred, average=None)\n",
    "                score_dict['lab'].append(scorvaln)\n",
    "                scorval = score_dict['func'](y[test], y_pred, average=\"weighted\")\n",
    "                score_dict['list'].append(scorval)\n",
    "                print(score_name, scorval, scorvaln)  \n",
    "            else:\n",
    "                scorval=score_dict['func'](y[test], y_pred)\n",
    "                score_dict['list'].append(scorval)\n",
    "                print(score_name, scorval)\n",
    "        print(\" \")\n",
    "\n",
    "    clear_output()\n",
    "    for score_name, score_dict in scores.items():\n",
    "        score_dict['avg'] = np.mean(score_dict['list'])\n",
    "        score_dict['std'] = np.std(score_dict['list'])\n",
    " \n",
    "    # Print stats\n",
    "    numlabels = scores[\"F1\"][\"lab\"][0].shape[0]\n",
    "    scores[\"F1\"][\"lab\"][0].shape[0] \n",
    "    head = \"| %-20s | %-10s |\" +  numlabels * \" %-10s |\" \n",
    "    headv = [\"Score\", \"Average\"]\n",
    "    headv.extend([\"Kat_\"+str(i+1) for i in range(numlabels)])\n",
    "    row=head % tuple(headv)\n",
    "    print(\"+\"*len(row))\n",
    "    print(row)\n",
    "    print(\"+\"*len(row))\n",
    "    for score_name, score_dict in sorted(scores.items()) :\n",
    "        headv = [score_name, np.mean(score_dict['list'])*100, np.std(score_dict['list'])*100]\n",
    "        for i in range(numlabels):\n",
    "            if score_name in [\"F1\",\"Precision\",\"Recall\"]:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels* \" %4.1f ± %3.1f |\"\n",
    "                vals = [v[i] for v in scores[score_name][\"lab\"]]\n",
    "                headv.append(np.mean(vals)*100)\n",
    "                headv.append(np.std(vals)*100)\n",
    "            else:\n",
    "                head = \"| %-20s | %4.1f ± %3.1f |\" + numlabels * \" %-10s |\" \n",
    "                headv.append(\"-\")\n",
    "        print(head % tuple(headv))\n",
    "    print(\"+\"*len(row))\n",
    "    return cls, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px dashed red\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 96.8 ± 0.2 | -          | -          |\n",
      "| Balanced Accuracy    | 76.6 ± 1.2 | -          | -          |\n",
      "| F1                   | 96.6 ± 0.2 | 98.3 ± 0.1 | 63.6 ± 1.8 |\n",
      "| G-mean               | 73.2 ± 1.7 | -          | -          |\n",
      "| Precision            | 96.5 ± 0.2 | 97.6 ± 0.1 | 77.3 ± 3.2 |\n",
      "| Recall               | 96.8 ± 0.2 | 99.1 ± 0.2 | 54.1 ± 2.5 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = DATA[\"X\"][DATA[\"category\"] == 0]\n",
    "y = DATA[\"y\"][DATA[\"category\"] == 0]\n",
    "\n",
    "pca = PCA(n_components=85)\n",
    "pca.fit_transform(X)\n",
    "\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "\n",
    "DATA[\"folds\"].append(foldids)\n",
    "model1, scores1 = experiment(foldids, pca.transform(X), y, LogisticRegression(max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 91.7 ± 0.3 | -          | -          |\n",
      "| Balanced Accuracy    | 86.9 ± 0.7 | -          | -          |\n",
      "| F1                   | 91.4 ± 0.3 | 94.4 ± 0.2 | 83.5 ± 0.7 |\n",
      "| G-mean               | 86.3 ± 0.8 | -          | -          |\n",
      "| Precision            | 91.7 ± 0.2 | 91.5 ± 0.5 | 92.3 ± 1.1 |\n",
      "| Recall               | 91.7 ± 0.3 | 97.5 ± 0.4 | 76.3 ± 1.8 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "X = DATA[\"X\"][DATA[\"category\"] == 1]\n",
    "y = DATA[\"y\"][DATA[\"category\"] == 1]\n",
    "\n",
    "pca = PCA(n_components=85)\n",
    "pca.fit_transform(X)\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "\n",
    "DATA[\"folds\"].append(foldids)\n",
    "model2, scores2 = experiment(foldids, pca.transform(X), y, LogisticRegression(max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 79.5 ± 0.4 | -          | -          |\n",
      "| Balanced Accuracy    | 77.9 ± 0.3 | -          | -          |\n",
      "| F1                   | 79.5 ± 0.4 | 72.4 ± 0.4 | 83.7 ± 0.3 |\n",
      "| G-mean               | 77.7 ± 0.3 | -          | -          |\n",
      "| Precision            | 79.4 ± 0.3 | 72.9 ± 0.7 | 83.3 ± 0.3 |\n",
      "| Recall               | 79.5 ± 0.4 | 71.9 ± 0.6 | 84.0 ± 0.6 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "X = DATA[\"X\"][DATA[\"category\"] == 2]\n",
    "y = DATA[\"y\"][DATA[\"category\"] == 2]\n",
    "\n",
    "pca = PCA(n_components=85)\n",
    "pca.fit_transform(X)\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "\n",
    "DATA[\"folds\"].append(foldids)\n",
    "model2, scores2 = experiment(foldids, pca.transform(X), y, LogisticRegression(max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Score                | Average    | Kat_1      | Kat_2      |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "| Accuracy             | 83.4 ± 0.2 | -          | -          |\n",
      "| Balanced Accuracy    | 80.2 ± 0.3 | -          | -          |\n",
      "| F1                   | 83.2 ± 0.2 | 87.8 ± 0.1 | 74.0 ± 0.3 |\n",
      "| G-mean               | 79.6 ± 0.3 | -          | -          |\n",
      "| Precision            | 83.2 ± 0.2 | 85.9 ± 0.2 | 77.7 ± 0.3 |\n",
      "| Recall               | 83.4 ± 0.2 | 89.8 ± 0.2 | 70.6 ± 0.6 |\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "X = DATA[\"X\"]\n",
    "y = DATA[\"y\"]\n",
    "\n",
    "pca = PCA(n_components=85)\n",
    "pca.fit_transform(X)\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1410)\n",
    "foldids = []\n",
    "for fold_idx, (train, test) in tqdm(enumerate(rskf.split(X, y)), total=rskf.get_n_splits()):\n",
    "    foldids.append((fold_idx,train,test))\n",
    "    \n",
    "print(\"shapes X\",X.shape,\"y\", y.shape)\n",
    "\n",
    "model3, scores3 = experiment(foldids, pca.transform(X), y, LogisticRegression(max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Alpha version of a version of ELLA that plays nicely with sklearn\n",
    "\t@author: Paul Ruvolo\n",
    "\"\"\"\n",
    "\n",
    "from math import log\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.linalg import sqrtm, inv, norm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score\n",
    "\n",
    "\n",
    "class ELLA(object):\n",
    "    \"\"\" The ELLA model \"\"\"\n",
    "    def __init__(self, d, k, base_learner, base_learner_kwargs = {}, mu = 1, lam = 1, k_init = False):\n",
    "        \"\"\" Initializes a new model for the given base_learner.\n",
    "            d: the number of parameters for the base learner\n",
    "            k: the number of latent model components\n",
    "            base_learner: the base learner to use (currently can only be\n",
    "                LinearRegression, Ridge, or LogisticRegression).\n",
    "            base_learner_kwargs: keyword arguments to base learner (for instance to\n",
    "                                 specify regularization strength)\n",
    "            mu: hyperparameter for sparsity\n",
    "            lam: L2 penalty on L\n",
    "            mu: the L_1 penalty to use\n",
    "            lam: the L_2 penalty to use\n",
    "            NOTE: currently only binary logistic regression is supported\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.k = k\n",
    "        self.L = np.random.randn(d,k)\n",
    "        self.A = np.zeros((d * k, d * k))\n",
    "        self.b = np.zeros((d * k, 1))\n",
    "        self.S = np.zeros((k, 0))\n",
    "        self.T = 0\n",
    "        self.mu = mu\n",
    "        self.lam = lam\n",
    "        self.k_init = k_init\n",
    "        if base_learner in [LinearRegression, Ridge]:\n",
    "            self.perf_metric = explained_variance_score\n",
    "        elif base_learner in [LogisticRegression]:\n",
    "            self.perf_metric = accuracy_score\n",
    "        else:\n",
    "            raise Exception(\"Unsupported Base Learner\")\n",
    "\n",
    "        self.base_learner = base_learner\n",
    "        self.base_learner_kwargs = base_learner_kwargs\n",
    "\n",
    "    def fit(self, X, y, task_id):\n",
    "        \"\"\" Fit the model to a new batch of training data.  The task_id must\n",
    "            start at 0 and increase by one each time this function is called.\n",
    "            Currently you cannot add new data to old tasks.\n",
    "\n",
    "            X: the training data\n",
    "            y: the trianing labels\n",
    "            task_id: the id of the task\n",
    "        \"\"\"\n",
    "        self.T += 1\n",
    "        single_task_model = self.base_learner(fit_intercept = False, **self.base_learner_kwargs).fit(X, y)\n",
    "        D_t = self.get_hessian(single_task_model, X, y)\n",
    "        D_t_sqrt = sqrtm(D_t)\n",
    "        theta_t = single_task_model.coef_\n",
    "\n",
    "        sparse_encode = Lasso(alpha = self.mu / (X.shape[0] * 2.0),\n",
    "                              fit_intercept = False).fit(D_t_sqrt.dot(self.L),\n",
    "                                                         D_t_sqrt.dot(theta_t.T))\n",
    "        if self.k_init and task_id < self.k:\n",
    "            sparse_coeffs = np.zeros((self.k,))\n",
    "            sparse_coeffs[task_id] = 1.0\n",
    "        else:\n",
    "            sparse_coeffs = sparse_encode.coef_\n",
    "        self.S = np.hstack((self.S, np.matrix(sparse_coeffs).T))\n",
    "        self.A += np.kron(self.S[:,task_id].dot(self.S[:,task_id].T), D_t)\n",
    "        self.b += np.kron(self.S[:,task_id].T, np.mat(theta_t).dot(D_t)).T\n",
    "        L_vectorized = inv(self.A / self.T + self.lam * np.eye(self.d * self.k, self.d * self.k)).dot(self.b) / self.T\n",
    "        self.L = L_vectorized.reshape((self.k, self.d)).T\n",
    "        self.revive_dead_components()\n",
    "\n",
    "    def revive_dead_components(self):\n",
    "        \"\"\" re-initailizes any components that have decayed to 0 \"\"\"\n",
    "        for i,val in enumerate(np.sum(self.L, axis = 0)):\n",
    "            if abs(val) < 10 ** -8:\n",
    "                self.L[:, i] = np.random.randn(self.d,)\n",
    "\n",
    "    def predict(self, X, task_id):\n",
    "        \"\"\" Output ELLA's predictions for the specified data on the specified\n",
    "            task_id.  If using a continuous model (Ridge and LinearRegression)\n",
    "            the result is the prediction.  If using a classification model\n",
    "            (LogisticRgerssion) the output is currently a probability.\n",
    "        \"\"\"\n",
    "        if self.base_learner == LinearRegression or self.base_learner == Ridge:\n",
    "            return X.dot(self.L.dot(self.S[:, task_id]))\n",
    "        elif self.base_learner == LogisticRegression:\n",
    "            return 1. / (1.0 + np.exp(-X.dot(self.L.dot(self.S[:, task_id])))) > 0.5\n",
    "\n",
    "    def predict_probs(self, X, task_id):\n",
    "        \"\"\" Output ELLA's predictions for the specified data on the specified\n",
    "            task_id.  If using a continuous model (Ridge and LinearRegression)\n",
    "            the result is the prediction.  If using a classification model\n",
    "            (LogisticRgerssion) the output is currently a probability.\n",
    "        \"\"\"\n",
    "        if self.base_learner == LinearRegression or self.base_learner == Ridge:\n",
    "            raise Exception(\"This base learner does not support predicting probabilities\")\n",
    "        elif self.base_learner == LogisticRegression:\n",
    "            return np.exp(self.predict_logprobs(X, task_id))\n",
    "\n",
    "    def predict_logprobs(self, X, task_id):\n",
    "        \"\"\" Output ELLA's predictions for the specified data on the specified\n",
    "            task_id.  If using a continuous model (Ridge and LinearRegression)\n",
    "            the result is the prediction.  If using a classification model\n",
    "            (LogisticRgerssion) the output is currently a probability.\n",
    "        \"\"\"\n",
    "        if self.base_learner == LinearRegression or self.base_learner == Ridge:\n",
    "            raise Exception(\"This base learner does not support predicting probabilities\")\n",
    "        elif self.base_learner == LogisticRegression:\n",
    "            return -logsumexp(np.hstack((np.zeros((X.shape[0], 1)), -X.dot(self.L.dot(self.S[:, task_id])))), axis = 1)\n",
    "\n",
    "    def score(self, X, y, task_id):\n",
    "        \"\"\" Output the score for ELLA's model on the specified testing data.\n",
    "            If using a continuous model (Ridge and LinearRegression)\n",
    "            the score is explained variance.  If using a classification model\n",
    "            (LogisticRegression) the score is accuracy.\n",
    "        \"\"\"\n",
    "        return self.perf_metric(self.predict(X, task_id), y)\n",
    "\n",
    "    def get_hessian(self, model, X, y):\n",
    "        \"\"\" ELLA requires that each single task learner provide the Hessian\n",
    "            of the loss function evaluated around the optimal single task\n",
    "            parameters.  This funciton implements this for the base learners\n",
    "            that are currently supported \"\"\"\n",
    "        theta_t = model.coef_\n",
    "        if self.base_learner == LinearRegression:\n",
    "            return X.T.dot(X)/(2.0 * X.shape[0])\n",
    "        elif self.base_learner == Ridge:\n",
    "            return X.T.dot(X)/(2.0 * X.shape[0]) + model.alpha * np.eye(self.d, self.d)\n",
    "        elif self.base_learner == LogisticRegression:\n",
    "            preds = 1. / (1.0 + np.exp(-X.dot(theta_t.T)))\n",
    "            base = np.tile(preds * (1 - preds), (1, X.shape[1]))\n",
    "            hessian = (np.multiply(X, base)).T.dot(X) / (2.0 * X.shape[0])\n",
    "            return hessian + np.eye(self.d,self.d) / (2.0 * model.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SparseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, K:int=32, M:int=128, R_lr:float=0.1, lmda:float=5e-3, device=None):\n",
    "        super(SparseNet, self).__init__()\n",
    "        self.K = K\n",
    "        self.M = M\n",
    "        self.R_lr = R_lr\n",
    "        self.lmda = lmda\n",
    "        # synaptic weights\n",
    "        self.device = torch.device(\"cpu\") if device is None else device\n",
    "        self.U = nn.Linear(self.K, self.M, bias=False).to(self.device)\n",
    "        # responses\n",
    "        self.R = None\n",
    "        self.normalize_weights()\n",
    "\n",
    "    def ista_(self, img_batch):\n",
    "        # create R\n",
    "        self.R = torch.zeros((img_batch.shape[0], self.K), requires_grad=True, device=self.device)\n",
    "        converged = False\n",
    "        # update R\n",
    "        optim = torch.optim.SGD([{'params': self.R, \"lr\": self.R_lr}])\n",
    "        # train\n",
    "        while not converged:\n",
    "            old_R = self.R.clone().detach()\n",
    "            # pred\n",
    "            pred = self.U(self.R)\n",
    "            # loss\n",
    "            loss = ((img_batch - pred) ** 2).sum()\n",
    "            loss.backward()\n",
    "            # update R in place\n",
    "            optim.step()\n",
    "            # zero grad\n",
    "            self.zero_grad()\n",
    "            # prox\n",
    "            self.R.data = SparseNet.soft_thresholding_(self.R, self.lmda)\n",
    "            # convergence\n",
    "            converged = torch.norm(self.R - old_R) / torch.norm(old_R) < 0.01\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_thresholding_(x, alpha):\n",
    "        with torch.no_grad():\n",
    "            rtn = F.relu(x - alpha) - F.relu(-x - alpha)\n",
    "        return rtn.data\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.R.grad.zero_()\n",
    "        self.U.zero_grad()\n",
    "\n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            self.U.weight.data = F.normalize(self.U.weight.data, dim=0)\n",
    "\n",
    "    def forward(self, img_batch):\n",
    "        # first fit\n",
    "        self.ista_(img_batch)\n",
    "        # now predict again\n",
    "        pred = self.U(self.R)\n",
    "        return pred, self.R\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.linalg import sqrtm, inv, norm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class ParamShare(object):\n",
    "    def __init__(self):\n",
    "        self.net = SparseNet()\n",
    "        self.base_learner = LogisticRegression\n",
    "        self.codes = []\n",
    "\n",
    "    def fit(self, X, y, task_id):\n",
    "        single_task_model = self.base_learner(max_iter=10000)\n",
    "        single_task_model.fit(X,y)\n",
    "        theta_t = torch.tensor(single_task_model.coef_)\n",
    "        optim = torch.optim.SGD([{'params': self.net.U.weight, \"lr\": 1e-2}])\n",
    "        \n",
    "        running_loss = 0\n",
    "        code = None\n",
    "        for c in range(10):\n",
    "            pred, code = self.net.forward(theta_t)\n",
    "            loss = ((theta_t - pred) ** 2).sum()\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            self.net.zero_grad()\n",
    "            self.net.normalize_weights()\n",
    "            #print('Loss', running_loss / (c+1))\n",
    "        \n",
    "        self.codes.append(code.clone().detach().numpy()[0])\n",
    "        tpred = self.predict(X,task_id)\n",
    "        print()\n",
    "        print(\"PMS-BAC:\", balanced_accuracy_score(y,tpred))\n",
    "        print(\"T-BAC:\", balanced_accuracy_score(y,single_task_model.predict(X)))\n",
    "\n",
    "    def predict(self, X, task_id):\n",
    "        code = self.codes[task_id]\n",
    "        theta_t = self.net.U(torch.tensor(code)).detach().numpy()\n",
    "        sign = 1. / (1.0 + np.exp(-X.dot(theta_t)))\n",
    "        pred = [1 if p > 0.5 else 0 for p in sign]\n",
    "        return np.array(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "\n",
      "PMS-BAC: 0.7798073058942624\n",
      "T-BAC: 0.8074086508869118\n",
      "\n",
      "PMS-BAC: 0.8798149437289708\n",
      "T-BAC: 0.8959276018099547\n",
      "\n",
      "PMS-BAC: 0.8223209217386738\n",
      "T-BAC: 0.807354889994566\n",
      "0 0.7756078375286042\n",
      "1 0.8640648567119156\n",
      "2 0.7911586876546357\n",
      "fold 1\n",
      "\n",
      "PMS-BAC: 0.78870852402746\n",
      "T-BAC: 0.8037021902582544\n",
      "\n",
      "PMS-BAC: 0.8787417333797425\n",
      "T-BAC: 0.885384035270913\n",
      "\n",
      "PMS-BAC: 0.8115487670788042\n",
      "T-BAC: 0.7971320312177952\n",
      "0 0.7737838346533998\n",
      "1 0.8692713771899292\n",
      "2 0.7905394417610565\n",
      "fold 2\n",
      "\n",
      "PMS-BAC: 0.7862702906181167\n",
      "T-BAC: 0.8117376552159161\n",
      "\n",
      "PMS-BAC: 0.8779005685114283\n",
      "T-BAC: 0.8781471168348995\n",
      "\n",
      "PMS-BAC: 0.8244794160247747\n",
      "T-BAC: 0.8071963108635796\n",
      "0 0.7854190503432494\n",
      "1 0.8659067177166724\n",
      "2 0.792542574500644\n",
      "fold 3\n",
      "\n",
      "PMS-BAC: 0.7826587528604119\n",
      "T-BAC: 0.8102239293886891\n",
      "\n",
      "PMS-BAC: 0.8813232393549135\n",
      "T-BAC: 0.899640329504583\n",
      "\n",
      "PMS-BAC: 0.8122435708150462\n",
      "T-BAC: 0.7974982354391128\n",
      "0 0.7603573081833952\n",
      "1 0.8643839192481726\n",
      "2 0.7975300542805255\n",
      "fold 4\n",
      "\n",
      "PMS-BAC: 0.780463015245624\n",
      "T-BAC: 0.8139021573804183\n",
      "\n",
      "PMS-BAC: 0.8815262791507135\n",
      "T-BAC: 0.8861381830838845\n",
      "\n",
      "PMS-BAC: 0.8171592720308836\n",
      "T-BAC: 0.7961697584546918\n",
      "0 0.7768419009480223\n",
      "1 0.8586843021232162\n",
      "2 0.7880851969535981\n",
      "fold 5\n",
      "\n",
      "PMS-BAC: 0.7848908957175548\n",
      "T-BAC: 0.7953589816933638\n",
      "\n",
      "PMS-BAC: 0.8812942336697993\n",
      "T-BAC: 0.8917507831534981\n",
      "\n",
      "PMS-BAC: 0.8210058880389272\n",
      "T-BAC: 0.8120785139158446\n",
      "0 0.784155131981219\n",
      "1 0.86246954403063\n",
      "2 0.7885296785388902\n",
      "fold 6\n",
      "\n",
      "PMS-BAC: 0.7843901496075409\n",
      "T-BAC: 0.795528560745952\n",
      "\n",
      "PMS-BAC: 0.881163708086785\n",
      "T-BAC: 0.8906920756468268\n",
      "\n",
      "PMS-BAC: 0.8196283592009228\n",
      "T-BAC: 0.8069210700152114\n",
      "0 0.773494197450147\n",
      "1 0.865964729086901\n",
      "2 0.7993181518001886\n",
      "fold 7\n",
      "\n",
      "PMS-BAC: 0.7843034897025172\n",
      "T-BAC: 0.8267336139261197\n",
      "\n",
      "PMS-BAC: 0.8802355261631281\n",
      "T-BAC: 0.8905325443786982\n",
      "\n",
      "PMS-BAC: 0.8142965185781551\n",
      "T-BAC: 0.8016932213100066\n",
      "0 0.7749401010270576\n",
      "1 0.8643404107205013\n",
      "2 0.7918664056676383\n",
      "fold 8\n",
      "\n",
      "PMS-BAC: 0.7905499514195167\n",
      "T-BAC: 0.8093381354250919\n",
      "\n",
      "PMS-BAC: 0.8818018331592992\n",
      "T-BAC: 0.8939117066945121\n",
      "\n",
      "PMS-BAC: 0.8146507925926375\n",
      "T-BAC: 0.8088954812018077\n",
      "0 0.7838325433148088\n",
      "1 0.8603376261747302\n",
      "2 0.7922254162386713\n",
      "fold 9\n",
      "\n",
      "PMS-BAC: 0.7827762340634195\n",
      "T-BAC: 0.8105763729977117\n",
      "\n",
      "PMS-BAC: 0.8770594036431141\n",
      "T-BAC: 0.8964206984568975\n",
      "\n",
      "PMS-BAC: 0.8136736433071614\n",
      "T-BAC: 0.8011541363915129\n",
      "0 0.7701410614454092\n",
      "1 0.8589888618169161\n",
      "2 0.8053784586916515\n"
     ]
    }
   ],
   "source": [
    "T = 3\n",
    "d = 128\n",
    "\n",
    "bacs = [[],[],[]]\n",
    "\n",
    "\n",
    "for _foldid in range(10):\n",
    "    print(\"fold\",_foldid)\n",
    "    ella = ParamShare()\n",
    "\n",
    "    ### Train\n",
    "    for task in range(T):\n",
    "        X = DATA[\"X\"][DATA[\"category\"] == task]\n",
    "        y = DATA[\"y\"][DATA[\"category\"] == task]\n",
    "        foldids = DATA[\"folds\"][task]\n",
    "        \n",
    "        pca = PCA(n_components=d)\n",
    "        pca.fit_transform(X)\n",
    "        X=pca.transform(X)\n",
    "\n",
    "        train = foldids[_foldid][1]\n",
    "        test = foldids[_foldid][2]\n",
    "\n",
    "        ella.fit(X[train], y[train], task)\n",
    " \n",
    "    ##### Test\n",
    "    for task in range(T):\n",
    "        X = DATA[\"X\"][DATA[\"category\"] == task]\n",
    "        y = DATA[\"y\"][DATA[\"category\"] == task]\n",
    "        foldids = DATA[\"folds\"][task]\n",
    "        \n",
    "        pca = PCA(n_components=d)\n",
    "        pca.fit_transform(X)\n",
    "        X=pca.transform(X)\n",
    "\n",
    "        train = foldids[_foldid][1]\n",
    "        test = foldids[_foldid][2]\n",
    "\n",
    "        pred=ella.predict(X[test], task)\n",
    "        bac = balanced_accuracy_score(y[test], pred)\n",
    "        print(task,bac)\n",
    "        bacs[task].append(bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0 0.7696142530238641\n",
      "1 0.8519839888618169\n",
      "2 0.7937192568906208\n",
      "fold 1\n",
      "0 0.7611986916334743\n",
      "1 0.8658051978187724\n",
      "2 0.7928269709769888\n",
      "fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.109e+00, tolerance: 1.034e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7606836384439359\n",
      "1 0.8652830954867154\n",
      "2 0.7823391641659958\n",
      "fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.672e+00, tolerance: 8.513e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7542351498873239\n",
      "1 0.8394100243647755\n",
      "2 0.7952607061114579\n",
      "fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.222e+00, tolerance: 9.417e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7710250490356325\n",
      "1 0.8699820164752292\n",
      "2 0.7962131976535336\n",
      "fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.376e-01, tolerance: 9.774e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7628132932480758\n",
      "1 0.8474881076691032\n",
      "2 0.7954776161011351\n",
      "fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+00, tolerance: 1.005e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7710832788492972\n",
      "1 0.8514038751595312\n",
      "2 0.7875865770547354\n",
      "fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.622e+00, tolerance: 9.290e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7543221369308326\n",
      "1 0.86267258382643\n",
      "2 0.7885551825074087\n",
      "fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkozik/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.977e-01, tolerance: 1.059e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.765324656750572\n",
      "1 0.8577851258846734\n",
      "2 0.7870041668033728\n",
      "fold 9\n",
      "0 0.7691211256428647\n",
      "1 0.8583072282167304\n",
      "2 0.796225311653445\n"
     ]
    }
   ],
   "source": [
    "T = 3\n",
    "d = 85\n",
    "k = 32\n",
    "noise_var = .1\n",
    "\n",
    "bacs = [[],[],[]]\n",
    "\n",
    "\n",
    "for _foldid in range(10):\n",
    "    print(\"fold\",_foldid)\n",
    "    ella = ELLA(d,k,LogisticRegression, base_learner_kwargs={'max_iter':10000},mu=1,lam=10**-5)\n",
    "\n",
    "    ### Train\n",
    "    for task in range(T):\n",
    "        X = DATA[\"X\"][DATA[\"category\"] == task]\n",
    "        y = DATA[\"y\"][DATA[\"category\"] == task]\n",
    "        foldids = DATA[\"folds\"][task]\n",
    "        \n",
    "        pca = PCA(n_components=d)\n",
    "        pca.fit_transform(X)\n",
    "        X=pca.transform(X)\n",
    "\n",
    "        train = foldids[_foldid][1]\n",
    "        test = foldids[_foldid][2]\n",
    "\n",
    "        ella.fit(X[train], y[train], task)\n",
    " \n",
    "    ##### Test\n",
    "    for task in range(T):\n",
    "        X = DATA[\"X\"][DATA[\"category\"] == task]\n",
    "        y = DATA[\"y\"][DATA[\"category\"] == task]\n",
    "        foldids = DATA[\"folds\"][task]\n",
    "        \n",
    "        pca = PCA(n_components=d)\n",
    "        pca.fit_transform(X)\n",
    "        X=pca.transform(X)\n",
    "\n",
    "        train = foldids[_foldid][1]\n",
    "        test = foldids[_foldid][2]\n",
    "\n",
    "        pred=ella.predict(X[test], task)\n",
    "        bac = balanced_accuracy_score(y[test], pred)\n",
    "        print(task,bac)\n",
    "        bacs[task].append(bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7758572966875313\n",
      "1 0.8634412344819584\n",
      "2 0.79371740660875\n"
     ]
    }
   ],
   "source": [
    "for t in range(T):\n",
    "    print(t,np.mean(bacs[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8110053125927466, 0.038171596451482374)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(bacs).flatten()), np.std(np.array(bacs).flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
